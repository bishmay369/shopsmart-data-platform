{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70334aac-1a70-4bea-a906-428d2421d679",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auth configured. Ready for streaming pipeline.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# NOTEBOOK 4: STREAMING PIPELINE\n",
    "# Cell 0: Authentication\n",
    "# ============================================================\n",
    "\n",
    "client_id     = dbutils.secrets.get(scope=\"shopsmart-scope\", key=\"datalake-sp-client-id\")\n",
    "client_secret = dbutils.secrets.get(scope=\"shopsmart-scope\", key=\"datalake-sp-client-secret\")\n",
    "tenant_id     = dbutils.secrets.get(scope=\"shopsmart-scope\", key=\"datalake-sp-tenant-id\")\n",
    "\n",
    "storage_account_name = \"dlsshopsmartdev123\"\n",
    "\n",
    "spark.conf.set(\"fs.azure.account.auth.type.\" + storage_account_name + \".dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(\"fs.azure.account.oauth.provider.type.\" + storage_account_name + \".dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.id.\" + storage_account_name + \".dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.secret.\" + storage_account_name + \".dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.endpoint.\" + storage_account_name + \".dfs.core.windows.net\", \"https://login.microsoftonline.com/\" + tenant_id + \"/oauth2/token\")\n",
    "\n",
    "BRONZE = \"abfss://bronze@\" + storage_account_name + \".dfs.core.windows.net\"\n",
    "SILVER = \"abfss://silver@\" + storage_account_name + \".dfs.core.windows.net\"\n",
    "GOLD   = \"abfss://gold@\" + storage_account_name + \".dfs.core.windows.net\"\n",
    "\n",
    "print(\"Auth configured. Ready for streaming pipeline.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1ce02c9-0c52-4555-8470-919202816582",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting azure-eventhub\n  Downloading azure_eventhub-5.15.1-py3-none-any.whl.metadata (30 kB)\nRequirement already satisfied: azure-core>=1.27.0 in /databricks/python3/lib/python3.12/site-packages (from azure-eventhub) (1.31.0)\nRequirement already satisfied: typing-extensions>=4.0.1 in /databricks/python3/lib/python3.12/site-packages (from azure-eventhub) (4.11.0)\nRequirement already satisfied: requests>=2.21.0 in /databricks/python3/lib/python3.12/site-packages (from azure-core>=1.27.0->azure-eventhub) (2.32.2)\nRequirement already satisfied: six>=1.11.0 in /usr/lib/python3/dist-packages (from azure-core>=1.27.0->azure-eventhub) (1.16.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.12/site-packages (from requests>=2.21.0->azure-core>=1.27.0->azure-eventhub) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.12/site-packages (from requests>=2.21.0->azure-core>=1.27.0->azure-eventhub) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /databricks/python3/lib/python3.12/site-packages (from requests>=2.21.0->azure-core>=1.27.0->azure-eventhub) (1.26.16)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.12/site-packages (from requests>=2.21.0->azure-core>=1.27.0->azure-eventhub) (2024.6.2)\nDownloading azure_eventhub-5.15.1-py3-none-any.whl (317 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/317.1 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m317.1/317.1 kB\u001B[0m \u001B[31m12.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hInstalling collected packages: azure-eventhub\nSuccessfully installed azure-eventhub-5.15.1\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n=================================================================\nSTREAMING PIPELINE - EVENT HUB CONSUMER\n=================================================================\n  Event Hub:      eh-clickstream\n  Consumer Group: $Default\n  Reading events...\n  Waiting 15 seconds for events...\n  Events received: 50\n  Successfully parsed: 50\n\n  Spark DataFrame created: 50 rows\n  Written to Bronze: abfss://bronze@dlsshopsmartdev123.dfs.core.windows.net/streaming_clickstream\n\n  Sample streaming events:\n+----------------+-----------+------------+----------+-----------+------------------------+\n|event_id        |customer_id|event_type  |product_id|device_type|event_timestamp         |\n+----------------+-----------+------------+----------+-----------+------------------------+\n|EVT-C3A559B20CE2|CUST396    |page_view   |PROD026   |mobile     |2026-02-18T08:30:23.502Z|\n|EVT-832FC70E0DDB|CUST077    |product_view|PROD007   |mobile     |2026-02-18T08:30:18.788Z|\n|EVT-80AF42D55ED3|CUST480    |page_view   |PROD004   |mobile     |2026-02-18T08:30:26.471Z|\n|EVT-C885AD27871E|CUST353    |page_view   |PROD024   |desktop    |2026-02-18T08:30:25.180Z|\n|EVT-A804F2B07CC7|CUST472    |add_to_cart |PROD034   |desktop    |2026-02-18T08:30:29.202Z|\n|EVT-D17920FB8D62|CUST192    |product_view|PROD027   |desktop    |2026-02-18T08:30:27.748Z|\n|EVT-D4908CDDEA07|CUST291    |product_view|PROD049   |desktop    |2026-02-18T08:30:31.749Z|\n|EVT-B2DEF3151BA1|CUST244    |add_to_cart |PROD049   |mobile     |2026-02-18T08:30:30.475Z|\n|EVT-AFDB91EE33FB|CUST437    |product_view|PROD034   |desktop    |2026-02-18T08:30:34.332Z|\n|EVT-6E01EDE1700B|CUST101    |page_view   |PROD002   |desktop    |2026-02-18T08:30:33.053Z|\n+----------------+-----------+------------+----------+-----------+------------------------+\nonly showing top 10 rows\n  Event type distribution (streaming):\n+----------------+-----+\n|      event_type|count|\n+----------------+-----+\n|     add_to_cart|   14|\n|       page_view|   13|\n|    product_view|   12|\n|        checkout|    5|\n|          search|    3|\n|remove_from_cart|    3|\n+----------------+-----+\n\n\n[DONE] Streaming events landed in Bronze!\n[NEXT] Process through Silver layer\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 1: READ FROM EVENT HUB + WRITE TO BRONZE\n",
    "# ============================================================\n",
    "#\n",
    "# HOW THIS WORKS:\n",
    "# 1. We install azure-eventhub on the cluster\n",
    "# 2. Read all available events from Event Hub using Python SDK\n",
    "# 3. Convert to Spark DataFrame\n",
    "# 4. Write to Bronze as Delta table\n",
    "# 5. Then process through Silver (same pattern as batch)\n",
    "#\n",
    "# WHY PYTHON SDK INSTEAD OF SPARK STREAMING CONNECTOR?\n",
    "# The Spark Event Hub connector requires:\n",
    "#   - Maven packages (com.microsoft.azure:azure-eventhubs-spark)\n",
    "#   - Single-user cluster (not shared)\n",
    "# Python SDK works everywhere and is simpler for demo.\n",
    "#\n",
    "# IN PRODUCTION:\n",
    "# You would use Spark Structured Streaming with the\n",
    "# Event Hub connector for true real-time processing.\n",
    "# This demo proves the concept works end-to-end.\n",
    "# ============================================================\n",
    "\n",
    "# Step 1: Install azure-eventhub on cluster\n",
    "%pip install azure-eventhub\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "from azure.eventhub import EventHubConsumerClient\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import json\n",
    "\n",
    "# Step 2: Get connection string from Key Vault\n",
    "eh_conn_str = dbutils.secrets.get(scope=\"shopsmart-scope\", key=\"eventhub-connection-string\")\n",
    "EVENTHUB_NAME = \"eh-clickstream\"\n",
    "CONSUMER_GROUP = \"$Default\"\n",
    "\n",
    "print(\"=\" * 65)\n",
    "print(\"STREAMING PIPELINE - EVENT HUB CONSUMER\")\n",
    "print(\"=\" * 65)\n",
    "print(\"  Event Hub:      \" + EVENTHUB_NAME)\n",
    "print(\"  Consumer Group: \" + CONSUMER_GROUP)\n",
    "print(\"  Reading events...\")\n",
    "\n",
    "# Step 3: Read events from Event Hub\n",
    "events_list = []\n",
    "\n",
    "def on_event(partition_context, event):\n",
    "    \"\"\"Callback for each event received\"\"\"\n",
    "    body = event.body_as_str()\n",
    "    events_list.append(body)\n",
    "    partition_context.update_checkpoint(event)\n",
    "\n",
    "# Create consumer and receive events\n",
    "consumer = EventHubConsumerClient.from_connection_string(\n",
    "    conn_str=eh_conn_str,\n",
    "    consumer_group=CONSUMER_GROUP,\n",
    "    eventhub_name=EVENTHUB_NAME\n",
    ")\n",
    "\n",
    "# Read for 15 seconds then stop\n",
    "import threading\n",
    "import time\n",
    "\n",
    "def receive_events():\n",
    "    with consumer:\n",
    "        consumer.receive(\n",
    "            on_event=on_event,\n",
    "            starting_position=\"-1\",  # Read from beginning\n",
    "        )\n",
    "\n",
    "# Run receiver in background thread with timeout\n",
    "receiver_thread = threading.Thread(target=receive_events, daemon=True)\n",
    "receiver_thread.start()\n",
    "\n",
    "# Wait 15 seconds for events to arrive\n",
    "print(\"  Waiting 15 seconds for events...\")\n",
    "time.sleep(15)\n",
    "\n",
    "# Consumer will stop when thread is killed (daemon=True)\n",
    "print(\"  Events received: \" + str(len(events_list)))\n",
    "\n",
    "if len(events_list) == 0:\n",
    "    print(\"\\n  WARNING: No events received!\")\n",
    "    print(\"  Make sure you ran stream_producer.py first\")\n",
    "    print(\"  and events were sent to: \" + EVENTHUB_NAME)\n",
    "else:\n",
    "    # Step 4: Parse JSON events into list of dicts\n",
    "    parsed_events = []\n",
    "    parse_errors = 0\n",
    "    for event_str in events_list:\n",
    "        try:\n",
    "            event_dict = json.loads(event_str)\n",
    "            # Flatten geo_location\n",
    "            if \"geo_location\" in event_dict:\n",
    "                event_dict[\"geo_city\"] = event_dict[\"geo_location\"].get(\"city\")\n",
    "                event_dict[\"geo_country\"] = event_dict[\"geo_location\"].get(\"country\")\n",
    "                del event_dict[\"geo_location\"]\n",
    "            parsed_events.append(event_dict)\n",
    "        except:\n",
    "            parse_errors += 1\n",
    "\n",
    "    print(\"  Successfully parsed: \" + str(len(parsed_events)))\n",
    "    if parse_errors > 0:\n",
    "        print(\"  Parse errors: \" + str(parse_errors))\n",
    "\n",
    "    # Step 5: Define schema for the streaming events\n",
    "    stream_schema = StructType([\n",
    "        StructField(\"event_id\", StringType(), True),\n",
    "        StructField(\"session_id\", StringType(), True),\n",
    "        StructField(\"customer_id\", StringType(), True),\n",
    "        StructField(\"event_type\", StringType(), True),\n",
    "        StructField(\"event_timestamp\", StringType(), True),\n",
    "        StructField(\"page_url\", StringType(), True),\n",
    "        StructField(\"product_id\", StringType(), True),\n",
    "        StructField(\"device_type\", StringType(), True),\n",
    "        StructField(\"browser\", StringType(), True),\n",
    "        StructField(\"os\", StringType(), True),\n",
    "        StructField(\"ip_address\", StringType(), True),\n",
    "        StructField(\"geo_city\", StringType(), True),\n",
    "        StructField(\"geo_country\", StringType(), True),\n",
    "        StructField(\"referrer\", StringType(), True),\n",
    "        StructField(\"search_query\", StringType(), True),\n",
    "        StructField(\"page_load_time_ms\", IntegerType(), True),\n",
    "        StructField(\"time_on_page_sec\", IntegerType(), True),\n",
    "        StructField(\"scroll_depth_pct\", IntegerType(), True),\n",
    "    ])\n",
    "\n",
    "    # Step 6: Create Spark DataFrame\n",
    "    df_stream_events = spark.createDataFrame(parsed_events, schema=stream_schema)\n",
    "\n",
    "    # Add streaming metadata\n",
    "    df_stream_bronze = df_stream_events \\\n",
    "        .withColumn(\"_ingestion_source\", lit(\"event_hub\")) \\\n",
    "        .withColumn(\"_ingestion_timestamp\", current_timestamp()) \\\n",
    "        .withColumn(\"_event_hub_name\", lit(EVENTHUB_NAME)) \\\n",
    "        .withColumn(\"_batch_id\", lit(\"stream_\" + time.strftime(\"%Y%m%d_%H%M%S\")))\n",
    "\n",
    "    stream_count = df_stream_bronze.count()\n",
    "    print(\"\\n  Spark DataFrame created: \" + str(stream_count) + \" rows\")\n",
    "\n",
    "    # Step 7: Write to Bronze as Delta (APPEND to existing clickstream)\n",
    "    bronze_stream_path = BRONZE + \"/streaming_clickstream\"\n",
    "\n",
    "    df_stream_bronze.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .option(\"mergeSchema\", True) \\\n",
    "        .save(bronze_stream_path)\n",
    "\n",
    "    print(\"  Written to Bronze: \" + bronze_stream_path)\n",
    "\n",
    "    # Step 8: Show sample\n",
    "    print(\"\\n  Sample streaming events:\")\n",
    "    df_stream_bronze.select(\n",
    "        \"event_id\", \"customer_id\", \"event_type\",\n",
    "        \"product_id\", \"device_type\", \"event_timestamp\"\n",
    "    ).show(10, truncate=False)\n",
    "\n",
    "    # Event type distribution\n",
    "    print(\"  Event type distribution (streaming):\")\n",
    "    df_stream_bronze.groupBy(\"event_type\").count().orderBy(desc(\"count\")).show()\n",
    "\n",
    "    print(\"\\n[DONE] Streaming events landed in Bronze!\")\n",
    "    print(\"[NEXT] Process through Silver layer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "542a6a38-4d48-452f-aca4-e80046e52b86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming events in Bronze: 50 rows\n\n=================================================================\nSTREAMING SILVER - COMPLETE\n=================================================================\n  Bronze events:   50\n  Silver events:   50\n  Path:            abfss://silver@dlsshopsmartdev123.dfs.core.windows.net/streaming_clickstream\n\n  DATA COMPARISON:\n    Batch clickstream (Silver):     3000 events\n    Streaming clickstream (Silver): 50 events\n    Total clickstream events:       3050\n\n  Streaming Silver sample:\n+----------------+-----------+------------+----------+---------------+------------+-----------+\n|event_id        |customer_id|event_type  |product_id|funnel_stage   |is_anonymous|data_source|\n+----------------+-----------+------------+----------+---------------+------------+-----------+\n|EVT-C3A559B20CE2|CUST396    |page_view   |PROD026   |1-Awareness    |false       |streaming  |\n|EVT-832FC70E0DDB|CUST077    |product_view|PROD007   |2-Interest     |false       |streaming  |\n|EVT-80AF42D55ED3|CUST480    |page_view   |PROD004   |1-Awareness    |false       |streaming  |\n|EVT-C885AD27871E|CUST353    |page_view   |PROD024   |1-Awareness    |false       |streaming  |\n|EVT-A804F2B07CC7|CUST472    |add_to_cart |PROD034   |3-Consideration|false       |streaming  |\n|EVT-D17920FB8D62|CUST192    |product_view|PROD027   |2-Interest     |false       |streaming  |\n|EVT-D4908CDDEA07|CUST291    |product_view|PROD049   |2-Interest     |false       |streaming  |\n|EVT-B2DEF3151BA1|CUST244    |add_to_cart |PROD049   |3-Consideration|false       |streaming  |\n|EVT-AFDB91EE33FB|CUST437    |product_view|PROD034   |2-Interest     |false       |streaming  |\n|EVT-6E01EDE1700B|CUST101    |page_view   |PROD002   |1-Awareness    |false       |streaming  |\n+----------------+-----------+------------+----------+---------------+------------+-----------+\nonly showing top 10 rows\n\n  Streaming funnel analysis:\n+---------------+------+--------+\n|   funnel_stage|events|sessions|\n+---------------+------+--------+\n|    1-Awareness|    13|      13|\n|     2-Interest|    12|      12|\n|3-Consideration|    20|      20|\n|     4-Purchase|     5|       5|\n+---------------+------+--------+\n\n\n  Streaming device distribution:\n+-----------+-----+\n|device_type|count|\n+-----------+-----+\n|     mobile|   24|\n|    desktop|   19|\n|     tablet|    7|\n+-----------+-----+\n\n\n=================================================================\nSTREAMING PIPELINE - COMPLETE!\n=================================================================\n\n  WHAT WE PROVED:\n    1. Python producer sends events to Azure Event Hub\n    2. Databricks reads events from Event Hub\n    3. Events land in Bronze (raw, with metadata)\n    4. Same Silver transformations apply to streaming data\n    5. Batch + streaming data coexist in the same lake\n\n  IN PRODUCTION (ARCHITECTURE DIAGRAM):\n    - stream_producer.py -> Azure Event Hub (DONE)\n    - Event Hub -> Spark Structured Streaming (simulated)\n    - Streaming -> Bronze -> Silver (DONE)\n    - Same Gold layer serves both batch + streaming data\n\n  THIS COMPLETES THE STREAMING PATH IN YOUR ARCHITECTURE!\n    \n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 2: PROCESS STREAMING DATA - BRONZE TO SILVER\n",
    "# ============================================================\n",
    "#\n",
    "# This applies the SAME transformations as our batch\n",
    "# clickstream pipeline (Cell 9 in notebook 01).\n",
    "# Proving that batch and streaming data follow the\n",
    "# same processing logic.\n",
    "# ============================================================\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Read streaming events from Bronze\n",
    "bronze_stream_path = BRONZE + \"/streaming_clickstream\"\n",
    "\n",
    "try:\n",
    "    df_stream = spark.read.format(\"delta\").load(bronze_stream_path)\n",
    "    stream_count = df_stream.count()\n",
    "    print(\"Streaming events in Bronze: \" + str(stream_count) + \" rows\")\n",
    "except:\n",
    "    print(\"ERROR: No streaming events found. Run Cell 1 first.\")\n",
    "    stream_count = 0\n",
    "\n",
    "if stream_count > 0:\n",
    "    # Apply same Silver transformations as batch pipeline\n",
    "    df_stream_silver = df_stream \\\n",
    "        .withColumn(\"event_id\", trim(col(\"event_id\"))) \\\n",
    "        .withColumn(\"session_id\", trim(col(\"session_id\"))) \\\n",
    "        .withColumn(\"customer_id\", trim(col(\"customer_id\"))) \\\n",
    "        .withColumn(\"event_type\", lower(trim(col(\"event_type\")))) \\\n",
    "        .withColumn(\"event_timestamp\", to_timestamp(col(\"event_timestamp\"))) \\\n",
    "        .withColumn(\"product_id\", trim(col(\"product_id\"))) \\\n",
    "        .withColumn(\"device_type\", lower(trim(col(\"device_type\")))) \\\n",
    "        .withColumn(\"browser\", initcap(trim(col(\"browser\")))) \\\n",
    "        .withColumn(\"os\", initcap(trim(col(\"os\")))) \\\n",
    "        .withColumn(\"referrer\", lower(trim(col(\"referrer\")))) \\\n",
    "        .withColumn(\"search_query\", lower(trim(col(\"search_query\")))) \\\n",
    "        .withColumn(\"event_date\", to_date(col(\"event_timestamp\"))) \\\n",
    "        .withColumn(\"event_hour\", hour(col(\"event_timestamp\"))) \\\n",
    "        .withColumn(\"day_name\", date_format(col(\"event_timestamp\"), \"EEEE\")) \\\n",
    "        .withColumn(\"is_weekend\",\n",
    "            when(dayofweek(col(\"event_timestamp\")).isin(1, 7), lit(True))\n",
    "            .otherwise(lit(False))) \\\n",
    "        .withColumn(\"is_anonymous\",\n",
    "            when(col(\"customer_id\").isNull(), lit(True)).otherwise(lit(False))) \\\n",
    "        .withColumn(\"is_purchase_intent\",\n",
    "            when(col(\"event_type\").isin(\"add_to_cart\", \"checkout\"), lit(True))\n",
    "            .otherwise(lit(False))) \\\n",
    "        .withColumn(\"funnel_stage\",\n",
    "            when(col(\"event_type\") == \"page_view\", lit(\"1-Awareness\"))\n",
    "            .when(col(\"event_type\") == \"product_view\", lit(\"2-Interest\"))\n",
    "            .when(col(\"event_type\").isin(\"add_to_cart\", \"search\", \"remove_from_cart\"), lit(\"3-Consideration\"))\n",
    "            .when(col(\"event_type\") == \"checkout\", lit(\"4-Purchase\"))\n",
    "            .otherwise(lit(\"Other\"))) \\\n",
    "        .withColumn(\"data_source\", lit(\"streaming\")) \\\n",
    "        .withColumn(\"_silver_processed_at\", current_timestamp()) \\\n",
    "        .withColumn(\"_silver_version\", lit(\"1.0\"))\n",
    "\n",
    "    # Write to Silver streaming table\n",
    "    silver_stream_path = SILVER + \"/streaming_clickstream\"\n",
    "\n",
    "    df_stream_silver.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .option(\"mergeSchema\", True) \\\n",
    "        .save(silver_stream_path)\n",
    "\n",
    "    silver_count = spark.read.format(\"delta\").load(silver_stream_path).count()\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"=\" * 65)\n",
    "    print(\"STREAMING SILVER - COMPLETE\")\n",
    "    print(\"=\" * 65)\n",
    "    print(\"  Bronze events:   \" + str(stream_count))\n",
    "    print(\"  Silver events:   \" + str(silver_count))\n",
    "    print(\"  Path:            \" + silver_stream_path)\n",
    "\n",
    "    # Compare batch vs streaming\n",
    "    batch_count = spark.read.format(\"delta\").load(SILVER + \"/clickstream\").count()\n",
    "    print(\"\\n  DATA COMPARISON:\")\n",
    "    print(\"    Batch clickstream (Silver):     \" + str(batch_count) + \" events\")\n",
    "    print(\"    Streaming clickstream (Silver): \" + str(silver_count) + \" events\")\n",
    "    print(\"    Total clickstream events:       \" + str(batch_count + silver_count))\n",
    "\n",
    "    # Show sample\n",
    "    print(\"\\n  Streaming Silver sample:\")\n",
    "    df_stream_silver.select(\n",
    "        \"event_id\", \"customer_id\", \"event_type\",\n",
    "        \"product_id\", \"funnel_stage\", \"is_anonymous\", \"data_source\"\n",
    "    ).show(10, truncate=False)\n",
    "\n",
    "    # Funnel analysis on streaming data\n",
    "    print(\"\\n  Streaming funnel analysis:\")\n",
    "    df_stream_silver.groupBy(\"funnel_stage\").agg(\n",
    "        count(\"*\").alias(\"events\"),\n",
    "        countDistinct(\"session_id\").alias(\"sessions\")\n",
    "    ).orderBy(\"funnel_stage\").show()\n",
    "\n",
    "    # Device distribution\n",
    "    print(\"\\n  Streaming device distribution:\")\n",
    "    df_stream_silver.groupBy(\"device_type\").count().orderBy(desc(\"count\")).show()\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 65)\n",
    "    print(\"STREAMING PIPELINE - COMPLETE!\")\n",
    "    print(\"=\" * 65)\n",
    "    print(\"\"\"\n",
    "  WHAT WE PROVED:\n",
    "    1. Python producer sends events to Azure Event Hub\n",
    "    2. Databricks reads events from Event Hub\n",
    "    3. Events land in Bronze (raw, with metadata)\n",
    "    4. Same Silver transformations apply to streaming data\n",
    "    5. Batch + streaming data coexist in the same lake\n",
    "\n",
    "  IN PRODUCTION (ARCHITECTURE DIAGRAM):\n",
    "    - stream_producer.py -> Azure Event Hub (DONE)\n",
    "    - Event Hub -> Spark Structured Streaming (simulated)\n",
    "    - Streaming -> Bronze -> Silver (DONE)\n",
    "    - Same Gold layer serves both batch + streaming data\n",
    "\n",
    "  THIS COMPLETES THE STREAMING PATH IN YOUR ARCHITECTURE!\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11377446-5826-47d9-984e-7e615690a91f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: azure-eventhub in /local_disk0/.ephemeral_nfs/envs/pythonEnv-8d2df315-1b6a-419f-8208-3ab3ad647e2f/lib/python3.12/site-packages (5.15.1)\nRequirement already satisfied: azure-core>=1.27.0 in /databricks/python3/lib/python3.12/site-packages (from azure-eventhub) (1.31.0)\nRequirement already satisfied: typing-extensions>=4.0.1 in /databricks/python3/lib/python3.12/site-packages (from azure-eventhub) (4.11.0)\nRequirement already satisfied: requests>=2.21.0 in /databricks/python3/lib/python3.12/site-packages (from azure-core>=1.27.0->azure-eventhub) (2.32.2)\nRequirement already satisfied: six>=1.11.0 in /usr/lib/python3/dist-packages (from azure-core>=1.27.0->azure-eventhub) (1.16.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.12/site-packages (from requests>=2.21.0->azure-core>=1.27.0->azure-eventhub) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.12/site-packages (from requests>=2.21.0->azure-core>=1.27.0->azure-eventhub) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /databricks/python3/lib/python3.12/site-packages (from requests>=2.21.0->azure-core>=1.27.0->azure-eventhub) (1.26.16)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.12/site-packages (from requests>=2.21.0->azure-core>=1.27.0->azure-eventhub) (2024.6.2)\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\nReading events from Event Hub...\nEvents received: 150\nWritten 150 events to Bronze streaming table.\n"
     ]
    }
   ],
   "source": [
    "# Install library\n",
    "%pip install azure-eventhub\n",
    "\n",
    "# ============================================================\n",
    "from azure.eventhub import EventHubConsumerClient\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import json\n",
    "import threading\n",
    "import time\n",
    "\n",
    "# Get connection string from Key Vault\n",
    "eh_conn_str = dbutils.secrets.get(scope=\"shopsmart-scope\", key=\"eventhub-connection-string\")\n",
    "EVENTHUB_NAME = \"eh-clickstream\"\n",
    "CONSUMER_GROUP = \"$Default\"\n",
    "\n",
    "print(\"Reading events from Event Hub...\")\n",
    "\n",
    "# Read events\n",
    "events_list = []\n",
    "def on_event(partition_context, event):\n",
    "    events_list.append(event.body_as_str())\n",
    "    partition_context.update_checkpoint(event)\n",
    "\n",
    "consumer = EventHubConsumerClient.from_connection_string(\n",
    "    conn_str=eh_conn_str,\n",
    "    consumer_group=CONSUMER_GROUP,\n",
    "    eventhub_name=EVENTHUB_NAME\n",
    ")\n",
    "\n",
    "def receive_events():\n",
    "    with consumer:\n",
    "        consumer.receive(on_event=on_event, starting_position=\"-1\")\n",
    "\n",
    "# Run in background for 15 seconds\n",
    "t = threading.Thread(target=receive_events, daemon=True)\n",
    "t.start()\n",
    "time.sleep(15)\n",
    "\n",
    "print(\"Events received: \" + str(len(events_list)))\n",
    "\n",
    "if len(events_list) > 0:\n",
    "    # Parse and Create DataFrame\n",
    "    parsed_events = []\n",
    "    for e in events_list:\n",
    "        try:\n",
    "            d = json.loads(e)\n",
    "            if \"geo_location\" in d:\n",
    "                d[\"geo_city\"] = d[\"geo_location\"].get(\"city\")\n",
    "                d[\"geo_country\"] = d[\"geo_location\"].get(\"country\")\n",
    "                del d[\"geo_location\"]\n",
    "            parsed_events.append(d)\n",
    "        except: pass\n",
    "\n",
    "    # Schema\n",
    "    schema = StructType([\n",
    "        StructField(\"event_id\", StringType(), True),\n",
    "        StructField(\"session_id\", StringType(), True),\n",
    "        StructField(\"customer_id\", StringType(), True),\n",
    "        StructField(\"event_type\", StringType(), True),\n",
    "        StructField(\"event_timestamp\", StringType(), True),\n",
    "        StructField(\"page_url\", StringType(), True),\n",
    "        StructField(\"product_id\", StringType(), True),\n",
    "        StructField(\"device_type\", StringType(), True),\n",
    "        StructField(\"browser\", StringType(), True),\n",
    "        StructField(\"os\", StringType(), True),\n",
    "        StructField(\"ip_address\", StringType(), True),\n",
    "        StructField(\"geo_city\", StringType(), True),\n",
    "        StructField(\"geo_country\", StringType(), True),\n",
    "        StructField(\"referrer\", StringType(), True),\n",
    "        StructField(\"search_query\", StringType(), True)\n",
    "    ])\n",
    "\n",
    "    df_stream = spark.createDataFrame(parsed_events, schema=schema) \\\n",
    "        .withColumn(\"_ingestion_source\", lit(\"event_hub\")) \\\n",
    "        .withColumn(\"_ingestion_timestamp\", current_timestamp())\n",
    "\n",
    "    # Write to Bronze\n",
    "    df_stream.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", True).save(BRONZE + \"/streaming_clickstream\")\n",
    "    print(\"Written \" + str(df_stream.count()) + \" events to Bronze streaming table.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e68a436-7fab-413f-ab0c-97b39fbf2534",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STREAMING SILVER COMPLETE\n  Path: abfss://silver@dlsshopsmartdev123.dfs.core.windows.net/streaming_clickstream\n  Rows: 200\n+---------------+-----+\n|   funnel_stage|count|\n+---------------+-----+\n|    1-Awareness|   61|\n|     2-Interest|   53|\n|3-Consideration|   57|\n|     4-Purchase|   16|\n|          Other|   13|\n+---------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 2: PROCESS STREAMING DATA - BRONZE TO SILVER\n",
    "# ============================================================\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Read streaming events from Bronze\n",
    "bronze_stream_path = BRONZE + \"/streaming_clickstream\"\n",
    "\n",
    "# Apply Silver Transformations\n",
    "df_stream = spark.read.format(\"delta\").load(bronze_stream_path)\n",
    "\n",
    "df_stream_silver = df_stream \\\n",
    "    .withColumn(\"event_id\", trim(col(\"event_id\"))) \\\n",
    "    .withColumn(\"event_type\", lower(trim(col(\"event_type\")))) \\\n",
    "    .withColumn(\"is_anonymous\", when(col(\"customer_id\").isNull(), lit(True)).otherwise(lit(False))) \\\n",
    "    .withColumn(\"event_timestamp\", to_timestamp(col(\"event_timestamp\"))) \\\n",
    "    .withColumn(\"funnel_stage\",\n",
    "        when(col(\"event_type\") == \"page_view\", lit(\"1-Awareness\"))\n",
    "        .when(col(\"event_type\") == \"product_view\", lit(\"2-Interest\"))\n",
    "        .when(col(\"event_type\").isin(\"add_to_cart\", \"search\"), lit(\"3-Consideration\"))\n",
    "        .when(col(\"event_type\") == \"checkout\", lit(\"4-Purchase\"))\n",
    "        .otherwise(lit(\"Other\"))) \\\n",
    "    .withColumn(\"_silver_processed_at\", current_timestamp())\n",
    "\n",
    "# Write to Silver\n",
    "silver_stream_path = SILVER + \"/streaming_clickstream\"\n",
    "df_stream_silver.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", True).save(silver_stream_path)\n",
    "\n",
    "print(\"STREAMING SILVER COMPLETE\")\n",
    "print(\"  Path: \" + silver_stream_path)\n",
    "print(\"  Rows: \" + str(df_stream_silver.count()))\n",
    "\n",
    "# Show Funnel Analysis\n",
    "df_stream_silver.groupBy(\"funnel_stage\").count().orderBy(\"funnel_stage\").show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04_Streaming_pipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}