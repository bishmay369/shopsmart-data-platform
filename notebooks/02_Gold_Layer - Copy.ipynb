{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "edb7af66-8bfd-485f-95c9-ad2e6e07ea5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 1: INFRASTRUCTURE CONNECTION & AUTHENTICATION\n",
    "# ============================================================\n",
    "# Compatible with: Shared Clusters / Unity Catalog\n",
    "# ============================================================\n",
    "\n",
    "# 1. Retrieve Service Principal credentials from Key Vault\n",
    "client_id     = dbutils.secrets.get(scope=\"shopsmart-scope\", key=\"datalake-sp-client-id\")\n",
    "client_secret = dbutils.secrets.get(scope=\"shopsmart-scope\", key=\"datalake-sp-client-secret\")\n",
    "tenant_id     = dbutils.secrets.get(scope=\"shopsmart-scope\", key=\"datalake-sp-tenant-id\")\n",
    "\n",
    "storage_account_name = \"dlsshopsmartdev123\"\n",
    "\n",
    "# 2. Configure Spark for OAuth 2.0 (this is the ONLY method needed)\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{storage_account_name}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{storage_account_name}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{storage_account_name}.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{storage_account_name}.dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{storage_account_name}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")\n",
    "\n",
    "# 3. Define base paths\n",
    "BRONZE = f\"abfss://bronze@{storage_account_name}.dfs.core.windows.net\"\n",
    "SILVER = f\"abfss://silver@{storage_account_name}.dfs.core.windows.net\"\n",
    "GOLD   = f\"abfss://gold@{storage_account_name}.dfs.core.windows.net\"\n",
    "\n",
    "print(f\"✅ Authentication configured for: {storage_account_name}\")\n",
    "print(f\"   \uD83D\uDCC1 Bronze: {BRONZE}\")\n",
    "print(f\"   \uD83D\uDCC1 Silver: {SILVER}\")\n",
    "print(f\"   \uD83D\uDCC1 Gold:   {GOLD}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf2a9688-0729-4724-b5c4-d934a90642a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 11: GOLD LAYER - dim_date (Date Dimension)\n",
    "# ============================================================\n",
    "#\n",
    "# WHAT IS A DATE DIMENSION?\n",
    "# -------------------------\n",
    "# A pre-built calendar table with one row per day.\n",
    "# Instead of calculating \"is this a weekend?\" or \n",
    "# \"what quarter is this?\" in every query, we calculate\n",
    "# it ONCE and store it.\n",
    "#\n",
    "# Every fact table joins to dim_date via a date key.\n",
    "# Example: fact_sales.order_date_key -> dim_date.date_key\n",
    "#\n",
    "# WHY NOT JUST USE THE DATE COLUMN DIRECTLY?\n",
    "# 1. Performance: pre-computed attributes avoid runtime functions\n",
    "# 2. Consistency: everyone uses the same fiscal year definition\n",
    "# 3. Filtering: \"Show Q4 2025\" is just WHERE quarter = 4\n",
    "# 4. Custom attributes: holidays, fiscal periods, pay days\n",
    "#    can't be derived from a raw date\n",
    "#\n",
    "# THIS IS ASKED IN EVERY DATA ENGINEERING INTERVIEW.\n",
    "# \"How would you design a date dimension?\" is a classic question.\n",
    "#\n",
    "# We generate dates from 2024-01-01 to 2026-12-31 (3 years)\n",
    "# to cover all historical and future data.\n",
    "# ============================================================\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 1: Generate a sequence of dates\n",
    "# ----------------------------------------------------------\n",
    "# HOW THIS WORKS:\n",
    "# 1. sequence() creates an array of dates from start to end\n",
    "#    sequence(2024-01-01, 2026-12-31) = [2024-01-01, 2024-01-02, ...]\n",
    "# 2. explode() converts the array into individual rows\n",
    "#    One array of 1096 dates -> 1096 rows\n",
    "#\n",
    "# This is a common Spark pattern for generating reference data\n",
    "# without needing an external source.\n",
    "\n",
    "df_dates = spark.sql(\"\"\"\n",
    "    SELECT explode(sequence(\n",
    "        to_date('2024-01-01'),\n",
    "        to_date('2026-12-31'),\n",
    "        interval 1 day\n",
    "    )) as date\n",
    "\"\"\")\n",
    "\n",
    "total_dates = df_dates.count()\n",
    "print(\"STEP 1: Generated \" + str(total_dates) + \" dates (2024-2026)\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 2: Build all date attributes\n",
    "# ----------------------------------------------------------\n",
    "# Each attribute serves a specific analytics purpose:\n",
    "#\n",
    "# date_key (int): 20250115 format\n",
    "#   Used as JOIN key with fact tables.\n",
    "#   Integer keys are faster than date JOINs.\n",
    "#   Format: YYYYMMDD\n",
    "#\n",
    "# year, quarter, month, day: Basic components\n",
    "#   \"Revenue by quarter\" -> GROUP BY quarter\n",
    "#\n",
    "# month_name, day_name: Human-readable labels\n",
    "#   Dashboards show \"January\" not \"1\"\n",
    "#\n",
    "# week_of_year: For weekly reporting\n",
    "#   \"Week-over-week growth\" needs this\n",
    "#\n",
    "# is_weekend: Saturday/Sunday flag\n",
    "#   \"Weekend vs weekday sales patterns\"\n",
    "#   dayofweek returns 1=Sunday, 7=Saturday\n",
    "#\n",
    "# is_month_start, is_month_end: Boundary flags\n",
    "#   Financial reporting often focuses on month boundaries\n",
    "#\n",
    "# quarter_label: \"Q1-2025\" format\n",
    "#   Clean label for dashboard filters\n",
    "#\n",
    "# day_of_year: 1-365\n",
    "#   Useful for year-over-year comparison at the day level\n",
    "\n",
    "df_dim_date = df_dates.select(\n",
    "    date_format(col(\"date\"), \"yyyyMMdd\").cast(\"int\").alias(\"date_key\"),\n",
    "    col(\"date\").alias(\"full_date\"),\n",
    "    year(\"date\").alias(\"year\"),\n",
    "    quarter(\"date\").alias(\"quarter\"),\n",
    "    month(\"date\").alias(\"month\"),\n",
    "    dayofmonth(\"date\").alias(\"day\"),\n",
    "    date_format(\"date\", \"MMMM\").alias(\"month_name\"),\n",
    "    date_format(\"date\", \"MMM\").alias(\"month_short\"),\n",
    "    dayofweek(\"date\").alias(\"day_of_week\"),\n",
    "    date_format(\"date\", \"EEEE\").alias(\"day_name\"),\n",
    "    date_format(\"date\", \"EEE\").alias(\"day_short\"),\n",
    "    weekofyear(\"date\").alias(\"week_of_year\"),\n",
    "    dayofyear(\"date\").alias(\"day_of_year\"),\n",
    "    when(dayofweek(\"date\").isin(1, 7), lit(True)).otherwise(lit(False)).alias(\"is_weekend\"),\n",
    "    when(dayofmonth(\"date\") == 1, lit(True)).otherwise(lit(False)).alias(\"is_month_start\"),\n",
    "    when(col(\"date\") == last_day(\"date\"), lit(True)).otherwise(lit(False)).alias(\"is_month_end\"),\n",
    "    when(month(\"date\").isin(11, 12, 1), lit(True)).otherwise(lit(False)).alias(\"is_holiday_season\"),\n",
    "    concat(lit(\"Q\"), quarter(\"date\").cast(\"string\"), lit(\"-\"), year(\"date\").cast(\"string\")).alias(\"quarter_label\"),\n",
    "    concat(date_format(\"date\", \"MMM\"), lit(\"-\"), year(\"date\").cast(\"string\")).alias(\"month_year_label\"),\n",
    "    when(month(\"date\") <= 6, lit(1)).otherwise(lit(2)).alias(\"half_year\"),\n",
    "    when(month(\"date\") <= 6, lit(\"H1\")).otherwise(lit(\"H2\")).alias(\"half_year_label\")\n",
    ")\n",
    "\n",
    "print(\"STEP 2: Date attributes built - \" + str(len(df_dim_date.columns)) + \" columns\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 3: Write to Gold layer\n",
    "# ----------------------------------------------------------\n",
    "gold_dim_date_path = GOLD + \"/dim_date\"\n",
    "\n",
    "df_dim_date.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", True) \\\n",
    "    .save(gold_dim_date_path)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 4: Verify\n",
    "# ----------------------------------------------------------\n",
    "df_verify = spark.read.format(\"delta\").load(gold_dim_date_path)\n",
    "final_count = df_verify.count()\n",
    "\n",
    "print(\"\")\n",
    "print(\"=\" * 65)\n",
    "print(\"GOLD dim_date - COMPLETE\")\n",
    "print(\"=\" * 65)\n",
    "print(\"  Total dates:  \" + str(final_count) + \" rows\")\n",
    "print(\"  Date range:   2024-01-01 to 2026-12-31\")\n",
    "print(\"  Columns:      \" + str(len(df_verify.columns)))\n",
    "print(\"  Path:         \" + gold_dim_date_path)\n",
    "\n",
    "print(\"\\n  Schema:\")\n",
    "df_verify.printSchema()\n",
    "\n",
    "print(\"\\n  Sample (first 5 days of 2025):\")\n",
    "df_verify.filter(\n",
    "    (col(\"year\") == 2025) & (col(\"month\") == 1) & (col(\"day\") <= 5)\n",
    ").orderBy(\"full_date\").show(truncate=False)\n",
    "\n",
    "print(\"\\n  Weekend vs Weekday count:\")\n",
    "df_verify.groupBy(\"is_weekend\").count().show()\n",
    "\n",
    "print(\"\\n  Records per year:\")\n",
    "df_verify.groupBy(\"year\").count().orderBy(\"year\").show()\n",
    "\n",
    "print(\"\\n  Quarter labels sample:\")\n",
    "df_verify.select(\"quarter_label\").distinct().orderBy(\"quarter_label\").show(12)\n",
    "\n",
    "print(\"[DONE] Gold dim_date complete!\")\n",
    "print(\"[NEXT] Cell 12 - Gold dim_customer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30d6ab00-dec7-465a-9a8b-918e3b2b650d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 12: GOLD LAYER - dim_customer (Customer Dimension)\n",
    "# ============================================================\n",
    "#\n",
    "# WHAT IS dim_customer?\n",
    "# ---------------------\n",
    "# The customer dimension describes WHO made the purchase.\n",
    "# Every row in fact_sales will JOIN to dim_customer to answer:\n",
    "#   \"Revenue by loyalty tier\"\n",
    "#   \"Orders by age group\"\n",
    "#   \"Customer count by state\"\n",
    "#\n",
    "# SOURCE: silver/customers (already cleaned & PII masked)\n",
    "#\n",
    "# WHAT WE ADD IN GOLD:\n",
    "# Gold layer is about BUSINESS PERSPECTIVE, not cleaning.\n",
    "# We select only the columns analysts need and add\n",
    "# surrogate keys.\n",
    "#\n",
    "# SURROGATE KEY vs NATURAL KEY:\n",
    "# - Natural key: customer_id (\"CUST001\") - from source system\n",
    "# - Surrogate key: customer_sk (1, 2, 3...) - generated by us\n",
    "#\n",
    "# WHY SURROGATE KEYS?\n",
    "# 1. Performance: integer JOINs faster than string JOINs\n",
    "# 2. Independence: if source changes CUST001 to C-001, \n",
    "#    our surrogate key stays the same\n",
    "# 3. History: enables SCD Type 2 (tracking changes over time)\n",
    "# 4. Standard practice: every data warehouse uses them\n",
    "#\n",
    "# SCD TYPE 2 (Slowly Changing Dimension):\n",
    "# When a customer moves from \"Silver\" to \"Gold\" loyalty tier,\n",
    "# we want to keep BOTH versions:\n",
    "#   customer_sk=1, CUST001, Silver, effective 2024-01-01 to 2025-06-01\n",
    "#   customer_sk=2, CUST001, Gold,   effective 2025-06-01 to 9999-12-31\n",
    "#\n",
    "# This lets us analyze: \"What was the customer's tier WHEN \n",
    "# they placed that order in March?\" — historical accuracy.\n",
    "#\n",
    "# For this project, we implement a SIMPLIFIED SCD Type 2\n",
    "# (single version per customer since we have point-in-time data).\n",
    "# In production, you'd run this incrementally with MERGE.\n",
    "# ============================================================\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 1: Read Silver Customers\n",
    "# ----------------------------------------------------------\n",
    "df_cust_silver = spark.read.format(\"delta\").load(SILVER + \"/customers\")\n",
    "\n",
    "silver_count = df_cust_silver.count()\n",
    "print(\"STEP 1: Silver Customers read - \" + str(silver_count) + \" rows\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 2: Build dim_customer with surrogate key\n",
    "# ----------------------------------------------------------\n",
    "# monotonically_increasing_id() generates unique IDs.\n",
    "# NOTE: These IDs are NOT sequential (1, 2, 3...).\n",
    "# They're globally unique across partitions.\n",
    "# For a true sequential key, we use row_number() instead.\n",
    "#\n",
    "# WHY row_number() OVER orderBy(customer_id)?\n",
    "# - Guarantees sequential: 1, 2, 3, 4, ...\n",
    "# - Deterministic: same input always gives same keys\n",
    "# - Sorted: customer_sk order matches customer_id order\n",
    "# This is important for debugging and testing.\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "window_sk = Window.orderBy(\"customer_id\")\n",
    "\n",
    "df_dim_customer = df_cust_silver \\\n",
    "    .withColumn(\"customer_sk\", row_number().over(window_sk)) \\\n",
    "    .select(\n",
    "        col(\"customer_sk\"),\n",
    "        col(\"customer_id\"),\n",
    "        col(\"first_name\"),\n",
    "        col(\"last_name\"),\n",
    "        col(\"first_name_initial\"),\n",
    "        col(\"last_name_initial\"),\n",
    "        col(\"email_hash\"),\n",
    "        col(\"email_domain\"),\n",
    "        col(\"phone_masked\"),\n",
    "        col(\"gender\"),\n",
    "        col(\"date_of_birth\"),\n",
    "        col(\"age\"),\n",
    "        col(\"age_group\"),\n",
    "        col(\"loyalty_tier\"),\n",
    "        col(\"registration_date\"),\n",
    "        col(\"customer_tenure_days\"),\n",
    "        col(\"tenure_category\"),\n",
    "        col(\"address_city\"),\n",
    "        col(\"address_state\"),\n",
    "        col(\"address_zip\"),\n",
    "        col(\"address_country\"),\n",
    "        col(\"pref_categories\"),\n",
    "        col(\"pref_communication\"),\n",
    "        col(\"has_email\"),\n",
    "        # SCD Type 2 columns\n",
    "        col(\"registration_date\").alias(\"effective_start_date\"),\n",
    "        to_date(lit(\"9999-12-31\")).alias(\"effective_end_date\"),\n",
    "        lit(True).alias(\"is_current\"),\n",
    "        current_timestamp().alias(\"_gold_processed_at\"),\n",
    "        lit(\"1.0\").alias(\"_gold_version\")\n",
    "    )\n",
    "\n",
    "print(\"STEP 2: dim_customer built with surrogate key and SCD2 columns\")\n",
    "print(\"  Columns: \" + str(len(df_dim_customer.columns)))\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 3: Write to Gold layer\n",
    "# ----------------------------------------------------------\n",
    "gold_dim_customer_path = GOLD + \"/dim_customer\"\n",
    "\n",
    "df_dim_customer.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", True) \\\n",
    "    .save(gold_dim_customer_path)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 4: Verify\n",
    "# ----------------------------------------------------------\n",
    "df_verify = spark.read.format(\"delta\").load(gold_dim_customer_path)\n",
    "final_count = df_verify.count()\n",
    "\n",
    "print(\"\")\n",
    "print(\"=\" * 65)\n",
    "print(\"GOLD dim_customer - COMPLETE\")\n",
    "print(\"=\" * 65)\n",
    "print(\"  Source (Silver):  \" + str(silver_count) + \" rows\")\n",
    "print(\"  Final Gold:       \" + str(final_count) + \" rows\")\n",
    "print(\"  Columns:          \" + str(len(df_verify.columns)))\n",
    "print(\"  Path:             \" + gold_dim_customer_path)\n",
    "\n",
    "print(\"\\n  Schema:\")\n",
    "df_verify.printSchema()\n",
    "\n",
    "# Show surrogate key assignment\n",
    "print(\"\\n  Surrogate key sample:\")\n",
    "df_verify.select(\n",
    "    \"customer_sk\", \"customer_id\", \"first_name\", \"last_name\",\n",
    "    \"gender\", \"age_group\", \"loyalty_tier\"\n",
    ").orderBy(\"customer_sk\").show(5, truncate=False)\n",
    "\n",
    "# Show SCD2 columns\n",
    "print(\"\\n  SCD Type 2 columns:\")\n",
    "df_verify.select(\n",
    "    \"customer_sk\", \"customer_id\", \"loyalty_tier\",\n",
    "    \"effective_start_date\", \"effective_end_date\", \"is_current\"\n",
    ").orderBy(\"customer_sk\").show(5, truncate=False)\n",
    "\n",
    "# Show geographic distribution\n",
    "print(\"\\n  Top 10 states by customer count:\")\n",
    "df_verify.groupBy(\"address_state\").count().orderBy(desc(\"count\")).show(10)\n",
    "\n",
    "# Loyalty by age group\n",
    "print(\"\\n  Loyalty tier by age group:\")\n",
    "df_verify.groupBy(\"age_group\", \"loyalty_tier\") \\\n",
    "    .count() \\\n",
    "    .orderBy(\"age_group\", \"loyalty_tier\") \\\n",
    "    .show(25)\n",
    "\n",
    "print(\"[DONE] Gold dim_customer complete!\")\n",
    "print(\"[NEXT] Cell 13 - Gold dim_product\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "38348b02-6bfc-48a3-8038-f09b1488dc2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 13: GOLD LAYER - dim_product (Product Dimension)\n",
    "# ============================================================\n",
    "#\n",
    "# WHAT IS dim_product?\n",
    "# --------------------\n",
    "# Describes WHAT was sold. Every fact_sales row joins here.\n",
    "# Business questions this enables:\n",
    "#   \"Revenue by category\" \n",
    "#   \"Top 10 brands by sales\"\n",
    "#   \"Average margin by price tier\"\n",
    "#   \"Products to discontinue (low rating + low sales)\"\n",
    "#\n",
    "# SOURCE: silver/products (already cleaned, attributes flattened)\n",
    "#\n",
    "# WHAT WE ADD IN GOLD:\n",
    "# - Surrogate key (product_sk)\n",
    "# - Only business-relevant columns (drop technical metadata)\n",
    "# - Consistent column ordering (keys first, then attributes)\n",
    "#\n",
    "# DESIGN PRINCIPLE:\n",
    "# Dimension tables should be WIDE (many columns) but SHORT \n",
    "# (few rows). Our dim_product has 50 rows and ~25 columns.\n",
    "# This is typical - a retail company might have 50,000 products\n",
    "# with 30+ attributes each.\n",
    "#\n",
    "# The \"width\" of dimensions is what makes star schema powerful.\n",
    "# One JOIN to dim_product gives you access to category, brand,\n",
    "# price tier, rating, margin - all in one hop.\n",
    "# ============================================================\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 1: Read Silver Products\n",
    "# ----------------------------------------------------------\n",
    "df_prod_silver = spark.read.format(\"delta\").load(SILVER + \"/products\")\n",
    "\n",
    "silver_count = df_prod_silver.count()\n",
    "print(\"STEP 1: Silver Products read - \" + str(silver_count) + \" rows\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 2: Build dim_product with surrogate key\n",
    "# ----------------------------------------------------------\n",
    "# Same row_number() approach as dim_customer.\n",
    "# Ordered by product_id for deterministic key assignment.\n",
    "\n",
    "window_sk = Window.orderBy(\"product_id\")\n",
    "\n",
    "df_dim_product = df_prod_silver \\\n",
    "    .withColumn(\"product_sk\", row_number().over(window_sk)) \\\n",
    "    .select(\n",
    "        # Keys (always first in a dimension)\n",
    "        col(\"product_sk\"),\n",
    "        col(\"product_id\"),\n",
    "        # Descriptive attributes\n",
    "        col(\"product_name\"),\n",
    "        col(\"category\"),\n",
    "        col(\"sub_category\"),\n",
    "        col(\"brand\"),\n",
    "        col(\"supplier_id\"),\n",
    "        # Price and cost\n",
    "        col(\"price\").alias(\"current_price\"),\n",
    "        col(\"cost_price\"),\n",
    "        col(\"profit_margin\"),\n",
    "        col(\"margin_pct\"),\n",
    "        col(\"price_tier\"),\n",
    "        # Product characteristics\n",
    "        col(\"weight_kg\"),\n",
    "        col(\"rating\"),\n",
    "        col(\"review_count\"),\n",
    "        col(\"rating_category\"),\n",
    "        col(\"is_active\"),\n",
    "        # Flattened attributes\n",
    "        col(\"attr_battery_life\"),\n",
    "        col(\"attr_colors\"),\n",
    "        col(\"attr_connectivity\"),\n",
    "        # Time attributes\n",
    "        col(\"created_at\").alias(\"product_created_at\"),\n",
    "        col(\"updated_at\").alias(\"product_updated_at\"),\n",
    "        col(\"product_age_days\"),\n",
    "        # Gold metadata\n",
    "        current_timestamp().alias(\"_gold_processed_at\"),\n",
    "        lit(\"1.0\").alias(\"_gold_version\")\n",
    "    )\n",
    "\n",
    "print(\"STEP 2: dim_product built - \" + str(len(df_dim_product.columns)) + \" columns\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 3: Write to Gold layer\n",
    "# ----------------------------------------------------------\n",
    "gold_dim_product_path = GOLD + \"/dim_product\"\n",
    "\n",
    "df_dim_product.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", True) \\\n",
    "    .save(gold_dim_product_path)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 4: Verify\n",
    "# ----------------------------------------------------------\n",
    "df_verify = spark.read.format(\"delta\").load(gold_dim_product_path)\n",
    "final_count = df_verify.count()\n",
    "\n",
    "print(\"\")\n",
    "print(\"=\" * 65)\n",
    "print(\"GOLD dim_product - COMPLETE\")\n",
    "print(\"=\" * 65)\n",
    "print(\"  Source (Silver):  \" + str(silver_count) + \" rows\")\n",
    "print(\"  Final Gold:       \" + str(final_count) + \" rows\")\n",
    "print(\"  Columns:          \" + str(len(df_verify.columns)))\n",
    "print(\"  Path:             \" + gold_dim_product_path)\n",
    "\n",
    "print(\"\\n  Schema:\")\n",
    "df_verify.printSchema()\n",
    "\n",
    "# Surrogate key sample\n",
    "print(\"\\n  Surrogate key sample:\")\n",
    "df_verify.select(\n",
    "    \"product_sk\", \"product_id\", \"product_name\",\n",
    "    \"category\", \"brand\", \"current_price\", \"price_tier\"\n",
    ").orderBy(\"product_sk\").show(5, truncate=False)\n",
    "\n",
    "# Category and margin analysis\n",
    "print(\"\\n  Category analysis:\")\n",
    "df_verify.groupBy(\"category\").agg(\n",
    "    count(\"*\").alias(\"products\"),\n",
    "    round(avg(\"current_price\"), 2).alias(\"avg_price\"),\n",
    "    round(avg(\"margin_pct\"), 2).alias(\"avg_margin_pct\"),\n",
    "    round(avg(\"rating\"), 2).alias(\"avg_rating\"),\n",
    "    sum(col(\"is_active\").cast(\"int\")).alias(\"active_count\")\n",
    ").orderBy(desc(\"products\")).show()\n",
    "\n",
    "# Price tier analysis\n",
    "print(\"\\n  Price tier analysis:\")\n",
    "df_verify.groupBy(\"price_tier\").agg(\n",
    "    count(\"*\").alias(\"products\"),\n",
    "    round(min(\"current_price\"), 2).alias(\"min_price\"),\n",
    "    round(max(\"current_price\"), 2).alias(\"max_price\"),\n",
    "    round(avg(\"margin_pct\"), 2).alias(\"avg_margin_pct\")\n",
    ").orderBy(\"min_price\").show()\n",
    "\n",
    "print(\"[DONE] Gold dim_product complete!\")\n",
    "print(\"[NEXT] Cell 14 - Gold fact_sales (THE MAIN FACT TABLE)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a788c7e-9fbf-4913-86fe-bfb806ef8249",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 14: GOLD LAYER - fact_sales (Main Fact Table)\n",
    "# ============================================================\n",
    "#\n",
    "# THIS IS THE HEART OF THE ENTIRE DATA PLATFORM.\n",
    "#\n",
    "# WHAT IS A FACT TABLE?\n",
    "# ---------------------\n",
    "# A fact table records BUSINESS EVENTS (things that happened).\n",
    "# Each row = one item sold in one order.\n",
    "# This is called the \"grain\" of the fact table.\n",
    "#\n",
    "# GRAIN = \"The most atomic level of detail in the fact table\"\n",
    "# Our grain: ONE ORDER ITEM (not one order!)\n",
    "#\n",
    "# WHY ORDER ITEM LEVEL (not order level)?\n",
    "# If we aggregate to order level, we lose product-level detail:\n",
    "#   - \"Which product sold the most?\" needs item-level\n",
    "#   - \"Revenue by category\" needs item + product JOIN\n",
    "#   - \"Average items per order\" needs item count per order\n",
    "#\n",
    "# Rule: Always choose the LOWEST useful grain. You can always\n",
    "# aggregate UP (items -> orders), but you can't disaggregate \n",
    "# DOWN (orders -> items) without the source data.\n",
    "#\n",
    "# HOW FACT TABLE CONNECTS TO DIMENSIONS:\n",
    "#\n",
    "#   dim_date -------- date_key -------- fact_sales\n",
    "#   dim_customer ---- customer_id ----- fact_sales  \n",
    "#   dim_product ----- product_id ------ fact_sales\n",
    "#\n",
    "# This is the STAR shape:\n",
    "#           dim_date\n",
    "#              |\n",
    "#   dim_customer -- fact_sales -- dim_product\n",
    "#\n",
    "# MEASURES (numeric values we analyze):\n",
    "#   quantity, unit_price, line_total, discount_amount,\n",
    "#   net_line_total, shipping_amount\n",
    "#\n",
    "# These are the numbers we SUM, AVG, COUNT in dashboards.\n",
    "#\n",
    "# DEGENERATE DIMENSIONS:\n",
    "#   order_id and item_id live in the fact table directly.\n",
    "#   They don't have their own dimension table because\n",
    "#   there's nothing more to describe about them.\n",
    "#   This is called a \"degenerate dimension\" - a dimension\n",
    "#   key without a dimension table.\n",
    "# ============================================================\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 1: Read Silver tables we need to JOIN\n",
    "# ----------------------------------------------------------\n",
    "df_orders = spark.read.format(\"delta\").load(SILVER + \"/orders\")\n",
    "df_items = spark.read.format(\"delta\").load(SILVER + \"/order_items\")\n",
    "\n",
    "orders_count = df_orders.count()\n",
    "items_count = df_items.count()\n",
    "print(\"STEP 1: Silver data read\")\n",
    "print(\"  Orders:      \" + str(orders_count) + \" rows\")\n",
    "print(\"  Order Items: \" + str(items_count) + \" rows\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 2: JOIN orders with order_items\n",
    "# ----------------------------------------------------------\n",
    "# WHY JOIN IN GOLD (not Silver)?\n",
    "# Silver tables are INDEPENDENT cleaned tables.\n",
    "# Gold is where we COMBINE them for analytics.\n",
    "#\n",
    "# JOIN TYPE: INNER JOIN\n",
    "# We only want items that belong to valid orders.\n",
    "# If an item has an order_id that doesn't exist in orders,\n",
    "# it's orphaned data and should not be in the fact table.\n",
    "#\n",
    "# WHAT COLUMNS COME FROM WHERE:\n",
    "# From orders: customer_id, order_date, order_status, \n",
    "#              payment_method, channel, shipping_amount\n",
    "# From items:  product_id, quantity, unit_price,\n",
    "#              discount_percent, line_total, net_line_total\n",
    "#\n",
    "# We prefix nothing because column names don't conflict\n",
    "# (we already designed Silver tables carefully).\n",
    "\n",
    "df_joined = df_orders.alias(\"o\").join(\n",
    "    df_items.alias(\"i\"),\n",
    "    col(\"o.order_id\") == col(\"i.order_id\"),\n",
    "    \"inner\"\n",
    ")\n",
    "\n",
    "joined_count = df_joined.count()\n",
    "print(\"STEP 2: Orders JOIN Order Items = \" + str(joined_count) + \" rows\")\n",
    "\n",
    "# Check for orphaned items (items without matching orders)\n",
    "orphan_items = df_items.join(df_orders, \"order_id\", \"left_anti\").count()\n",
    "print(\"  Orphaned items (no matching order): \" + str(orphan_items))\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 3: Build fact_sales\n",
    "# ----------------------------------------------------------\n",
    "# SELECT COLUMNS carefully:\n",
    "#\n",
    "# KEYS (for joining to dimensions):\n",
    "#   order_date_key -> joins to dim_date.date_key\n",
    "#   customer_id    -> joins to dim_customer.customer_id\n",
    "#   product_id     -> joins to dim_product.product_id\n",
    "#\n",
    "# DEGENERATE DIMENSIONS (identifiers without dim tables):\n",
    "#   order_id       -> for drill-down to specific order\n",
    "#   item_id        -> for uniqueness (primary key of fact)\n",
    "#\n",
    "# MEASURES (numbers we aggregate):\n",
    "#   quantity              -> SUM for total units sold\n",
    "#   unit_price            -> for price analysis\n",
    "#   line_total            -> quantity * unit_price (gross)\n",
    "#   discount_percent      -> for discount analysis\n",
    "#   discount_amount       -> SUM for total discounts given\n",
    "#   net_line_total        -> revenue after discount\n",
    "#   shipping_amount       -> allocated from order level\n",
    "#\n",
    "# ATTRIBUTES (for filtering/grouping):\n",
    "#   order_status, payment_method, channel\n",
    "#   item_status\n",
    "#\n",
    "# FLAGS (pre-computed boolean filters):\n",
    "#   is_cancelled, is_returned, has_discount\n",
    "#   is_weekend\n",
    "#\n",
    "# TIME ATTRIBUTES (from orders, for quick filtering):\n",
    "#   order_year, order_month, order_hour, day_name\n",
    "\n",
    "df_fact_sales = df_joined.select(\n",
    "    # Primary key\n",
    "    col(\"i.item_id\").alias(\"sales_key\"),\n",
    "    # Dimension keys\n",
    "    date_format(col(\"o.order_date\"), \"yyyyMMdd\").cast(\"int\").alias(\"order_date_key\"),\n",
    "    col(\"o.customer_id\"),\n",
    "    col(\"i.product_id\"),\n",
    "    # Degenerate dimensions\n",
    "    col(\"o.order_id\"),\n",
    "    col(\"i.item_id\"),\n",
    "    # Measures\n",
    "    col(\"i.quantity\"),\n",
    "    col(\"i.unit_price\"),\n",
    "    col(\"i.line_total\"),\n",
    "    col(\"i.discount_percent\"),\n",
    "    col(\"i.discount_amount\"),\n",
    "    col(\"i.net_line_total\"),\n",
    "    col(\"o.shipping_amount\"),\n",
    "    # Order attributes\n",
    "    col(\"o.order_date\"),\n",
    "    col(\"o.order_status\"),\n",
    "    col(\"o.payment_method\"),\n",
    "    col(\"o.channel\"),\n",
    "    col(\"i.item_status\"),\n",
    "    # Pre-computed flags\n",
    "    col(\"o.is_cancelled\"),\n",
    "    col(\"o.is_returned\"),\n",
    "    col(\"i.has_discount\"),\n",
    "    col(\"o.is_weekend\"),\n",
    "    col(\"o.has_free_shipping\"),\n",
    "    # Time parts (for quick filtering without joining dim_date)\n",
    "    col(\"o.order_year\"),\n",
    "    col(\"o.order_month\"),\n",
    "    col(\"o.order_day\"),\n",
    "    col(\"o.order_hour\"),\n",
    "    col(\"o.day_name\"),\n",
    "    # Metadata\n",
    "    current_timestamp().alias(\"_gold_processed_at\"),\n",
    "    lit(\"1.0\").alias(\"_gold_version\")\n",
    ")\n",
    "\n",
    "print(\"STEP 3: fact_sales built - \" + str(len(df_fact_sales.columns)) + \" columns\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 4: Write to Gold layer (partitioned by year, month)\n",
    "# ----------------------------------------------------------\n",
    "# WHY PARTITION fact_sales?\n",
    "# Fact tables are LARGE (millions of rows in production).\n",
    "# Partitioning by year/month means:\n",
    "#   - Query \"SELECT * WHERE order_year = 2025\" only reads \n",
    "#     2025 files, skipping all other years\n",
    "#   - This is called PARTITION PRUNING\n",
    "#   - Can reduce query time by 90%+\n",
    "#\n",
    "# WHY year + month (not just date)?\n",
    "# - Partitioning by date creates too many small files\n",
    "#   (365 partitions per year, each with few rows)\n",
    "# - year + month = 12 partitions per year (optimal)\n",
    "# - This is a common production pattern\n",
    "\n",
    "gold_fact_sales_path = GOLD + \"/fact_sales\"\n",
    "\n",
    "df_fact_sales.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"order_year\", \"order_month\") \\\n",
    "    .option(\"overwriteSchema\", True) \\\n",
    "    .save(gold_fact_sales_path)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 5: Verify and analyze\n",
    "# ----------------------------------------------------------\n",
    "df_verify = spark.read.format(\"delta\").load(gold_fact_sales_path)\n",
    "final_count = df_verify.count()\n",
    "\n",
    "print(\"\")\n",
    "print(\"=\" * 65)\n",
    "print(\"GOLD fact_sales - COMPLETE\")\n",
    "print(\"=\" * 65)\n",
    "print(\"  Orders (Silver):    \" + str(orders_count))\n",
    "print(\"  Items (Silver):     \" + str(items_count))\n",
    "print(\"  Joined rows:        \" + str(joined_count))\n",
    "print(\"  Final fact_sales:   \" + str(final_count) + \" rows\")\n",
    "print(\"  Columns:            \" + str(len(df_verify.columns)))\n",
    "print(\"  Partitioned by:     order_year, order_month\")\n",
    "print(\"  Path:               \" + gold_fact_sales_path)\n",
    "\n",
    "print(\"\\n  Schema:\")\n",
    "df_verify.printSchema()\n",
    "\n",
    "print(\"\\n  Sample data:\")\n",
    "df_verify.select(\n",
    "    \"sales_key\", \"order_date_key\", \"order_id\", \"customer_id\",\n",
    "    \"product_id\", \"quantity\", \"unit_price\", \"net_line_total\",\n",
    "    \"order_status\", \"channel\"\n",
    ").show(5, truncate=False)\n",
    "\n",
    "# ============================================================\n",
    "# BUSINESS KPI QUERIES (This is what the Gold layer enables!)\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 65)\n",
    "print(\"BUSINESS KPIs FROM fact_sales\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "# KPI 1: Total Revenue\n",
    "print(\"\\n  KPI 1: Revenue Summary\")\n",
    "df_verify.agg(\n",
    "    count(\"*\").alias(\"total_line_items\"),\n",
    "    countDistinct(\"order_id\").alias(\"total_orders\"),\n",
    "    countDistinct(\"customer_id\").alias(\"total_customers\"),\n",
    "    round(sum(\"net_line_total\"), 2).alias(\"total_revenue\"),\n",
    "    round(avg(\"net_line_total\"), 2).alias(\"avg_line_item_value\"),\n",
    "    round(sum(\"discount_amount\"), 2).alias(\"total_discounts\")\n",
    ").show(truncate=False)\n",
    "\n",
    "# KPI 2: Revenue by Channel\n",
    "print(\"\\n  KPI 2: Revenue by Channel\")\n",
    "df_verify.groupBy(\"channel\").agg(\n",
    "    countDistinct(\"order_id\").alias(\"orders\"),\n",
    "    round(sum(\"net_line_total\"), 2).alias(\"revenue\"),\n",
    "    round(avg(\"net_line_total\"), 2).alias(\"avg_item_value\")\n",
    ").orderBy(desc(\"revenue\")).show()\n",
    "\n",
    "# KPI 3: Revenue by Month\n",
    "print(\"\\n  KPI 3: Monthly Revenue Trend\")\n",
    "df_verify.groupBy(\"order_year\", \"order_month\").agg(\n",
    "    countDistinct(\"order_id\").alias(\"orders\"),\n",
    "    round(sum(\"net_line_total\"), 2).alias(\"revenue\")\n",
    ").orderBy(\"order_year\", \"order_month\").show(15)\n",
    "\n",
    "# KPI 4: Order Status Breakdown\n",
    "print(\"\\n  KPI 4: Order Status Breakdown\")\n",
    "df_verify.groupBy(\"order_status\").agg(\n",
    "    countDistinct(\"order_id\").alias(\"orders\"),\n",
    "    round(sum(\"net_line_total\"), 2).alias(\"revenue\")\n",
    ").orderBy(desc(\"orders\")).show()\n",
    "\n",
    "# KPI 5: Payment Method Analysis\n",
    "print(\"\\n  KPI 5: Payment Method Analysis\")\n",
    "df_verify.groupBy(\"payment_method\").agg(\n",
    "    countDistinct(\"order_id\").alias(\"orders\"),\n",
    "    round(sum(\"net_line_total\"), 2).alias(\"revenue\")\n",
    ").orderBy(desc(\"revenue\")).show()\n",
    "\n",
    "print(\"[DONE] Gold fact_sales complete!\")\n",
    "print(\"[NEXT] Cell 15 - Gold agg_daily_sales (Pre-aggregated metrics)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c791bd08-c398-4444-8f82-9bcfa3fa8d5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 15: GOLD LAYER - agg_daily_sales (Pre-Aggregated Table)\n",
    "# ============================================================\n",
    "#\n",
    "# WHAT IS A PRE-AGGREGATED TABLE?\n",
    "# --------------------------------\n",
    "# Instead of running heavy GROUP BY queries on 4780 rows \n",
    "# (or millions in production) every time a dashboard loads,\n",
    "# we pre-compute daily summaries.\n",
    "#\n",
    "# Dashboard query WITHOUT pre-aggregation:\n",
    "#   SELECT date, SUM(net_line_total) FROM fact_sales \n",
    "#   GROUP BY date\n",
    "#   -> Scans ALL rows every time (slow at scale)\n",
    "#\n",
    "# Dashboard query WITH pre-aggregation:\n",
    "#   SELECT * FROM agg_daily_sales WHERE date = '2025-01-15'\n",
    "#   -> Reads ONE pre-computed row (instant)\n",
    "#\n",
    "# WHY IS THIS IMPORTANT?\n",
    "# - Power BI dashboards refresh every 15-30 minutes\n",
    "# - Each refresh runs all queries\n",
    "# - Without pre-agg: 50 queries * millions of rows = slow\n",
    "# - With pre-agg: 50 queries * hundreds of rows = instant\n",
    "#\n",
    "# WHAT METRICS DO WE PRE-COMPUTE?\n",
    "# Everything a business executive looks at daily:\n",
    "#   - Total revenue, orders, customers\n",
    "#   - Average order value\n",
    "#   - Cancellation and return rates\n",
    "#   - Revenue by channel\n",
    "#   - Units sold\n",
    "#\n",
    "# GRAIN: One row per day per channel\n",
    "# This lets us analyze both:\n",
    "#   - Total daily metrics (GROUP BY date)\n",
    "#   - Channel comparison (GROUP BY date, channel)\n",
    "# ============================================================\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 1: Read fact_sales from Gold\n",
    "# ----------------------------------------------------------\n",
    "df_fact = spark.read.format(\"delta\").load(GOLD + \"/fact_sales\")\n",
    "fact_count = df_fact.count()\n",
    "print(\"STEP 1: fact_sales read - \" + str(fact_count) + \" rows\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 2: Build daily aggregation by channel\n",
    "# ----------------------------------------------------------\n",
    "# WHY GROUP BY date AND channel?\n",
    "# \n",
    "# If we only group by date:\n",
    "#   2025-01-15 | revenue: $50,000 | orders: 200\n",
    "#   (Can't drill down by channel)\n",
    "#\n",
    "# If we group by date + channel:\n",
    "#   2025-01-15 | web        | revenue: $15,000 | orders: 60\n",
    "#   2025-01-15 | mobile_app | revenue: $18,000 | orders: 75\n",
    "#   2025-01-15 | in_store   | revenue: $10,000 | orders: 40\n",
    "#   2025-01-15 | marketplace| revenue: $7,000  | orders: 25\n",
    "#   (Can see channel breakdown AND roll up to daily total)\n",
    "#\n",
    "# METRICS EXPLAINED:\n",
    "#   total_orders: COUNT DISTINCT order_id (not line items!)\n",
    "#     One order with 3 items = 1 order, not 3\n",
    "#\n",
    "#   total_customers: unique buyers that day\n",
    "#     Important for \"customer acquisition\" tracking\n",
    "#\n",
    "#   total_items_sold: SUM of quantity\n",
    "#     Physical units moved (logistics metric)\n",
    "#\n",
    "#   gross_revenue: before discounts\n",
    "#   total_discount: money given away\n",
    "#   net_revenue: what we actually earned\n",
    "#   avg_order_value: revenue / orders\n",
    "#     Key e-commerce KPI, target is to increase this\n",
    "#\n",
    "#   cancel_rate: % of orders cancelled\n",
    "#     High cancel rate = UX problem or fraud\n",
    "#\n",
    "#   return_rate: % of orders returned\n",
    "#     High return rate = product quality issue\n",
    "\n",
    "df_agg_daily = df_fact.groupBy(\n",
    "    col(\"order_year\"),\n",
    "    col(\"order_month\"),\n",
    "    col(\"order_day\"),\n",
    "    to_date(col(\"order_date\")).alias(\"order_date\"),\n",
    "    col(\"channel\")\n",
    ").agg(\n",
    "    # Volume metrics\n",
    "    countDistinct(\"order_id\").alias(\"total_orders\"),\n",
    "    countDistinct(\"customer_id\").alias(\"total_customers\"),\n",
    "    count(\"*\").alias(\"total_line_items\"),\n",
    "    sum(\"quantity\").alias(\"total_items_sold\"),\n",
    "    # Revenue metrics\n",
    "    round(sum(\"line_total\"), 2).alias(\"gross_revenue\"),\n",
    "    round(sum(\"discount_amount\"), 2).alias(\"total_discount\"),\n",
    "    round(sum(\"net_line_total\"), 2).alias(\"net_revenue\"),\n",
    "    round(sum(\"shipping_amount\"), 2).alias(\"total_shipping\"),\n",
    "    # Averages\n",
    "    round(avg(\"net_line_total\"), 2).alias(\"avg_item_value\"),\n",
    "    round(avg(\"quantity\"), 2).alias(\"avg_quantity_per_item\"),\n",
    "    # Status counts\n",
    "    countDistinct(when(col(\"order_status\") == \"CANCELLED\", col(\"order_id\"))).alias(\"cancelled_orders\"),\n",
    "    countDistinct(when(col(\"order_status\") == \"RETURNED\", col(\"order_id\"))).alias(\"returned_orders\"),\n",
    "    countDistinct(when(col(\"order_status\") == \"DELIVERED\", col(\"order_id\"))).alias(\"delivered_orders\"),\n",
    "    # Discount metrics\n",
    "    countDistinct(when(col(\"has_discount\") == True, col(\"order_id\"))).alias(\"orders_with_discount\"),\n",
    "    # Weekend flag\n",
    "    first(\"is_weekend\").alias(\"is_weekend\"),\n",
    "    first(\"day_name\").alias(\"day_name\")\n",
    ")\n",
    "\n",
    "# Add calculated rates\n",
    "df_agg_enriched = df_agg_daily \\\n",
    "    .withColumn(\"avg_order_value\",\n",
    "        when(col(\"total_orders\") > 0,\n",
    "            round(col(\"net_revenue\") / col(\"total_orders\"), 2))\n",
    "        .otherwise(lit(0.0))) \\\n",
    "    .withColumn(\"cancel_rate_pct\",\n",
    "        when(col(\"total_orders\") > 0,\n",
    "            round(col(\"cancelled_orders\") / col(\"total_orders\") * 100, 2))\n",
    "        .otherwise(lit(0.0))) \\\n",
    "    .withColumn(\"return_rate_pct\",\n",
    "        when(col(\"total_orders\") > 0,\n",
    "            round(col(\"returned_orders\") / col(\"total_orders\") * 100, 2))\n",
    "        .otherwise(lit(0.0))) \\\n",
    "    .withColumn(\"discount_rate_pct\",\n",
    "        when(col(\"total_orders\") > 0,\n",
    "            round(col(\"orders_with_discount\") / col(\"total_orders\") * 100, 2))\n",
    "        .otherwise(lit(0.0))) \\\n",
    "    .withColumn(\"_gold_processed_at\", current_timestamp()) \\\n",
    "    .withColumn(\"_gold_version\", lit(\"1.0\"))\n",
    "\n",
    "agg_count = df_agg_enriched.count()\n",
    "print(\"STEP 2: Daily aggregation built - \" + str(agg_count) + \" rows\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 3: Write to Gold\n",
    "# ----------------------------------------------------------\n",
    "gold_agg_path = GOLD + \"/agg_daily_sales\"\n",
    "\n",
    "df_agg_enriched.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", True) \\\n",
    "    .save(gold_agg_path)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 4: Verify and show executive dashboard metrics\n",
    "# ----------------------------------------------------------\n",
    "df_verify = spark.read.format(\"delta\").load(gold_agg_path)\n",
    "final_count = df_verify.count()\n",
    "\n",
    "print(\"\")\n",
    "print(\"=\" * 65)\n",
    "print(\"GOLD agg_daily_sales - COMPLETE\")\n",
    "print(\"=\" * 65)\n",
    "print(\"  Fact rows aggregated: \" + str(fact_count))\n",
    "print(\"  Aggregated rows:      \" + str(final_count))\n",
    "print(\"  Compression ratio:    \" + str(fact_count) + \" -> \" + str(final_count) + \" rows\")\n",
    "print(\"  Path:                 \" + gold_agg_path)\n",
    "\n",
    "print(\"\\n  Schema:\")\n",
    "df_verify.printSchema()\n",
    "\n",
    "print(\"\\n  Sample daily data:\")\n",
    "df_verify.select(\n",
    "    \"order_date\", \"channel\", \"total_orders\", \"total_customers\",\n",
    "    \"net_revenue\", \"avg_order_value\", \"cancel_rate_pct\"\n",
    ").orderBy(desc(\"order_date\")).show(10, truncate=False)\n",
    "\n",
    "# ============================================================\n",
    "# EXECUTIVE DASHBOARD QUERIES\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 65)\n",
    "print(\"EXECUTIVE DASHBOARD METRICS\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "# Overall platform metrics (roll up all channels)\n",
    "print(\"\\n  PLATFORM TOTALS:\")\n",
    "df_verify.agg(\n",
    "    sum(\"total_orders\").alias(\"total_orders\"),\n",
    "    sum(\"total_customers\").alias(\"total_customer_visits\"),\n",
    "    round(sum(\"net_revenue\"), 2).alias(\"total_net_revenue\"),\n",
    "    round(sum(\"total_discount\"), 2).alias(\"total_discounts_given\"),\n",
    "    round(sum(\"total_items_sold\")).alias(\"total_units_sold\"),\n",
    "    round(avg(\"avg_order_value\"), 2).alias(\"platform_avg_order_value\")\n",
    ").show(truncate=False)\n",
    "\n",
    "# Monthly trend (what executives look at first)\n",
    "print(\"\\n  MONTHLY REVENUE TREND:\")\n",
    "df_verify.groupBy(\"order_year\", \"order_month\").agg(\n",
    "    sum(\"total_orders\").alias(\"orders\"),\n",
    "    round(sum(\"net_revenue\"), 2).alias(\"revenue\"),\n",
    "    round(avg(\"avg_order_value\"), 2).alias(\"avg_order_value\"),\n",
    "    round(avg(\"cancel_rate_pct\"), 2).alias(\"avg_cancel_rate\")\n",
    ").orderBy(\"order_year\", \"order_month\").show(15)\n",
    "\n",
    "# Channel performance comparison\n",
    "print(\"\\n  CHANNEL PERFORMANCE:\")\n",
    "df_verify.groupBy(\"channel\").agg(\n",
    "    sum(\"total_orders\").alias(\"orders\"),\n",
    "    round(sum(\"net_revenue\"), 2).alias(\"revenue\"),\n",
    "    round(avg(\"avg_order_value\"), 2).alias(\"avg_order_value\"),\n",
    "    round(avg(\"cancel_rate_pct\"), 2).alias(\"avg_cancel_rate\"),\n",
    "    round(avg(\"return_rate_pct\"), 2).alias(\"avg_return_rate\")\n",
    ").orderBy(desc(\"revenue\")).show()\n",
    "\n",
    "# Weekend vs Weekday\n",
    "print(\"\\n  WEEKEND vs WEEKDAY:\")\n",
    "df_verify.groupBy(\"is_weekend\").agg(\n",
    "    sum(\"total_orders\").alias(\"orders\"),\n",
    "    round(sum(\"net_revenue\"), 2).alias(\"revenue\"),\n",
    "    round(avg(\"avg_order_value\"), 2).alias(\"avg_order_value\")\n",
    ").show()\n",
    "\n",
    "# ============================================================\n",
    "# FINAL GOLD LAYER SUMMARY\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 65)\n",
    "print(\"GOLD LAYER - ALL TABLES COMPLETE!\")\n",
    "print(\"=\" * 65)\n",
    "print(\"  1. gold/dim_date         - 1096 rows  (calendar dimension)\")\n",
    "print(\"  2. gold/dim_customer     - 500 rows   (customer dimension)\")\n",
    "print(\"  3. gold/dim_product      - 50 rows    (product dimension)\")\n",
    "print(\"  4. gold/fact_sales       - \" + str(fact_count) + \" rows  (main fact table)\")\n",
    "print(\"  5. gold/agg_daily_sales  - \" + str(final_count) + \" rows   (pre-aggregated)\")\n",
    "print(\"\")\n",
    "print(\"  STAR SCHEMA:\")\n",
    "print(\"        dim_date\")\n",
    "print(\"           |\")\n",
    "print(\"  dim_customer --- fact_sales --- dim_product\")\n",
    "print(\"                       |\")\n",
    "print(\"                 agg_daily_sales\")\n",
    "print(\"\")\n",
    "print(\"[NEXT] Cell 16+ : ML Models (Customer Segmentation, Forecasting)\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_Gold_Layer",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}