{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "366c2622-32c2-4fee-8f77-a950fbcf6f30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Authentication configured for: dlsshopsmartdev123\n   \uD83D\uDCC1 Bronze: abfss://bronze@dlsshopsmartdev123.dfs.core.windows.net\n   \uD83D\uDCC1 Silver: abfss://silver@dlsshopsmartdev123.dfs.core.windows.net\n   \uD83D\uDCC1 Gold:   abfss://gold@dlsshopsmartdev123.dfs.core.windows.net\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 1: INFRASTRUCTURE CONNECTION & AUTHENTICATION\n",
    "# ============================================================\n",
    "# Compatible with: Shared Clusters / Unity Catalog\n",
    "# ============================================================\n",
    "\n",
    "# 1. Retrieve Service Principal credentials from Key Vault\n",
    "client_id     = dbutils.secrets.get(scope=\"shopsmart-scope\", key=\"datalake-sp-client-id\")\n",
    "client_secret = dbutils.secrets.get(scope=\"shopsmart-scope\", key=\"datalake-sp-client-secret\")\n",
    "tenant_id     = dbutils.secrets.get(scope=\"shopsmart-scope\", key=\"datalake-sp-tenant-id\")\n",
    "\n",
    "storage_account_name = \"dlsshopsmartdev123\"\n",
    "\n",
    "# 2. Configure Spark for OAuth 2.0 (this is the ONLY method needed)\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{storage_account_name}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{storage_account_name}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{storage_account_name}.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{storage_account_name}.dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{storage_account_name}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")\n",
    "\n",
    "# 3. Define base paths\n",
    "BRONZE = f\"abfss://bronze@{storage_account_name}.dfs.core.windows.net\"\n",
    "SILVER = f\"abfss://silver@{storage_account_name}.dfs.core.windows.net\"\n",
    "GOLD   = f\"abfss://gold@{storage_account_name}.dfs.core.windows.net\"\n",
    "\n",
    "print(f\"✅ Authentication configured for: {storage_account_name}\")\n",
    "print(f\"   \uD83D\uDCC1 Bronze: {BRONZE}\")\n",
    "print(f\"   \uD83D\uDCC1 Silver: {SILVER}\")\n",
    "print(f\"   \uD83D\uDCC1 Gold:   {GOLD}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1c86f1d-74c5-4561-aa35-c37e6efbf5f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n\uD83D\uDD0D FINDING YOUR BRONZE DATA FILES\n============================================================\n\n\uD83D\uDD0E Searching for orders.csv...\n\n   ✅ Pattern 1: FOUND! (2000 rows)\n      Path: abfss://bronze@dlsshopsmartdev123.dfs.core.windows.net/source1_orders_pg/orders.csv\n\n\uD83C\uDFAF BASE PATH FOUND: abfss://bronze@dlsshopsmartdev123.dfs.core.windows.net\n\n============================================================\n\uD83D\uDCCB SHARE THIS OUTPUT — I'll give you the exact next step\n============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 2: FIND FILES & VERIFY BRONZE DATA ACCESS\n",
    "# ============================================================\n",
    "# Strategy: Use spark.read directly (works on shared clusters)\n",
    "# We'll try multiple path patterns to find where az cli put files\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"\uD83D\uDD0D FINDING YOUR BRONZE DATA FILES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Your az cli could have placed files at different paths.\n",
    "# Let's try all possibilities for orders.csv first\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "test_paths = [\n",
    "    (\"Pattern 1\", f\"{BRONZE}/source1_orders_pg/orders.csv\"),\n",
    "    (\"Pattern 2\", f\"{BRONZE}/output_data/source1_orders_pg/orders.csv\"),\n",
    "    (\"Pattern 3\", f\"{BRONZE}/orders.csv\"),\n",
    "    (\"Pattern 4\", f\"{BRONZE}/source1_orders_pg/\"),\n",
    "    (\"Pattern 5\", f\"{BRONZE}/output_data/\"),\n",
    "]\n",
    "\n",
    "found_base = None\n",
    "\n",
    "print(\"\\n\uD83D\uDD0E Searching for orders.csv...\\n\")\n",
    "\n",
    "for name, path in test_paths:\n",
    "    try:\n",
    "        if path.endswith(\".csv\"):\n",
    "            df = spark.read.option(\"header\", True).csv(path)\n",
    "            count = df.count()\n",
    "            print(f\"   ✅ {name}: FOUND! ({count} rows)\")\n",
    "            print(f\"      Path: {path}\")\n",
    "            found_base = path.replace(\"/orders.csv\", \"\").replace(\"/source1_orders_pg\", \"\")\n",
    "            break\n",
    "        else:\n",
    "            # Try reading any CSV in this directory\n",
    "            df = spark.read.option(\"header\", True).csv(path)\n",
    "            count = df.count()\n",
    "            print(f\"   ✅ {name}: FOUND directory! ({count} rows)\")\n",
    "            print(f\"      Path: {path}\")\n",
    "            found_base = path.replace(\"/source1_orders_pg/\", \"\").replace(\"/output_data/\", \"\")\n",
    "            break\n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ {name}: Not here\")\n",
    "        # print(f\"      {str(e)[:80]}\")\n",
    "\n",
    "if found_base:\n",
    "    print(f\"\\n\uD83C\uDFAF BASE PATH FOUND: {found_base}\")\n",
    "else:\n",
    "    print(f\"\\n⚠️ Could not find files automatically.\")\n",
    "    print(f\"   Let's try listing the bronze root...\")\n",
    "    \n",
    "    # Last resort: try to read anything from bronze\n",
    "    try:\n",
    "        # Try if files are directly in bronze root\n",
    "        df_test = spark.read.text(f\"{BRONZE}/*\")\n",
    "        print(f\"   Found {df_test.count()} lines of text in bronze root\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Bronze root also empty or inaccessible\")\n",
    "        print(f\"   Error: {str(e)[:100]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\uD83D\uDCCB SHARE THIS OUTPUT — I'll give you the exact next step\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8b072af-b140-4de1-a1e5-0fa5f5abf150",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\nREADING ALL BRONZE DATASETS\n=================================================================\nOrders loaded:       2000 rows\nOrder Items loaded:  4904 rows\nCustomers loaded:    500 rows\nProducts loaded:     50 rows\nClickstream loaded:  3000 rows\nInventory loaded:    150 rows\nPayments loaded:     2000 rows\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 3: BRONZE DATA VERIFICATION & PROFILING\n",
    "# ============================================================\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 3.1 Define all Bronze source paths\n",
    "# ----------------------------------------------------------\n",
    "BRONZE_ORDERS      = BRONZE + \"/source1_orders_pg/orders.csv\"\n",
    "BRONZE_ORDER_ITEMS = BRONZE + \"/source1_orders_pg/order_items.csv\"\n",
    "BRONZE_CUSTOMERS   = BRONZE + \"/source2_customers_api/customers.json\"\n",
    "BRONZE_PRODUCTS    = BRONZE + \"/source3_products_mongo/products.json\"\n",
    "BRONZE_CLICKSTREAM = BRONZE + \"/source4_clickstream_eventhub/clickstream.json\"\n",
    "BRONZE_INVENTORY   = BRONZE + \"/source5_inventory_csv/inventory.csv\"\n",
    "BRONZE_PAYMENTS    = BRONZE + \"/source6_payments_api/payments.json\"\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 3.2 Read ALL Bronze datasets\n",
    "# ----------------------------------------------------------\n",
    "print(\"=\" * 65)\n",
    "print(\"READING ALL BRONZE DATASETS\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "df_orders_raw = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(BRONZE_ORDERS)\n",
    "orders_count = df_orders_raw.count()\n",
    "print(\"Orders loaded:       \" + str(orders_count) + \" rows\")\n",
    "\n",
    "df_items_raw = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(BRONZE_ORDER_ITEMS)\n",
    "items_count = df_items_raw.count()\n",
    "print(\"Order Items loaded:  \" + str(items_count) + \" rows\")\n",
    "\n",
    "df_customers_raw = spark.read.option(\"multiLine\", True).json(BRONZE_CUSTOMERS)\n",
    "cust_count = df_customers_raw.count()\n",
    "print(\"Customers loaded:    \" + str(cust_count) + \" rows\")\n",
    "\n",
    "df_products_raw = spark.read.option(\"multiLine\", True).json(BRONZE_PRODUCTS)\n",
    "prod_count = df_products_raw.count()\n",
    "print(\"Products loaded:     \" + str(prod_count) + \" rows\")\n",
    "\n",
    "df_clicks_raw = spark.read.json(BRONZE_CLICKSTREAM)\n",
    "clicks_count = df_clicks_raw.count()\n",
    "print(\"Clickstream loaded:  \" + str(clicks_count) + \" rows\")\n",
    "\n",
    "df_inventory_raw = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(BRONZE_INVENTORY)\n",
    "inv_count = df_inventory_raw.count()\n",
    "print(\"Inventory loaded:    \" + str(inv_count) + \" rows\")\n",
    "\n",
    "df_payments_raw = spark.read.option(\"multiLine\", True).json(BRONZE_PAYMENTS)\n",
    "pay_count = df_payments_raw.count()\n",
    "print(\"Payments loaded:     \" + str(pay_count) + \" rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67734213-9bbb-48e1-959a-1c0dab80ac13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\nSOURCE 1a: ORDERS - Data Issues\n=================================================================\n  NULL order_status:     52 rows (2 pct)\n  Negative total_amount: 0 rows\n  Duplicate order_ids:   0 rows\n\n=================================================================\nSOURCE 2: CUSTOMERS - Data Issues\n=================================================================\n  NULL email:    53 rows (10 pct)\n  PII ALERT:     email has plaintext data\n  PII ALERT:     phone has plaintext data\n  NESTED STRUCT: address needs flattening\n  NESTED STRUCT: preferences needs flattening\n\n=================================================================\nSOURCE 3: PRODUCTS - Data Issues\n=================================================================\n  NULL prices:   0\n  Zero prices:   0\n  NESTED STRUCT: attributes needs flattening\n\n=================================================================\nSOURCE 5: INVENTORY - Data Issues\n=================================================================\n  Negative stock: 4 rows (2 pct)\n\n=================================================================\nSOURCE 4: CLICKSTREAM - Data Issues\n=================================================================\n  NULL customer_id: 626 rows (20 pct)\n  NESTED STRUCT:    geo_location needs flattening\n\n=================================================================\nSOURCE 6: PAYMENTS - Data Issues\n=================================================================\n  Risk categorization needed\n  Date standardization needed\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 3b: PROFILE EACH DATASET\n",
    "# ============================================================\n",
    "\n",
    "# ----- ORDERS -----\n",
    "print(\"=\" * 65)\n",
    "print(\"SOURCE 1a: ORDERS - Data Issues\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "null_status = df_orders_raw.filter(col(\"order_status\").isNull()).count()\n",
    "neg_amount = df_orders_raw.filter(col(\"total_amount\") < 0).count()\n",
    "distinct_orders = df_orders_raw.select(\"order_id\").distinct().count()\n",
    "dup_orders = orders_count - distinct_orders\n",
    "\n",
    "pct_null = str(int(null_status * 100 / orders_count))\n",
    "\n",
    "print(\"  NULL order_status:     \" + str(null_status) + \" rows (\" + pct_null + \" pct)\")\n",
    "print(\"  Negative total_amount: \" + str(neg_amount) + \" rows\")\n",
    "print(\"  Duplicate order_ids:   \" + str(dup_orders) + \" rows\")\n",
    "\n",
    "# ----- CUSTOMERS -----\n",
    "print(\"\\n\" + \"=\" * 65)\n",
    "print(\"SOURCE 2: CUSTOMERS - Data Issues\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "null_emails = df_customers_raw.filter(col(\"email\").isNull()).count()\n",
    "pct_email = str(int(null_emails * 100 / cust_count))\n",
    "\n",
    "print(\"  NULL email:    \" + str(null_emails) + \" rows (\" + pct_email + \" pct)\")\n",
    "print(\"  PII ALERT:     email has plaintext data\")\n",
    "print(\"  PII ALERT:     phone has plaintext data\")\n",
    "print(\"  NESTED STRUCT: address needs flattening\")\n",
    "print(\"  NESTED STRUCT: preferences needs flattening\")\n",
    "\n",
    "# ----- PRODUCTS -----\n",
    "print(\"\\n\" + \"=\" * 65)\n",
    "print(\"SOURCE 3: PRODUCTS - Data Issues\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "null_prices = df_products_raw.filter(col(\"price\").isNull()).count()\n",
    "zero_prices = df_products_raw.filter(col(\"price\") == 0).count()\n",
    "print(\"  NULL prices:   \" + str(null_prices))\n",
    "print(\"  Zero prices:   \" + str(zero_prices))\n",
    "print(\"  NESTED STRUCT: attributes needs flattening\")\n",
    "\n",
    "# ----- INVENTORY -----\n",
    "print(\"\\n\" + \"=\" * 65)\n",
    "print(\"SOURCE 5: INVENTORY - Data Issues\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "neg_stock = df_inventory_raw.filter(col(\"quantity_on_hand\") < 0).count()\n",
    "pct_neg = str(int(neg_stock * 100 / inv_count))\n",
    "\n",
    "print(\"  Negative stock: \" + str(neg_stock) + \" rows (\" + pct_neg + \" pct)\")\n",
    "\n",
    "# ----- CLICKSTREAM -----\n",
    "print(\"\\n\" + \"=\" * 65)\n",
    "print(\"SOURCE 4: CLICKSTREAM - Data Issues\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "null_cust_clicks = df_clicks_raw.filter(col(\"customer_id\").isNull()).count()\n",
    "pct_anon = str(int(null_cust_clicks * 100 / clicks_count))\n",
    "\n",
    "print(\"  NULL customer_id: \" + str(null_cust_clicks) + \" rows (\" + pct_anon + \" pct)\")\n",
    "print(\"  NESTED STRUCT:    geo_location needs flattening\")\n",
    "\n",
    "# ----- PAYMENTS -----\n",
    "print(\"\\n\" + \"=\" * 65)\n",
    "print(\"SOURCE 6: PAYMENTS - Data Issues\")\n",
    "print(\"=\" * 65)\n",
    "print(\"  Risk categorization needed\")\n",
    "print(\"  Date standardization needed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fce140e-7c29-40fe-bba3-476743a11a35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\nALL SCHEMAS\n=================================================================\n\n--- Orders Schema ---\nroot\n |-- order_id: string (nullable = true)\n |-- customer_id: string (nullable = true)\n |-- order_date: timestamp (nullable = true)\n |-- order_status: string (nullable = true)\n |-- total_amount: double (nullable = true)\n |-- discount_amount: double (nullable = true)\n |-- shipping_amount: double (nullable = true)\n |-- payment_method: string (nullable = true)\n |-- channel: string (nullable = true)\n |-- shipping_address_id: string (nullable = true)\n |-- created_at: timestamp (nullable = true)\n |-- updated_at: timestamp (nullable = true)\n\n\n--- Order Items Schema ---\nroot\n |-- item_id: string (nullable = true)\n |-- order_id: string (nullable = true)\n |-- product_id: string (nullable = true)\n |-- quantity: integer (nullable = true)\n |-- unit_price: double (nullable = true)\n |-- discount_percent: double (nullable = true)\n |-- item_status: string (nullable = true)\n |-- created_at: timestamp (nullable = true)\n\n\n--- Customers Schema ---\nroot\n |-- address: struct (nullable = true)\n |    |-- city: string (nullable = true)\n |    |-- country: string (nullable = true)\n |    |-- state: string (nullable = true)\n |    |-- street: string (nullable = true)\n |    |-- zip: string (nullable = true)\n |-- customer_id: string (nullable = true)\n |-- date_of_birth: string (nullable = true)\n |-- email: string (nullable = true)\n |-- first_name: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- last_name: string (nullable = true)\n |-- loyalty_tier: string (nullable = true)\n |-- phone: string (nullable = true)\n |-- preferences: struct (nullable = true)\n |    |-- categories: array (nullable = true)\n |    |    |-- element: string (containsNull = true)\n |    |-- communication: array (nullable = true)\n |    |    |-- element: string (containsNull = true)\n |-- registration_date: string (nullable = true)\n\n\n--- Products Schema ---\nroot\n |-- attributes: struct (nullable = true)\n |    |-- battery_life: string (nullable = true)\n |    |-- color: array (nullable = true)\n |    |    |-- element: string (containsNull = true)\n |    |-- connectivity: string (nullable = true)\n |-- brand: string (nullable = true)\n |-- category: string (nullable = true)\n |-- cost_price: double (nullable = true)\n |-- created_at: string (nullable = true)\n |-- is_active: boolean (nullable = true)\n |-- price: double (nullable = true)\n |-- product_id: string (nullable = true)\n |-- product_name: string (nullable = true)\n |-- rating: double (nullable = true)\n |-- review_count: long (nullable = true)\n |-- sub_category: string (nullable = true)\n |-- supplier_id: string (nullable = true)\n |-- updated_at: string (nullable = true)\n |-- weight_kg: double (nullable = true)\n\n\n--- Clickstream Schema ---\nroot\n |-- browser: string (nullable = true)\n |-- customer_id: string (nullable = true)\n |-- device_type: string (nullable = true)\n |-- event_id: string (nullable = true)\n |-- event_timestamp: string (nullable = true)\n |-- event_type: string (nullable = true)\n |-- geo_location: struct (nullable = true)\n |    |-- city: string (nullable = true)\n |    |-- country: string (nullable = true)\n |-- ip_address: string (nullable = true)\n |-- os: string (nullable = true)\n |-- page_url: string (nullable = true)\n |-- product_id: string (nullable = true)\n |-- referrer: string (nullable = true)\n |-- search_query: string (nullable = true)\n |-- session_id: string (nullable = true)\n\n\n--- Inventory Schema ---\nroot\n |-- product_id: string (nullable = true)\n |-- warehouse_id: string (nullable = true)\n |-- quantity_on_hand: integer (nullable = true)\n |-- quantity_reserved: integer (nullable = true)\n |-- reorder_point: integer (nullable = true)\n |-- reorder_quantity: integer (nullable = true)\n |-- last_restock_date: date (nullable = true)\n |-- snapshot_date: date (nullable = true)\n\n\n--- Payments Schema ---\nroot\n |-- amount: double (nullable = true)\n |-- card_type: string (nullable = true)\n |-- currency: string (nullable = true)\n |-- device_fingerprint: string (nullable = true)\n |-- gateway_response_code: string (nullable = true)\n |-- ip_address: string (nullable = true)\n |-- is_international: boolean (nullable = true)\n |-- order_id: string (nullable = true)\n |-- payment_method: string (nullable = true)\n |-- risk_score: long (nullable = true)\n |-- status: string (nullable = true)\n |-- transaction_id: string (nullable = true)\n |-- transaction_timestamp: string (nullable = true)\n\n\n=================================================================\nKEY DISTRIBUTIONS\n=================================================================\n\nOrders - Status distribution:\n+------------+-----+\n|order_status|count|\n+------------+-----+\n|     pending|  347|\n|   delivered|  334|\n|     shipped|  329|\n|    returned|  328|\n|   cancelled|  307|\n|   confirmed|  303|\n|        NULL|   52|\n+------------+-----+\n\n\nProducts - Category distribution:\n+-----------+-----+\n|   category|count|\n+-----------+-----+\n|       Home|   15|\n|    Fashion|   10|\n|Electronics|   10|\n|     Beauty|    9|\n|     Sports|    6|\n+-----------+-----+\n\n\nClickstream - Event type distribution:\n+----------------+-----+\n|      event_type|count|\n+----------------+-----+\n|       page_view|  544|\n|          search|  511|\n|        checkout|  498|\n|remove_from_cart|  492|\n|    product_view|  485|\n|     add_to_cart|  470|\n+----------------+-----+\n\n\nPayments - Status distribution:\n+-------+-----+\n| status|count|\n+-------+-----+\n|success| 1693|\n| failed|  307|\n+-------+-----+\n\n\nPayments - Risk score stats:\n+--------+--------+--------+\n|min_risk|max_risk|avg_risk|\n+--------+--------+--------+\n|       1|      99|  50.577|\n+--------+--------+--------+\n\n\nInventory - Negative stock samples:\n+----------+------------+----------------+-----------------+-------------+----------------+-----------------+-------------+\n|product_id|warehouse_id|quantity_on_hand|quantity_reserved|reorder_point|reorder_quantity|last_restock_date|snapshot_date|\n+----------+------------+----------------+-----------------+-------------+----------------+-----------------+-------------+\n|PROD020   |WH002       |-16             |0                |100          |250             |2026-02-16       |2026-02-17   |\n|PROD039   |WH002       |-28             |0                |100          |250             |2026-02-16       |2026-02-17   |\n|PROD042   |WH003       |-17             |0                |100          |250             |2026-02-16       |2026-02-17   |\n|PROD048   |WH002       |-16             |0                |100          |250             |2026-02-16       |2026-02-17   |\n+----------+------------+----------------+-----------------+-------------+----------------+-----------------+-------------+\n\n\n=================================================================\nSUMMARY\n=================================================================\n  Orders:       2000 rows | Issues: 52 null status, 0 dupes\n  Order Items:  4904 rows | Issues: type casting\n  Customers:    500 rows | Issues: 53 null emails, PII, nested\n  Products:     50 rows | Issues: nested attributes\n  Clickstream:  3000 rows | Issues: 626 null cust_id, nested\n  Inventory:    150 rows | Issues: 4 negative stock\n  Payments:     2000 rows | Issues: risk categorization\n\n[DONE] Bronze profiling complete!\n[NEXT] Cell 4 - Silver Layer Transformations\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 3c: SCHEMAS & SUMMARY\n",
    "# ============================================================\n",
    "\n",
    "# Show all schemas\n",
    "print(\"=\" * 65)\n",
    "print(\"ALL SCHEMAS\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "print(\"\\n--- Orders Schema ---\")\n",
    "df_orders_raw.printSchema()\n",
    "\n",
    "print(\"\\n--- Order Items Schema ---\")\n",
    "df_items_raw.printSchema()\n",
    "\n",
    "print(\"\\n--- Customers Schema ---\")\n",
    "df_customers_raw.printSchema()\n",
    "\n",
    "print(\"\\n--- Products Schema ---\")\n",
    "df_products_raw.printSchema()\n",
    "\n",
    "print(\"\\n--- Clickstream Schema ---\")\n",
    "df_clicks_raw.printSchema()\n",
    "\n",
    "print(\"\\n--- Inventory Schema ---\")\n",
    "df_inventory_raw.printSchema()\n",
    "\n",
    "print(\"\\n--- Payments Schema ---\")\n",
    "df_payments_raw.printSchema()\n",
    "\n",
    "# Show distributions using Spark (safer than print math)\n",
    "print(\"\\n\" + \"=\" * 65)\n",
    "print(\"KEY DISTRIBUTIONS\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "print(\"\\nOrders - Status distribution:\")\n",
    "df_orders_raw.groupBy(\"order_status\").count().orderBy(desc(\"count\")).show()\n",
    "\n",
    "print(\"\\nProducts - Category distribution:\")\n",
    "df_products_raw.groupBy(\"category\").count().orderBy(desc(\"count\")).show()\n",
    "\n",
    "print(\"\\nClickstream - Event type distribution:\")\n",
    "df_clicks_raw.groupBy(\"event_type\").count().orderBy(desc(\"count\")).show()\n",
    "\n",
    "print(\"\\nPayments - Status distribution:\")\n",
    "df_payments_raw.groupBy(\"status\").count().orderBy(desc(\"count\")).show()\n",
    "\n",
    "print(\"\\nPayments - Risk score stats:\")\n",
    "df_payments_raw.select(\n",
    "    min(\"risk_score\").alias(\"min_risk\"),\n",
    "    max(\"risk_score\").alias(\"max_risk\"),\n",
    "    avg(\"risk_score\").alias(\"avg_risk\")\n",
    ").show()\n",
    "\n",
    "print(\"\\nInventory - Negative stock samples:\")\n",
    "df_inventory_raw.filter(col(\"quantity_on_hand\") < 0).show(5, truncate=False)\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 65)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\" * 65)\n",
    "print(\"  Orders:       \" + str(orders_count) + \" rows | Issues: \" + str(null_status) + \" null status, \" + str(dup_orders) + \" dupes\")\n",
    "print(\"  Order Items:  \" + str(items_count) + \" rows | Issues: type casting\")\n",
    "print(\"  Customers:    \" + str(cust_count) + \" rows | Issues: \" + str(null_emails) + \" null emails, PII, nested\")\n",
    "print(\"  Products:     \" + str(prod_count) + \" rows | Issues: nested attributes\")\n",
    "print(\"  Clickstream:  \" + str(clicks_count) + \" rows | Issues: \" + str(null_cust_clicks) + \" null cust_id, nested\")\n",
    "print(\"  Inventory:    \" + str(inv_count) + \" rows | Issues: \" + str(neg_stock) + \" negative stock\")\n",
    "print(\"  Payments:     \" + str(pay_count) + \" rows | Issues: risk categorization\")\n",
    "print(\"\\n[DONE] Bronze profiling complete!\")\n",
    "print(\"[NEXT] Cell 4 - Silver Layer Transformations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "398adbf5-7001-4580-98d0-5c676937d75a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1: Bronze Orders read - 2000 rows\nSTEP 3: Data Quality Split\n  Good records:        1948\n  Quarantined records: 52\n  Quarantine saved to: bronze/quarantine/orders\nSTEP 4: Deduplication - 0 duplicates removed, 1948 remaining\n\n=================================================================\nSILVER ORDERS - COMPLETE\n=================================================================\n  Source (Bronze):     2000 rows\n  Quarantined:         52 rows (null status)\n  Duplicates removed:  0 rows\n  Final Silver:        1948 rows\n  Columns:             30\n  Format:              Delta Lake\n  Partitioned by:      order_year, order_month\n  Path:                abfss://silver@dlsshopsmartdev123.dfs.core.windows.net/orders\n\n  Schema:\nroot\n |-- order_id: string (nullable = true)\n |-- customer_id: string (nullable = true)\n |-- order_date: timestamp (nullable = true)\n |-- order_status: string (nullable = true)\n |-- total_amount: double (nullable = true)\n |-- discount_amount: double (nullable = true)\n |-- shipping_amount: double (nullable = true)\n |-- payment_method: string (nullable = true)\n |-- channel: string (nullable = true)\n |-- shipping_address_id: string (nullable = true)\n |-- created_at: timestamp (nullable = true)\n |-- updated_at: timestamp (nullable = true)\n |-- _bronze_loaded_at: timestamp (nullable = true)\n |-- _source_file: string (nullable = true)\n |-- net_amount: double (nullable = true)\n |-- gross_with_shipping: double (nullable = true)\n |-- discount_pct: double (nullable = true)\n |-- has_discount: boolean (nullable = true)\n |-- has_free_shipping: boolean (nullable = true)\n |-- order_year: integer (nullable = true)\n |-- order_month: integer (nullable = true)\n |-- order_day: integer (nullable = true)\n |-- order_hour: integer (nullable = true)\n |-- order_day_of_week: integer (nullable = true)\n |-- day_name: string (nullable = true)\n |-- is_weekend: boolean (nullable = true)\n |-- is_cancelled: boolean (nullable = true)\n |-- is_returned: boolean (nullable = true)\n |-- _silver_processed_at: timestamp (nullable = true)\n |-- _silver_version: string (nullable = true)\n\n\n  Sample data:\n+--------+-----------+-------------------+------------+------------+----------+------------+-----------+----------+--------+\n|order_id|customer_id|order_date         |order_status|total_amount|net_amount|discount_pct|channel    |is_weekend|day_name|\n+--------+-----------+-------------------+------------+------------+----------+------------+-----------+----------+--------+\n|ORD01996|CUST235    |2025-10-27 07:47:26|SHIPPED     |3706.02     |3520.72   |5.0         |marketplace|false     |Monday  |\n|ORD01994|CUST353    |2025-10-16 23:23:49|DELIVERED   |60.14       |57.13     |5.0         |marketplace|false     |Thursday|\n|ORD01989|CUST211    |2025-10-16 11:16:31|CANCELLED   |4649.7      |4417.21   |5.0         |web        |false     |Thursday|\n|ORD01986|CUST400    |2025-10-25 12:50:40|DELIVERED   |3773.04     |3584.39   |5.0         |web        |true      |Saturday|\n|ORD01964|CUST216    |2025-10-03 14:47:19|DELIVERED   |3213.71     |3053.02   |5.0         |web        |false     |Friday  |\n+--------+-----------+-------------------+------------+------------+----------+------------+-----------+----------+--------+\nonly showing top 5 rows\n\n  Status distribution:\n+------------+-----+\n|order_status|count|\n+------------+-----+\n|     PENDING|  347|\n|   DELIVERED|  334|\n|     SHIPPED|  329|\n|    RETURNED|  328|\n|   CANCELLED|  307|\n|   CONFIRMED|  303|\n+------------+-----+\n\n\n  Channel distribution:\n+-----------+-----+\n|    channel|count|\n+-----------+-----+\n|   in_store|  496|\n|marketplace|  494|\n| mobile_app|  486|\n|        web|  472|\n+-----------+-----+\n\n\n  Records by year-month:\n+----------+-----------+-----+\n|order_year|order_month|count|\n+----------+-----------+-----+\n|      2025|          2|   79|\n|      2025|          3|  172|\n|      2025|          4|  172|\n|      2025|          5|  177|\n|      2025|          6|  154|\n|      2025|          7|  143|\n|      2025|          8|  174|\n|      2025|          9|  151|\n|      2025|         10|  142|\n|      2025|         11|  153|\n|      2025|         12|  180|\n|      2026|          1|  167|\n|      2026|          2|   84|\n+----------+-----------+-----+\n\n[DONE] Silver Orders complete!\n[NEXT] Cell 5 - Silver Order Items\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 4: SILVER LAYER - ORDERS TRANSFORMATION\n",
    "# ============================================================\n",
    "# Bronze Issues Fixed:\n",
    "#   1. 52 NULL order_status -> quarantined\n",
    "#   2. Deduplication on order_id\n",
    "#   3. Data type standardization\n",
    "#   4. Derived columns (time parts, net_amount, flags)\n",
    "#   5. Written as Delta to Silver layer\n",
    "# ============================================================\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 1: Read Bronze Orders\n",
    "# ----------------------------------------------------------\n",
    "df_orders_bronze = spark.read \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"inferSchema\", True) \\\n",
    "    .csv(BRONZE + \"/source1_orders_pg/orders.csv\")\n",
    "\n",
    "bronze_count = df_orders_bronze.count()\n",
    "print(\"STEP 1: Bronze Orders read - \" + str(bronze_count) + \" rows\")\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 2: Add Bronze audit columns\n",
    "# ----------------------------------------------------------\n",
    "df_orders_bronze = df_orders_bronze \\\n",
    "    .withColumn(\"_bronze_loaded_at\", current_timestamp()) \\\n",
    "    .withColumn(\"_source_file\", lit(\"source1_orders_pg/orders.csv\"))\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 3: Separate GOOD vs QUARANTINE records\n",
    "# ----------------------------------------------------------\n",
    "# Rule: order_status must NOT be null (52 rows will be quarantined)\n",
    "df_orders_good = df_orders_bronze.filter(col(\"order_status\").isNotNull())\n",
    "df_orders_quarantine = df_orders_bronze.filter(col(\"order_status\").isNull())\n",
    "\n",
    "good_count = df_orders_good.count()\n",
    "quarantine_count = df_orders_quarantine.count()\n",
    "\n",
    "print(\"STEP 3: Data Quality Split\")\n",
    "print(\"  Good records:        \" + str(good_count))\n",
    "print(\"  Quarantined records: \" + str(quarantine_count))\n",
    "\n",
    "# Save quarantine records\n",
    "df_orders_quarantine \\\n",
    "    .withColumn(\"_quarantine_reason\", lit(\"NULL order_status\")) \\\n",
    "    .withColumn(\"_quarantine_timestamp\", current_timestamp()) \\\n",
    "    .write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", True) \\\n",
    "    .save(BRONZE + \"/quarantine/orders\")\n",
    "\n",
    "print(\"  Quarantine saved to: bronze/quarantine/orders\")\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 4: Deduplicate on order_id (keep latest by updated_at)\n",
    "# ----------------------------------------------------------\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "window_dedup = Window.partitionBy(\"order_id\").orderBy(col(\"updated_at\").desc())\n",
    "\n",
    "df_orders_deduped = df_orders_good \\\n",
    "    .withColumn(\"_row_num\", row_number().over(window_dedup)) \\\n",
    "    .filter(col(\"_row_num\") == 1) \\\n",
    "    .drop(\"_row_num\")\n",
    "\n",
    "dedup_count = df_orders_deduped.count()\n",
    "dupes_removed = good_count - dedup_count\n",
    "print(\"STEP 4: Deduplication - \" + str(dupes_removed) + \" duplicates removed, \" + str(dedup_count) + \" remaining\")\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 5: Standardize and clean columns\n",
    "# ----------------------------------------------------------\n",
    "df_orders_clean = df_orders_deduped \\\n",
    "    .withColumn(\"order_id\", trim(col(\"order_id\"))) \\\n",
    "    .withColumn(\"customer_id\", trim(col(\"customer_id\"))) \\\n",
    "    .withColumn(\"order_status\", upper(trim(col(\"order_status\")))) \\\n",
    "    .withColumn(\"payment_method\", lower(trim(col(\"payment_method\")))) \\\n",
    "    .withColumn(\"channel\", lower(trim(col(\"channel\")))) \\\n",
    "    .withColumn(\"total_amount\", col(\"total_amount\").cast(\"double\")) \\\n",
    "    .withColumn(\"discount_amount\", coalesce(col(\"discount_amount\").cast(\"double\"), lit(0.0))) \\\n",
    "    .withColumn(\"shipping_amount\", coalesce(col(\"shipping_amount\").cast(\"double\"), lit(0.0)))\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 6: Add derived business columns\n",
    "# ----------------------------------------------------------\n",
    "df_orders_enriched = df_orders_clean \\\n",
    "    .withColumn(\"net_amount\", \n",
    "        round(col(\"total_amount\") - col(\"discount_amount\"), 2)) \\\n",
    "    .withColumn(\"gross_with_shipping\", \n",
    "        round(col(\"total_amount\") + col(\"shipping_amount\"), 2)) \\\n",
    "    .withColumn(\"discount_pct\",\n",
    "        when(col(\"total_amount\") > 0,\n",
    "            round(col(\"discount_amount\") / col(\"total_amount\") * 100, 2))\n",
    "        .otherwise(lit(0.0))) \\\n",
    "    .withColumn(\"has_discount\",\n",
    "        when(col(\"discount_amount\") > 0, lit(True)).otherwise(lit(False))) \\\n",
    "    .withColumn(\"has_free_shipping\",\n",
    "        when(col(\"shipping_amount\") == 0, lit(True)).otherwise(lit(False))) \\\n",
    "    .withColumn(\"order_year\", year(\"order_date\")) \\\n",
    "    .withColumn(\"order_month\", month(\"order_date\")) \\\n",
    "    .withColumn(\"order_day\", dayofmonth(\"order_date\")) \\\n",
    "    .withColumn(\"order_hour\", hour(\"order_date\")) \\\n",
    "    .withColumn(\"order_day_of_week\", dayofweek(\"order_date\")) \\\n",
    "    .withColumn(\"day_name\", date_format(\"order_date\", \"EEEE\")) \\\n",
    "    .withColumn(\"is_weekend\",\n",
    "        when(dayofweek(\"order_date\").isin(1, 7), lit(True)).otherwise(lit(False))) \\\n",
    "    .withColumn(\"is_cancelled\",\n",
    "        when(col(\"order_status\") == \"CANCELLED\", lit(True)).otherwise(lit(False))) \\\n",
    "    .withColumn(\"is_returned\",\n",
    "        when(col(\"order_status\") == \"RETURNED\", lit(True)).otherwise(lit(False)))\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 7: Add Silver metadata\n",
    "# ----------------------------------------------------------\n",
    "df_orders_silver = df_orders_enriched \\\n",
    "    .withColumn(\"_silver_processed_at\", current_timestamp()) \\\n",
    "    .withColumn(\"_silver_version\", lit(\"1.0\"))\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 8: Write to Silver as Delta (partitioned by year, month)\n",
    "# ----------------------------------------------------------\n",
    "silver_orders_path = SILVER + \"/orders\"\n",
    "\n",
    "df_orders_silver.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"order_year\", \"order_month\") \\\n",
    "    .option(\"overwriteSchema\", True) \\\n",
    "    .save(silver_orders_path)\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 9: Verify\n",
    "# ----------------------------------------------------------\n",
    "df_verify = spark.read.format(\"delta\").load(silver_orders_path)\n",
    "final_count = df_verify.count()\n",
    "final_cols = len(df_verify.columns)\n",
    "\n",
    "print(\"\")\n",
    "print(\"=\" * 65)\n",
    "print(\"SILVER ORDERS - COMPLETE\")\n",
    "print(\"=\" * 65)\n",
    "print(\"  Source (Bronze):     \" + str(bronze_count) + \" rows\")\n",
    "print(\"  Quarantined:         \" + str(quarantine_count) + \" rows (null status)\")\n",
    "print(\"  Duplicates removed:  \" + str(dupes_removed) + \" rows\")\n",
    "print(\"  Final Silver:        \" + str(final_count) + \" rows\")\n",
    "print(\"  Columns:             \" + str(final_cols))\n",
    "print(\"  Format:              Delta Lake\")\n",
    "print(\"  Partitioned by:      order_year, order_month\")\n",
    "print(\"  Path:                \" + silver_orders_path)\n",
    "\n",
    "print(\"\\n  Schema:\")\n",
    "df_verify.printSchema()\n",
    "\n",
    "print(\"\\n  Sample data:\")\n",
    "df_verify.select(\n",
    "    \"order_id\", \"customer_id\", \"order_date\", \"order_status\",\n",
    "    \"total_amount\", \"net_amount\", \"discount_pct\",\n",
    "    \"channel\", \"is_weekend\", \"day_name\"\n",
    ").show(5, truncate=False)\n",
    "\n",
    "print(\"\\n  Status distribution:\")\n",
    "df_verify.groupBy(\"order_status\").count().orderBy(desc(\"count\")).show()\n",
    "\n",
    "print(\"\\n  Channel distribution:\")\n",
    "df_verify.groupBy(\"channel\").count().orderBy(desc(\"count\")).show()\n",
    "\n",
    "print(\"\\n  Records by year-month:\")\n",
    "df_verify.groupBy(\"order_year\", \"order_month\").count().orderBy(\"order_year\", \"order_month\").show(15)\n",
    "\n",
    "print(\"[DONE] Silver Orders complete!\")\n",
    "print(\"[NEXT] Cell 5 - Silver Order Items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d73e4aa9-e188-4de6-8038-3d6888d4e334",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1: Bronze Order Items read - 4904 rows\nSTEP 2: Deduplication - 0 duplicates removed\nSTEP 5: Quality check - 4904 good, 0 bad\n\n=================================================================\nSILVER ORDER ITEMS - COMPLETE\n=================================================================\n  Source (Bronze):    4904 rows\n  Duplicates removed: 0 rows\n  Quality rejected:   0 rows\n  Final Silver:       4904 rows\n  Path:               abfss://silver@dlsshopsmartdev123.dfs.core.windows.net/order_items\n\n  Schema:\nroot\n |-- item_id: string (nullable = true)\n |-- order_id: string (nullable = true)\n |-- product_id: string (nullable = true)\n |-- quantity: integer (nullable = true)\n |-- unit_price: double (nullable = true)\n |-- discount_percent: double (nullable = true)\n |-- item_status: string (nullable = true)\n |-- created_at: timestamp (nullable = true)\n |-- line_total: double (nullable = true)\n |-- discount_amount: double (nullable = true)\n |-- net_line_total: double (nullable = true)\n |-- has_discount: boolean (nullable = true)\n |-- _silver_processed_at: timestamp (nullable = true)\n |-- _silver_version: string (nullable = true)\n\n\n  Sample data:\n+-------------+--------+----------+--------+----------+----------------+----------+--------------+\n|item_id      |order_id|product_id|quantity|unit_price|discount_percent|line_total|net_line_total|\n+-------------+--------+----------+--------+----------+----------------+----------+--------------+\n|ITMORD00040-1|ORD00040|PROD044   |1       |586.88    |3.08            |586.88    |568.8         |\n|ITMORD00056-1|ORD00056|PROD017   |1       |735.92    |7.96            |735.92    |677.34        |\n|ITMORD00199-0|ORD00199|PROD009   |2       |824.47    |7.48            |1648.94   |1525.6        |\n|ITMORD00201-0|ORD00201|PROD044   |3       |586.88    |10.79           |1760.64   |1570.67       |\n|ITMORD00321-0|ORD00321|PROD013   |1       |686.48    |8.56            |686.48    |627.72        |\n+-------------+--------+----------+--------+----------+----------------+----------+--------------+\nonly showing top 5 rows\n\n  Item status distribution:\n+-----------+-----+\n|item_status|count|\n+-----------+-----+\n|  delivered| 1637|\n|     packed| 1635|\n|    shipped| 1632|\n+-----------+-----+\n\n\n  Top 10 products by quantity sold:\n+----------+---------+------------------+-----------+\n|product_id|total_qty|total_revenue     |order_count|\n+----------+---------+------------------+-----------+\n|PROD006   |247      |13693.370000000006|118        |\n|PROD034   |236      |67608.70000000001 |112        |\n|PROD048   |234      |118906.05000000002|112        |\n|PROD036   |231      |137091.19999999998|111        |\n|PROD014   |231      |3868.6799999999994|107        |\n|PROD039   |224      |75632.72999999997 |111        |\n|PROD012   |221      |122988.95000000006|118        |\n|PROD049   |219      |90434.68000000001 |107        |\n|PROD008   |217      |146972.66999999998|102        |\n|PROD044   |213      |115647.01999999995|112        |\n+----------+---------+------------------+-----------+\nonly showing top 10 rows\n[DONE] Silver Order Items complete!\n[NEXT] Cell 6 - Silver Customers (PII Masking)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 5: SILVER LAYER - ORDER ITEMS TRANSFORMATION\n",
    "# ============================================================\n",
    "# \n",
    "# WHAT IS THIS TABLE?\n",
    "# -------------------\n",
    "# Order Items is the \"line item\" detail of each order.\n",
    "# One order can have multiple items (1 order -> many items).\n",
    "# Example: Order ORD001 has 3 items (headphones, cable, case)\n",
    "#\n",
    "# This is called a \"parent-child\" relationship:\n",
    "#   orders (parent) --< order_items (child)\n",
    "#   Linked by: order_id\n",
    "#\n",
    "# WHY IS THIS IMPORTANT?\n",
    "# ----------------------\n",
    "# In the Gold layer, this becomes our FACT TABLE (fact_sales).\n",
    "# Every analytics query (revenue, quantity, top products) \n",
    "# needs this data at the line-item level.\n",
    "#\n",
    "# BRONZE ISSUES TO FIX:\n",
    "# - Possible duplicate item_ids\n",
    "# - Need derived columns (line_total, discount_amount)\n",
    "# - Data type standardization\n",
    "# ============================================================\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 1: Read Bronze Order Items\n",
    "# ----------------------------------------------------------\n",
    "# WHY inferSchema=True?\n",
    "# Without it, all columns come as strings.\n",
    "# With it, Spark automatically detects:\n",
    "#   quantity -> integer\n",
    "#   unit_price -> double\n",
    "#   created_at -> timestamp\n",
    "# This saves us manual casting for most columns.\n",
    "\n",
    "df_items_bronze = spark.read \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"inferSchema\", True) \\\n",
    "    .csv(BRONZE + \"/source1_orders_pg/order_items.csv\")\n",
    "\n",
    "bronze_count = df_items_bronze.count()\n",
    "print(\"STEP 1: Bronze Order Items read - \" + str(bronze_count) + \" rows\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 2: Deduplicate on item_id\n",
    "# ----------------------------------------------------------\n",
    "# WHY DEDUPLICATE?\n",
    "# In real production systems, duplicates happen due to:\n",
    "#   - API retries (network timeout -> retry -> same record sent twice)\n",
    "#   - CDC (Change Data Capture) replaying events\n",
    "#   - ETL pipeline re-runs\n",
    "#\n",
    "# dropDuplicates([\"item_id\"]) keeps FIRST occurrence and \n",
    "# removes any row where item_id appears more than once.\n",
    "#\n",
    "# In production, you'd use a Window function to keep the \n",
    "# LATEST version (like we did for orders). But for items,\n",
    "# dropDuplicates is sufficient since items don't get updated.\n",
    "\n",
    "df_items_deduped = df_items_bronze.dropDuplicates([\"item_id\"])\n",
    "dedup_count = df_items_deduped.count()\n",
    "dupes = bronze_count - dedup_count\n",
    "print(\"STEP 2: Deduplication - \" + str(dupes) + \" duplicates removed\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 3: Clean and standardize\n",
    "# ----------------------------------------------------------\n",
    "# WHY TRIM?\n",
    "# Data from source systems often has leading/trailing spaces.\n",
    "# \"PROD001 \" != \"PROD001\" -- this breaks JOINs later!\n",
    "# trim() removes those invisible spaces.\n",
    "#\n",
    "# WHY LOWER for item_status?\n",
    "# Source might have \"Shipped\", \"SHIPPED\", \"shipped\" \n",
    "# We standardize to one format for consistent GROUP BY.\n",
    "#\n",
    "# WHY COALESCE for discount_percent?\n",
    "# coalesce(value, default) returns:\n",
    "#   - value if it's NOT null\n",
    "#   - default (0.0) if value IS null\n",
    "# This prevents null propagation in calculations.\n",
    "# Example: 100 * null = null (bad!)\n",
    "#          100 * 0.0 = 0.0 (good!)\n",
    "\n",
    "df_items_clean = df_items_deduped \\\n",
    "    .withColumn(\"item_id\", trim(col(\"item_id\"))) \\\n",
    "    .withColumn(\"order_id\", trim(col(\"order_id\"))) \\\n",
    "    .withColumn(\"product_id\", trim(col(\"product_id\"))) \\\n",
    "    .withColumn(\"item_status\", lower(trim(col(\"item_status\")))) \\\n",
    "    .withColumn(\"quantity\", col(\"quantity\").cast(\"int\")) \\\n",
    "    .withColumn(\"unit_price\", col(\"unit_price\").cast(\"double\")) \\\n",
    "    .withColumn(\"discount_percent\", coalesce(col(\"discount_percent\").cast(\"double\"), lit(0.0)))\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 4: Add derived columns\n",
    "# ----------------------------------------------------------\n",
    "# WHY PRE-CALCULATE THESE?\n",
    "# In the Gold layer and dashboards, these calculations \n",
    "# would be repeated millions of times. Pre-computing them \n",
    "# in Silver means:\n",
    "#   1. Faster queries (no runtime math)\n",
    "#   2. Consistent results (everyone uses same formula)\n",
    "#   3. Single source of truth\n",
    "#\n",
    "# FORMULAS:\n",
    "#   line_total = quantity * unit_price\n",
    "#     Example: 2 headphones * $79.99 = $159.98\n",
    "#\n",
    "#   discount_amount = line_total * discount_percent / 100\n",
    "#     Example: $159.98 * 10% = $15.99\n",
    "#\n",
    "#   net_line_total = line_total - discount_amount\n",
    "#     Formula: quantity * unit_price * (1 - discount/100)\n",
    "#     Example: $159.98 * (1 - 0.10) = $143.98\n",
    "#\n",
    "# round(value, 2) ensures we get exactly 2 decimal places\n",
    "# for currency values. Without it: $143.982000001\n",
    "\n",
    "df_items_enriched = df_items_clean \\\n",
    "    .withColumn(\"line_total\",\n",
    "        round(col(\"quantity\") * col(\"unit_price\"), 2)) \\\n",
    "    .withColumn(\"discount_amount\",\n",
    "        round(col(\"quantity\") * col(\"unit_price\") * col(\"discount_percent\") / 100, 2)) \\\n",
    "    .withColumn(\"net_line_total\",\n",
    "        round(col(\"quantity\") * col(\"unit_price\") * (1 - col(\"discount_percent\") / 100), 2)) \\\n",
    "    .withColumn(\"has_discount\",\n",
    "        when(col(\"discount_percent\") > 0, lit(True)).otherwise(lit(False)))\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 5: Data Quality - remove rows with null keys\n",
    "# ----------------------------------------------------------\n",
    "# WHY FILTER INSTEAD OF QUARANTINE?\n",
    "# For the main orders table, we used a formal quarantine \n",
    "# process (save bad records separately). That's because \n",
    "# orders are high-value business records.\n",
    "#\n",
    "# For order_items, we do a simpler approach:\n",
    "#   - Filter out clearly invalid rows\n",
    "#   - Count them for monitoring\n",
    "#   - In production, you'd quarantine these too\n",
    "#\n",
    "# RULES:\n",
    "#   1. item_id must not be null (primary key)\n",
    "#   2. order_id must not be null (foreign key to orders)\n",
    "#   3. product_id must not be null (foreign key to products)\n",
    "#   4. quantity must be > 0 (can't sell 0 or negative items)\n",
    "#   5. unit_price must be > 0 (can't have free/negative price)\n",
    "#\n",
    "# The & operator means AND - ALL conditions must be true.\n",
    "# The | operator means OR - ANY condition being true = bad record.\n",
    "\n",
    "df_items_good = df_items_enriched.filter(\n",
    "    col(\"item_id\").isNotNull() &\n",
    "    col(\"order_id\").isNotNull() &\n",
    "    col(\"product_id\").isNotNull() &\n",
    "    (col(\"quantity\") > 0) &\n",
    "    (col(\"unit_price\") > 0)\n",
    ")\n",
    "\n",
    "df_items_bad = df_items_enriched.filter(\n",
    "    col(\"item_id\").isNull() |\n",
    "    col(\"order_id\").isNull() |\n",
    "    col(\"product_id\").isNull() |\n",
    "    (col(\"quantity\") <= 0) |\n",
    "    (col(\"unit_price\") <= 0)\n",
    ")\n",
    "\n",
    "good_count = df_items_good.count()\n",
    "bad_count = df_items_bad.count()\n",
    "print(\"STEP 5: Quality check - \" + str(good_count) + \" good, \" + str(bad_count) + \" bad\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 6: Add metadata and write\n",
    "# ----------------------------------------------------------\n",
    "# WHY ADD METADATA COLUMNS?\n",
    "# _silver_processed_at: When was this row processed?\n",
    "#   - Useful for debugging (\"when did this data arrive?\")\n",
    "#   - Useful for incremental loads (\"give me everything \n",
    "#     processed after yesterday\")\n",
    "#\n",
    "# _silver_version: Track schema/logic changes\n",
    "#   - Version \"1.0\" = current transformation logic\n",
    "#   - If we change business rules later, bump to \"2.0\"\n",
    "#   - Helps answer: \"which version of logic created this?\"\n",
    "#\n",
    "# WHY DELTA FORMAT?\n",
    "# Delta Lake gives us:\n",
    "#   1. ACID transactions (write either fully succeeds or fails)\n",
    "#   2. Schema enforcement (wrong data types get rejected)\n",
    "#   3. Time travel (query data as it was yesterday)\n",
    "#   4. UPSERT support (merge new + existing data)\n",
    "#   5. Audit history (who changed what, when)\n",
    "#\n",
    "# WHY mode(\"overwrite\")?\n",
    "# For this project, we rebuild Silver from scratch each run.\n",
    "# In production, you'd use MERGE (upsert) for incremental loads.\n",
    "#\n",
    "# WHY overwriteSchema=True?\n",
    "# If we add/remove columns between runs, Delta would normally\n",
    "# reject the write. This flag says \"trust me, update the schema.\"\n",
    "\n",
    "df_items_silver = df_items_good \\\n",
    "    .withColumn(\"_silver_processed_at\", current_timestamp()) \\\n",
    "    .withColumn(\"_silver_version\", lit(\"1.0\"))\n",
    "\n",
    "silver_items_path = SILVER + \"/order_items\"\n",
    "\n",
    "df_items_silver.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", True) \\\n",
    "    .save(silver_items_path)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 7: Verify\n",
    "# ----------------------------------------------------------\n",
    "# WHY READ BACK AND VERIFY?\n",
    "# \"Trust but verify\" - we read back what we just wrote to:\n",
    "#   1. Confirm the write succeeded\n",
    "#   2. Check row counts match expectations\n",
    "#   3. Validate schema is correct\n",
    "#   4. Show sample data for visual inspection\n",
    "#\n",
    "# This is a PRODUCTION BEST PRACTICE.\n",
    "# Many pipelines silently write corrupt or empty data.\n",
    "# Verification catches these issues immediately.\n",
    "\n",
    "df_verify = spark.read.format(\"delta\").load(silver_items_path)\n",
    "final_count = df_verify.count()\n",
    "\n",
    "print(\"\")\n",
    "print(\"=\" * 65)\n",
    "print(\"SILVER ORDER ITEMS - COMPLETE\")\n",
    "print(\"=\" * 65)\n",
    "print(\"  Source (Bronze):    \" + str(bronze_count) + \" rows\")\n",
    "print(\"  Duplicates removed: \" + str(dupes) + \" rows\")\n",
    "print(\"  Quality rejected:   \" + str(bad_count) + \" rows\")\n",
    "print(\"  Final Silver:       \" + str(final_count) + \" rows\")\n",
    "print(\"  Path:               \" + silver_items_path)\n",
    "\n",
    "print(\"\\n  Schema:\")\n",
    "df_verify.printSchema()\n",
    "\n",
    "print(\"\\n  Sample data:\")\n",
    "df_verify.select(\n",
    "    \"item_id\", \"order_id\", \"product_id\", \"quantity\",\n",
    "    \"unit_price\", \"discount_percent\", \"line_total\", \"net_line_total\"\n",
    ").show(5, truncate=False)\n",
    "\n",
    "# WHY SHOW DISTRIBUTIONS?\n",
    "# This is \"sanity checking\" - does the data LOOK right?\n",
    "# If item_status shows \"xyz_garbage\", we know something is wrong.\n",
    "# If one product has 99% of sales, that's suspicious.\n",
    "\n",
    "print(\"\\n  Item status distribution:\")\n",
    "df_verify.groupBy(\"item_status\").count().orderBy(desc(\"count\")).show()\n",
    "\n",
    "print(\"\\n  Top 10 products by quantity sold:\")\n",
    "df_verify.groupBy(\"product_id\") \\\n",
    "    .agg(\n",
    "        sum(\"quantity\").alias(\"total_qty\"),\n",
    "        sum(\"net_line_total\").alias(\"total_revenue\"),\n",
    "        count(\"*\").alias(\"order_count\")\n",
    "    ) \\\n",
    "    .orderBy(desc(\"total_qty\")) \\\n",
    "    .show(10, truncate=False)\n",
    "\n",
    "print(\"[DONE] Silver Order Items complete!\")\n",
    "print(\"[NEXT] Cell 6 - Silver Customers (PII Masking)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05971c77-863b-4695-a3de-ff835727cb80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1: Bronze Customers read - 500 rows\n  Columns: ['address', 'customer_id', 'date_of_birth', 'email', 'first_name', 'gender', 'last_name', 'loyalty_tier', 'phone', 'preferences', 'registration_date']\nSTEP 2: Nested structs flattened\n  address -> address_street, address_city, address_state, address_zip, address_country\n  preferences -> pref_categories, pref_communication\nSTEP 3: PII Masking applied\n  email  -> sha256 hash + domain extracted (original DROPPED)\n  phone  -> masked to ***-***-XXXX (original DROPPED)\n  names  -> initials extracted (originals KEPT for now)\nSTEP 4: Data types converted and columns enriched\nSTEP 5: Deduplication - 0 duplicates removed\nSTEP 6: Quality flags added - 53 customers without email\n\n=================================================================\nSILVER CUSTOMERS - COMPLETE\n=================================================================\n  Source (Bronze):    500 rows\n  Duplicates removed: 0 rows\n  Final Silver:       500 rows\n  PII Masked:         email (sha256), phone (masked), names (initials)\n  Nested Flattened:   address (5 cols), preferences (2 cols)\n  Path:               abfss://silver@dlsshopsmartdev123.dfs.core.windows.net/customers\n\n  Schema:\nroot\n |-- customer_id: string (nullable = true)\n |-- date_of_birth: date (nullable = true)\n |-- first_name: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- last_name: string (nullable = true)\n |-- loyalty_tier: string (nullable = true)\n |-- registration_date: date (nullable = true)\n |-- address_street: string (nullable = true)\n |-- address_city: string (nullable = true)\n |-- address_state: string (nullable = true)\n |-- address_zip: string (nullable = true)\n |-- address_country: string (nullable = true)\n |-- pref_categories: string (nullable = true)\n |-- pref_communication: string (nullable = true)\n |-- age: integer (nullable = true)\n |-- age_group: string (nullable = true)\n |-- customer_tenure_days: integer (nullable = true)\n |-- tenure_category: string (nullable = true)\n |-- email_hash: string (nullable = true)\n |-- email_domain: string (nullable = true)\n |-- phone_masked: string (nullable = true)\n |-- first_name_initial: string (nullable = true)\n |-- last_name_initial: string (nullable = true)\n |-- has_email: boolean (nullable = true)\n |-- _silver_processed_at: timestamp (nullable = true)\n |-- _silver_version: string (nullable = true)\n |-- _pii_masked: boolean (nullable = true)\n\n\n  PII Masking verification (compare with bronze):\n+-----------+----------+------------------+----------------------------------------------------------------+------------+------------+\n|customer_id|first_name|first_name_initial|email_hash                                                      |email_domain|phone_masked|\n+-----------+----------+------------------+----------------------------------------------------------------+------------+------------+\n|CUST001    |Kayla     |K                 |ccf21f97e69da379c2e94f5fd903ac403cb773da5c5658fef67e58be2ec1f821|example.com |***-***-3287|\n|CUST002    |Julia     |J                 |fda2978b88eb0f7a9a22c580d3cb369849c87c7df1eb73bd6c045d20dca22f6b|example.com |***-***-5812|\n|CUST003    |Sophia    |S                 |3466a56026698b28aef4e0145262dea24148d77f44e07b281cde77e7d8f16803|example.com |***-***-6706|\n|CUST004    |Matthew   |M                 |9b13aefc1e85edbaf4cc7f2bfb31a4af7fca7da3cdcef1c618dbdcba09f8ec9d|example.org |***-***-3309|\n|CUST005    |Anthony   |A                 |33405b5fc19329da47005a0856de6ce36a2937c5e4926ca8dc85e5a685ab8642|example.org |***-***-9190|\n+-----------+----------+------------------+----------------------------------------------------------------+------------+------------+\nonly showing top 5 rows\n\n  Flattened address verification:\n+-----------+----------------+-------------+-----------+\n|customer_id|address_city    |address_state|address_zip|\n+-----------+----------------+-------------+-----------+\n|CUST001    |Rivasside       |NV           |21362      |\n|CUST002    |Brownchester    |MS           |77057      |\n|CUST003    |Nicholasborough |NH           |43341      |\n|CUST004    |Port Loriport   |CT           |76350      |\n|CUST005    |Zimmermanchester|HI           |96681      |\n+-----------+----------------+-------------+-----------+\nonly showing top 5 rows\n\n  Enriched columns verification:\n+-----------+---+---------+------+------------+--------------------+-----------------------+\n|customer_id|age|age_group|gender|loyalty_tier|customer_tenure_days|tenure_category        |\n+-----------+---+---------+------+------------+--------------------+-----------------------+\n|CUST001    |18 |18-24    |Male  |Platinum    |73                  |New (< 3 months)       |\n|CUST002    |54 |45-54    |Other |Bronze      |192                 |Growing (3-12 months)  |\n|CUST003    |31 |25-34    |Other |Platinum    |603                 |Established (1-2 years)|\n|CUST004    |72 |65+      |Female|Gold        |364                 |Growing (3-12 months)  |\n|CUST005    |35 |35-44    |Male  |Gold        |444                 |Established (1-2 years)|\n+-----------+---+---------+------+------------+--------------------+-----------------------+\nonly showing top 5 rows\n\n  Gender distribution:\n+------+-----+\n|gender|count|\n+------+-----+\n|Female|  176|\n|  Male|  169|\n| Other|  155|\n+------+-----+\n\n\n  Age group distribution:\n+---------+-----+\n|age_group|count|\n+---------+-----+\n|    18-24|   51|\n|    25-34|   80|\n|    35-44|   86|\n|    45-54|   84|\n|    55-64|   74|\n|      65+|  125|\n+---------+-----+\n\n\n  Loyalty tier distribution:\n+------------+-----+\n|loyalty_tier|count|\n+------------+-----+\n|    Platinum|  145|\n|        Gold|  123|\n|      Bronze|  118|\n|      Silver|  114|\n+------------+-----+\n\n\n  Tenure category distribution:\n+--------------------+-----+\n|     tenure_category|count|\n+--------------------+-----+\n|Established (1-2 ...|  226|\n|Growing (3-12 mon...|  202|\n|    New (< 3 months)|   72|\n+--------------------+-----+\n\n\n  Email coverage:\n+---------+-----+\n|has_email|count|\n+---------+-----+\n|     true|  447|\n|    false|   53|\n+---------+-----+\n\n[DONE] Silver Customers complete!\n[NEXT] Cell 7 - Silver Products\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 6: SILVER LAYER - CUSTOMERS TRANSFORMATION\n",
    "# ============================================================\n",
    "#\n",
    "# WHY IS THIS THE MOST IMPORTANT SILVER TABLE?\n",
    "# ---------------------------------------------\n",
    "# 1. PII MASKING (Privacy compliance)\n",
    "#    - GDPR (Europe), CCPA (California), DPDPA (India)\n",
    "#    - Storing plaintext emails/phones is a LEGAL RISK\n",
    "#    - Companies get fined millions for PII exposure\n",
    "#    - In interviews: \"How do you handle PII?\" is very common\n",
    "#\n",
    "# 2. NESTED JSON FLATTENING\n",
    "#    - Source JSON has nested structs (address, preferences)\n",
    "#    - Analytics tools (SQL, Power BI) can't query nested data\n",
    "#    - We must flatten into individual columns\n",
    "#\n",
    "# 3. DATA ENRICHMENT\n",
    "#    - Derive age from date_of_birth\n",
    "#    - Create age_group for segmentation\n",
    "#    - Calculate customer tenure\n",
    "#\n",
    "# BRONZE ISSUES TO FIX:\n",
    "#   - 53 null emails (10%) -> keep but flag\n",
    "#   - Plaintext email -> SHA256 hash\n",
    "#   - Plaintext phone -> mask (show last 4 only)\n",
    "#   - Nested address struct -> flatten to columns\n",
    "#   - Nested preferences struct -> flatten\n",
    "#   - date_of_birth is string -> convert to date\n",
    "#   - registration_date is string -> convert to date\n",
    "# ============================================================\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 1: Read Bronze Customers\n",
    "# ----------------------------------------------------------\n",
    "# WHY multiLine=True?\n",
    "# Your customers.json is a JSON ARRAY format:\n",
    "#   [ {\"customer_id\": \"CUST001\", ...}, {\"customer_id\": \"CUST002\", ...} ]\n",
    "#\n",
    "# This is ONE json structure spanning MULTIPLE lines.\n",
    "# Without multiLine=True, Spark expects one JSON per line\n",
    "# and would fail or read garbage.\n",
    "#\n",
    "# Compare with clickstream.json which is JSON-Lines format:\n",
    "#   {\"event_id\": \"EVT001\", ...}\n",
    "#   {\"event_id\": \"EVT002\", ...}\n",
    "# That one does NOT need multiLine=True.\n",
    "\n",
    "df_cust_bronze = spark.read \\\n",
    "    .option(\"multiLine\", True) \\\n",
    "    .json(BRONZE + \"/source2_customers_api/customers.json\")\n",
    "\n",
    "bronze_count = df_cust_bronze.count()\n",
    "print(\"STEP 1: Bronze Customers read - \" + str(bronze_count) + \" rows\")\n",
    "print(\"  Columns: \" + str(df_cust_bronze.columns))\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 2: Flatten nested STRUCT columns\n",
    "# ----------------------------------------------------------\n",
    "# WHAT IS A STRUCT IN SPARK?\n",
    "# A struct is a column that CONTAINS other columns inside it.\n",
    "# In your schema:\n",
    "#   address: struct\n",
    "#     |-- city: string\n",
    "#     |-- state: string\n",
    "#     |-- zip: string\n",
    "#     |-- street: string\n",
    "#     |-- country: string\n",
    "#\n",
    "# To access nested fields, use DOT notation: col(\"address.city\")\n",
    "# But for analytics, we want flat columns: address_city, address_state\n",
    "#\n",
    "# WHY FLATTEN?\n",
    "# 1. SQL queries can't easily filter on nested fields\n",
    "# 2. Power BI / Tableau can't read nested structs\n",
    "# 3. JOIN conditions need flat columns\n",
    "# 4. GROUP BY on nested fields is complex and slow\n",
    "#\n",
    "# WHAT ABOUT PREFERENCES?\n",
    "# preferences has arrays inside (categories, communication)\n",
    "# We convert arrays to comma-separated strings for simplicity.\n",
    "# Example: [\"Electronics\", \"Fashion\"] -> \"Electronics,Fashion\"\n",
    "\n",
    "df_cust_flat = df_cust_bronze \\\n",
    "    .withColumn(\"address_street\", col(\"address.street\")) \\\n",
    "    .withColumn(\"address_city\", col(\"address.city\")) \\\n",
    "    .withColumn(\"address_state\", col(\"address.state\")) \\\n",
    "    .withColumn(\"address_zip\", col(\"address.zip\")) \\\n",
    "    .withColumn(\"address_country\", col(\"address.country\")) \\\n",
    "    .withColumn(\"pref_categories\", concat_ws(\",\", col(\"preferences.categories\"))) \\\n",
    "    .withColumn(\"pref_communication\", concat_ws(\",\", col(\"preferences.communication\"))) \\\n",
    "    .drop(\"address\", \"preferences\")\n",
    "\n",
    "print(\"STEP 2: Nested structs flattened\")\n",
    "print(\"  address -> address_street, address_city, address_state, address_zip, address_country\")\n",
    "print(\"  preferences -> pref_categories, pref_communication\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 3: PII MASKING\n",
    "# ----------------------------------------------------------\n",
    "# THIS IS CRITICAL FOR INTERVIEWS!\n",
    "#\n",
    "# TECHNIQUE 1: SHA-256 HASHING (for email)\n",
    "#   sha2(\"john@email.com\", 256) -> \"a1b2c3d4e5f6...\"\n",
    "#   - One-way: cannot reverse hash back to email\n",
    "#   - Deterministic: same email always gives same hash\n",
    "#   - WHY deterministic matters: we can still JOIN and \n",
    "#     GROUP BY on the hash! Two orders from same email \n",
    "#     will have the same hash.\n",
    "#   - We also extract the DOMAIN (gmail.com, yahoo.com)\n",
    "#     because domain is not PII but useful for analytics\n",
    "#\n",
    "# TECHNIQUE 2: MASKING (for phone)\n",
    "#   \"+1-555-0123\" -> \"***-***-0123\"\n",
    "#   - Shows last 4 digits (enough for customer service)\n",
    "#   - Hides the rest\n",
    "#   - substring(phone, -4, 4) gets last 4 characters\n",
    "#\n",
    "# TECHNIQUE 3: NAME MASKING\n",
    "#   \"John\" -> \"J****\"\n",
    "#   - First initial kept (useful for display)\n",
    "#   - Rest replaced with asterisks\n",
    "#\n",
    "# WHAT DO WE DROP?\n",
    "# Original email and phone columns are DROPPED entirely.\n",
    "# They should NEVER exist in Silver/Gold layers.\n",
    "# Only Bronze (raw) retains the original PII.\n",
    "\n",
    "df_cust_masked = df_cust_flat \\\n",
    "    .withColumn(\"email_hash\",\n",
    "        when(col(\"email\").isNotNull(), sha2(lower(trim(col(\"email\"))), 256))\n",
    "        .otherwise(lit(None))) \\\n",
    "    .withColumn(\"email_domain\",\n",
    "        when(col(\"email\").isNotNull(), regexp_extract(col(\"email\"), \"@(.+)$\", 1))\n",
    "        .otherwise(lit(None))) \\\n",
    "    .withColumn(\"phone_masked\",\n",
    "        when(col(\"phone\").isNotNull(),\n",
    "            concat(lit(\"***-***-\"), substring(regexp_replace(col(\"phone\"), \"[^0-9]\", \"\"), -4, 4)))\n",
    "        .otherwise(lit(None))) \\\n",
    "    .withColumn(\"first_name_initial\", substring(col(\"first_name\"), 1, 1)) \\\n",
    "    .withColumn(\"last_name_initial\", substring(col(\"last_name\"), 1, 1)) \\\n",
    "    .drop(\"email\", \"phone\")\n",
    "\n",
    "print(\"STEP 3: PII Masking applied\")\n",
    "print(\"  email  -> sha256 hash + domain extracted (original DROPPED)\")\n",
    "print(\"  phone  -> masked to ***-***-XXXX (original DROPPED)\")\n",
    "print(\"  names  -> initials extracted (originals KEPT for now)\")\n",
    "\n",
    "# NOTE ON NAMES: In a real production system, you might also\n",
    "# drop first_name and last_name entirely, keeping only initials.\n",
    "# For this project, we keep them to demonstrate the choice.\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 4: Data type conversions and enrichment\n",
    "# ----------------------------------------------------------\n",
    "# WHY CONVERT date_of_birth FROM STRING TO DATE?\n",
    "# As string: \"1990-05-15\" - Spark treats it as text\n",
    "#   - Can't do date math (calculate age)\n",
    "#   - Can't filter by date ranges\n",
    "#   - Sorts incorrectly (\"9\" > \"10\" in string sort!)\n",
    "# As date: 1990-05-15 - Spark treats it as a date\n",
    "#   - Can calculate age: datediff(today, dob) / 365\n",
    "#   - Can filter: WHERE dob > '1990-01-01'\n",
    "#   - Sorts correctly\n",
    "#\n",
    "# AGE CALCULATION:\n",
    "#   datediff(current_date, date_of_birth) gives days\n",
    "#   Divide by 365.25 (accounts for leap years)\n",
    "#   cast to int (drop decimals: 34.7 -> 34)\n",
    "#\n",
    "# AGE GROUPS: \n",
    "#   Used for customer segmentation and marketing\n",
    "#   \"Show me revenue by age group\" is a common business ask\n",
    "#\n",
    "# CUSTOMER TENURE:\n",
    "#   How long has this person been a customer?\n",
    "#   New customers (< 90 days) behave differently than \n",
    "#   loyal customers (2+ years). This drives business strategy.\n",
    "\n",
    "df_cust_enriched = df_cust_flat \\\n",
    "    .withColumn(\"customer_id\", trim(col(\"customer_id\"))) \\\n",
    "    .withColumn(\"date_of_birth\", to_date(col(\"date_of_birth\"))) \\\n",
    "    .withColumn(\"registration_date\", to_date(col(\"registration_date\"))) \\\n",
    "    .withColumn(\"gender\",\n",
    "        when(upper(col(\"gender\")).isin(\"M\", \"MALE\"), lit(\"Male\"))\n",
    "        .when(upper(col(\"gender\")).isin(\"F\", \"FEMALE\"), lit(\"Female\"))\n",
    "        .otherwise(lit(\"Other\"))) \\\n",
    "    .withColumn(\"loyalty_tier\", initcap(trim(col(\"loyalty_tier\")))) \\\n",
    "    .withColumn(\"age\",\n",
    "        floor(datediff(current_date(), col(\"date_of_birth\")) / 365.25).cast(\"int\")) \\\n",
    "    .withColumn(\"age_group\",\n",
    "        when(col(\"age\") < 25, lit(\"18-24\"))\n",
    "        .when(col(\"age\") < 35, lit(\"25-34\"))\n",
    "        .when(col(\"age\") < 45, lit(\"35-44\"))\n",
    "        .when(col(\"age\") < 55, lit(\"45-54\"))\n",
    "        .when(col(\"age\") < 65, lit(\"55-64\"))\n",
    "        .otherwise(lit(\"65+\"))) \\\n",
    "    .withColumn(\"customer_tenure_days\",\n",
    "        datediff(current_date(), col(\"registration_date\"))) \\\n",
    "    .withColumn(\"tenure_category\",\n",
    "        when(col(\"customer_tenure_days\") < 90, lit(\"New (< 3 months)\"))\n",
    "        .when(col(\"customer_tenure_days\") < 365, lit(\"Growing (3-12 months)\"))\n",
    "        .when(col(\"customer_tenure_days\") < 730, lit(\"Established (1-2 years)\"))\n",
    "        .otherwise(lit(\"Loyal (2+ years)\")))\n",
    "\n",
    "# WAIT - we need to apply masking to the ENRICHED dataframe!\n",
    "# Let me combine steps 3 and 4 properly:\n",
    "\n",
    "df_cust_silver_prep = df_cust_enriched \\\n",
    "    .withColumn(\"email_hash\",\n",
    "        when(col(\"email\").isNotNull(), sha2(lower(trim(col(\"email\"))), 256))\n",
    "        .otherwise(lit(None))) \\\n",
    "    .withColumn(\"email_domain\",\n",
    "        when(col(\"email\").isNotNull(), regexp_extract(col(\"email\"), \"@(.+)$\", 1))\n",
    "        .otherwise(lit(None))) \\\n",
    "    .withColumn(\"phone_masked\",\n",
    "        when(col(\"phone\").isNotNull(),\n",
    "            concat(lit(\"***-***-\"), substring(regexp_replace(col(\"phone\"), \"[^0-9]\", \"\"), -4, 4)))\n",
    "        .otherwise(lit(None))) \\\n",
    "    .withColumn(\"first_name_initial\", substring(col(\"first_name\"), 1, 1)) \\\n",
    "    .withColumn(\"last_name_initial\", substring(col(\"last_name\"), 1, 1)) \\\n",
    "    .drop(\"email\", \"phone\")\n",
    "\n",
    "print(\"STEP 4: Data types converted and columns enriched\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 5: Deduplicate on customer_id\n",
    "# ----------------------------------------------------------\n",
    "df_cust_deduped = df_cust_silver_prep.dropDuplicates([\"customer_id\"])\n",
    "dedup_count = df_cust_deduped.count()\n",
    "dupes = bronze_count - dedup_count\n",
    "print(\"STEP 5: Deduplication - \" + str(dupes) + \" duplicates removed\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 6: Data Quality check\n",
    "# ----------------------------------------------------------\n",
    "# We keep customers with null emails (they are valid customers\n",
    "# who just didn't provide email). But we FLAG them.\n",
    "\n",
    "df_cust_final = df_cust_deduped \\\n",
    "    .withColumn(\"has_email\", col(\"email_hash\").isNotNull()) \\\n",
    "    .withColumn(\"_silver_processed_at\", current_timestamp()) \\\n",
    "    .withColumn(\"_silver_version\", lit(\"1.0\")) \\\n",
    "    .withColumn(\"_pii_masked\", lit(True))\n",
    "\n",
    "null_email_count = df_cust_final.filter(col(\"has_email\") == False).count()\n",
    "print(\"STEP 6: Quality flags added - \" + str(null_email_count) + \" customers without email\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 7: Write to Silver\n",
    "# ----------------------------------------------------------\n",
    "silver_customers_path = SILVER + \"/customers\"\n",
    "\n",
    "df_cust_final.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", True) \\\n",
    "    .save(silver_customers_path)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 8: Verify\n",
    "# ----------------------------------------------------------\n",
    "df_verify = spark.read.format(\"delta\").load(silver_customers_path)\n",
    "final_count = df_verify.count()\n",
    "\n",
    "print(\"\")\n",
    "print(\"=\" * 65)\n",
    "print(\"SILVER CUSTOMERS - COMPLETE\")\n",
    "print(\"=\" * 65)\n",
    "print(\"  Source (Bronze):    \" + str(bronze_count) + \" rows\")\n",
    "print(\"  Duplicates removed: \" + str(dupes) + \" rows\")\n",
    "print(\"  Final Silver:       \" + str(final_count) + \" rows\")\n",
    "print(\"  PII Masked:         email (sha256), phone (masked), names (initials)\")\n",
    "print(\"  Nested Flattened:   address (5 cols), preferences (2 cols)\")\n",
    "print(\"  Path:               \" + silver_customers_path)\n",
    "\n",
    "print(\"\\n  Schema:\")\n",
    "df_verify.printSchema()\n",
    "\n",
    "# Show PII masking result\n",
    "print(\"\\n  PII Masking verification (compare with bronze):\")\n",
    "df_verify.select(\n",
    "    \"customer_id\", \"first_name\", \"first_name_initial\",\n",
    "    \"email_hash\", \"email_domain\", \"phone_masked\"\n",
    ").show(5, truncate=False)\n",
    "\n",
    "# Show flattened address\n",
    "print(\"\\n  Flattened address verification:\")\n",
    "df_verify.select(\n",
    "    \"customer_id\", \"address_city\", \"address_state\", \"address_zip\"\n",
    ").show(5, truncate=False)\n",
    "\n",
    "# Show enriched columns\n",
    "print(\"\\n  Enriched columns verification:\")\n",
    "df_verify.select(\n",
    "    \"customer_id\", \"age\", \"age_group\", \"gender\",\n",
    "    \"loyalty_tier\", \"customer_tenure_days\", \"tenure_category\"\n",
    ").show(5, truncate=False)\n",
    "\n",
    "# Distributions\n",
    "print(\"\\n  Gender distribution:\")\n",
    "df_verify.groupBy(\"gender\").count().orderBy(desc(\"count\")).show()\n",
    "\n",
    "print(\"\\n  Age group distribution:\")\n",
    "df_verify.groupBy(\"age_group\").count().orderBy(\"age_group\").show()\n",
    "\n",
    "print(\"\\n  Loyalty tier distribution:\")\n",
    "df_verify.groupBy(\"loyalty_tier\").count().orderBy(desc(\"count\")).show()\n",
    "\n",
    "print(\"\\n  Tenure category distribution:\")\n",
    "df_verify.groupBy(\"tenure_category\").count().orderBy(desc(\"count\")).show()\n",
    "\n",
    "print(\"\\n  Email coverage:\")\n",
    "df_verify.groupBy(\"has_email\").count().show()\n",
    "\n",
    "print(\"[DONE] Silver Customers complete!\")\n",
    "print(\"[NEXT] Cell 7 - Silver Products\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "badc1d18-30a2-496f-a860-afe316a02896",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1: Bronze Products read - 50 rows\nSTEP 2: Nested attributes flattened\n  attributes.battery_life -> attr_battery_life\n  attributes.color (array) -> attr_colors (comma-separated string)\n  attributes.connectivity -> attr_connectivity\nSTEP 3: Data types standardized\nSTEP 4: Business columns derived\n  profit_margin, margin_pct, price_tier\n  rating_category, product_age_days\nSTEP 5: Quality check\n  Duplicates removed: 0\n  Good records:       50\n  Bad records:        0\n\n=================================================================\nSILVER PRODUCTS - COMPLETE\n=================================================================\n  Source (Bronze):    50 rows\n  Duplicates removed: 0\n  Quality rejected:   0\n  Final Silver:       50 rows\n  Path:               abfss://silver@dlsshopsmartdev123.dfs.core.windows.net/products\n\n  Schema:\nroot\n |-- brand: string (nullable = true)\n |-- category: string (nullable = true)\n |-- cost_price: double (nullable = true)\n |-- created_at: timestamp (nullable = true)\n |-- is_active: boolean (nullable = true)\n |-- price: double (nullable = true)\n |-- product_id: string (nullable = true)\n |-- product_name: string (nullable = true)\n |-- rating: double (nullable = true)\n |-- review_count: integer (nullable = true)\n |-- sub_category: string (nullable = true)\n |-- supplier_id: string (nullable = true)\n |-- updated_at: timestamp (nullable = true)\n |-- weight_kg: double (nullable = true)\n |-- attr_battery_life: string (nullable = true)\n |-- attr_colors: string (nullable = true)\n |-- attr_connectivity: string (nullable = true)\n |-- profit_margin: double (nullable = true)\n |-- margin_pct: double (nullable = true)\n |-- price_tier: string (nullable = true)\n |-- rating_category: string (nullable = true)\n |-- product_age_days: integer (nullable = true)\n |-- _silver_processed_at: timestamp (nullable = true)\n |-- _silver_version: string (nullable = true)\n\n\n  Sample data:\n+----------+--------------------------------------+-----------+----------------+------+----------+-------------+----------+----------+\n|product_id|product_name                          |category   |brand           |price |cost_price|profit_margin|margin_pct|price_tier|\n+----------+--------------------------------------+-----------+----------------+------+----------+-------------+----------+----------+\n|PROD001   |Sharable bifurcated algorithm         |Electronics|Sanchez-taylor  |644.83|401.38    |243.45       |37.75     |Luxury    |\n|PROD002   |Optimized 5thgeneration algorithm     |Electronics|Hall Plc        |46.31 |21.56     |24.75        |53.44     |Budget    |\n|PROD003   |Total needs-based instruction set     |Home       |Lawrence-pacheco|457.47|294.08    |163.39       |35.72     |Premium   |\n|PROD004   |Mandatory even-keeled collaboration   |Home       |Garcia-james    |227.08|97.79     |129.29       |56.94     |Premium   |\n|PROD005   |Balanced upward-trending knowledgebase|Beauty     |Dudley Group    |733.78|320.99    |412.79       |56.26     |Luxury    |\n+----------+--------------------------------------+-----------+----------------+------+----------+-------------+----------+----------+\nonly showing top 5 rows\n\n  Flattened attributes sample:\n+----------+-----------------+-------------------------+-----------------+\n|product_id|attr_battery_life|attr_colors              |attr_connectivity|\n+----------+-----------------+-------------------------+-----------------+\n|PROD001   |32 hours         |DarkKhaki,YellowGreen    |Bluetooth 5.0    |\n|PROD002   |19 hours         |Bisque,Gray              |Bluetooth 5.0    |\n|PROD003   |14 hours         |AntiqueWhite,DimGray     |Bluetooth 5.0    |\n|PROD004   |7 hours          |MintCream,MediumTurquoise|Bluetooth 5.0    |\n|PROD005   |9 hours          |Cyan,LightYellow         |Bluetooth 5.0    |\n+----------+-----------------+-------------------------+-----------------+\nonly showing top 5 rows\n\n  Category distribution:\n+-----------+-------------+---------+--------------+\n|   category|product_count|avg_price|avg_margin_pct|\n+-----------+-------------+---------+--------------+\n|       Home|           15|   478.27|         46.39|\n|    Fashion|           10|   480.67|         42.99|\n|Electronics|           10|   559.28|         46.07|\n|     Beauty|            9|   500.44|         42.19|\n|     Sports|            6|   604.22|         46.94|\n+-----------+-------------+---------+--------------+\n\n\n  Price tier distribution:\n+----------+-----+---------+----------+\n|price_tier|count|avg_price|avg_rating|\n+----------+-----+---------+----------+\n|    Budget|    3|     32.9|       3.7|\n| Mid-Range|    5|    96.68|      3.72|\n|   Premium|   13|    364.9|      4.18|\n|    Luxury|   29|   702.66|      3.44|\n+----------+-----+---------+----------+\n\n\n  Rating category distribution:\n+---------------+-----+\n|rating_category|count|\n+---------------+-----+\n|        Average|   15|\n|      Excellent|   12|\n|      Very Good|   12|\n|           Good|   11|\n+---------------+-----+\n\n\n  Active status:\n+---------+-----+\n|is_active|count|\n+---------+-----+\n|     true|   42|\n|    false|    8|\n+---------+-----+\n\n[DONE] Silver Products complete!\n[NEXT] Cell 8 - Silver Inventory\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 7: SILVER LAYER - PRODUCTS TRANSFORMATION\n",
    "# ============================================================\n",
    "#\n",
    "# WHAT IS THIS TABLE?\n",
    "# -------------------\n",
    "# The product catalog - master data for all items we sell.\n",
    "# This becomes dim_product in the Gold layer (Star Schema).\n",
    "#\n",
    "# \"Master data\" means it changes slowly (new products added,\n",
    "# prices updated occasionally). Unlike orders which grow fast.\n",
    "# In data modeling, this is called a \"Slowly Changing Dimension\".\n",
    "#\n",
    "# BRONZE ISSUES TO FIX:\n",
    "#   - Nested \"attributes\" struct (color array, battery_life, connectivity)\n",
    "#   - created_at and updated_at are strings -> convert to timestamps\n",
    "#   - Derive: price_tier, profit_margin, margin_percentage\n",
    "#   - No null prices in our data, but we handle it defensively\n",
    "#\n",
    "# WHY IS THIS TABLE IMPORTANT?\n",
    "# Every revenue query needs product info:\n",
    "#   \"Revenue by category\" = JOIN fact_sales WITH dim_product\n",
    "#   \"Top brands by margin\" = product table only\n",
    "#   \"Products to discontinue\" = low rating + low margin\n",
    "# ============================================================\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 1: Read Bronze Products\n",
    "# ----------------------------------------------------------\n",
    "df_prod_bronze = spark.read \\\n",
    "    .option(\"multiLine\", True) \\\n",
    "    .json(BRONZE + \"/source3_products_mongo/products.json\")\n",
    "\n",
    "bronze_count = df_prod_bronze.count()\n",
    "print(\"STEP 1: Bronze Products read - \" + str(bronze_count) + \" rows\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 2: Flatten nested attributes struct\n",
    "# ----------------------------------------------------------\n",
    "# Your products have this nested structure:\n",
    "#   attributes: struct\n",
    "#     |-- battery_life: string (\"20 hours\")\n",
    "#     |-- color: array of strings ([\"Black\", \"White\"])\n",
    "#     |-- connectivity: string (\"Bluetooth 5.0\")\n",
    "#\n",
    "# We extract each field into its own column.\n",
    "#\n",
    "# FOR THE COLOR ARRAY:\n",
    "# arrays can't be used in GROUP BY or WHERE easily.\n",
    "# concat_ws(\",\", array) converts [\"Black\",\"White\"] -> \"Black,White\"\n",
    "# This is called \"denormalization\" - trading storage for query speed.\n",
    "\n",
    "df_prod_flat = df_prod_bronze \\\n",
    "    .withColumn(\"attr_battery_life\", col(\"attributes.battery_life\")) \\\n",
    "    .withColumn(\"attr_colors\", concat_ws(\",\", col(\"attributes.color\"))) \\\n",
    "    .withColumn(\"attr_connectivity\", col(\"attributes.connectivity\")) \\\n",
    "    .drop(\"attributes\")\n",
    "\n",
    "print(\"STEP 2: Nested attributes flattened\")\n",
    "print(\"  attributes.battery_life -> attr_battery_life\")\n",
    "print(\"  attributes.color (array) -> attr_colors (comma-separated string)\")\n",
    "print(\"  attributes.connectivity -> attr_connectivity\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 3: Data type conversions\n",
    "# ----------------------------------------------------------\n",
    "# WHY CONVERT created_at and updated_at?\n",
    "# They come as strings like \"2024-01-15T10:30:00Z\"\n",
    "# As strings, we can't:\n",
    "#   - Calculate \"products added last month\"\n",
    "#   - Sort by creation date properly\n",
    "#   - Filter by date ranges\n",
    "#\n",
    "# to_timestamp() parses the string into a proper timestamp type.\n",
    "\n",
    "df_prod_typed = df_prod_flat \\\n",
    "    .withColumn(\"product_id\", trim(col(\"product_id\"))) \\\n",
    "    .withColumn(\"product_name\", trim(col(\"product_name\"))) \\\n",
    "    .withColumn(\"category\", initcap(trim(col(\"category\")))) \\\n",
    "    .withColumn(\"sub_category\", initcap(trim(col(\"sub_category\")))) \\\n",
    "    .withColumn(\"brand\", initcap(trim(col(\"brand\")))) \\\n",
    "    .withColumn(\"supplier_id\", trim(col(\"supplier_id\"))) \\\n",
    "    .withColumn(\"price\", col(\"price\").cast(\"double\")) \\\n",
    "    .withColumn(\"cost_price\", col(\"cost_price\").cast(\"double\")) \\\n",
    "    .withColumn(\"weight_kg\", col(\"weight_kg\").cast(\"double\")) \\\n",
    "    .withColumn(\"rating\", col(\"rating\").cast(\"double\")) \\\n",
    "    .withColumn(\"review_count\", col(\"review_count\").cast(\"int\")) \\\n",
    "    .withColumn(\"is_active\", col(\"is_active\").cast(\"boolean\")) \\\n",
    "    .withColumn(\"created_at\", to_timestamp(col(\"created_at\"))) \\\n",
    "    .withColumn(\"updated_at\", to_timestamp(col(\"updated_at\")))\n",
    "\n",
    "print(\"STEP 3: Data types standardized\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 4: Derive business columns\n",
    "# ----------------------------------------------------------\n",
    "# PROFIT MARGIN:\n",
    "#   profit_margin = price - cost_price\n",
    "#   Example: Selling at $100, cost is $60 -> margin = $40\n",
    "#\n",
    "# MARGIN PERCENTAGE:\n",
    "#   margin_pct = (price - cost) / price * 100\n",
    "#   Example: ($100 - $60) / $100 * 100 = 40%\n",
    "#   This tells us what % of revenue is profit\n",
    "#   Healthy e-commerce margin: 30-50%\n",
    "#\n",
    "# PRICE TIER:\n",
    "#   Categorize products by price range for analysis\n",
    "#   \"What % of revenue comes from Premium products?\"\n",
    "#   Business teams think in tiers, not exact prices\n",
    "#\n",
    "# RATING CATEGORY:\n",
    "#   Convert numeric rating to human-readable label\n",
    "#   Used in dashboards: \"Show me all Poor-rated products\"\n",
    "#\n",
    "# PRODUCT AGE:\n",
    "#   How long has this product been in our catalog?\n",
    "#   Old products with low sales might need to be discontinued\n",
    "\n",
    "df_prod_enriched = df_prod_typed \\\n",
    "    .withColumn(\"profit_margin\",\n",
    "        round(col(\"price\") - col(\"cost_price\"), 2)) \\\n",
    "    .withColumn(\"margin_pct\",\n",
    "        when(col(\"price\") > 0,\n",
    "            round((col(\"price\") - col(\"cost_price\")) / col(\"price\") * 100, 2))\n",
    "        .otherwise(lit(0.0))) \\\n",
    "    .withColumn(\"price_tier\",\n",
    "        when(col(\"price\") < 50, lit(\"Budget\"))\n",
    "        .when(col(\"price\") < 200, lit(\"Mid-Range\"))\n",
    "        .when(col(\"price\") < 500, lit(\"Premium\"))\n",
    "        .otherwise(lit(\"Luxury\"))) \\\n",
    "    .withColumn(\"rating_category\",\n",
    "        when(col(\"rating\") >= 4.5, lit(\"Excellent\"))\n",
    "        .when(col(\"rating\") >= 4.0, lit(\"Very Good\"))\n",
    "        .when(col(\"rating\") >= 3.0, lit(\"Good\"))\n",
    "        .when(col(\"rating\") >= 2.0, lit(\"Average\"))\n",
    "        .otherwise(lit(\"Poor\"))) \\\n",
    "    .withColumn(\"product_age_days\",\n",
    "        datediff(current_date(), col(\"created_at\")))\n",
    "\n",
    "print(\"STEP 4: Business columns derived\")\n",
    "print(\"  profit_margin, margin_pct, price_tier\")\n",
    "print(\"  rating_category, product_age_days\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 5: Deduplicate and quality check\n",
    "# ----------------------------------------------------------\n",
    "df_prod_deduped = df_prod_enriched.dropDuplicates([\"product_id\"])\n",
    "dedup_count = df_prod_deduped.count()\n",
    "dupes = bronze_count - dedup_count\n",
    "\n",
    "# Quality: ensure product_id and price are valid\n",
    "df_prod_good = df_prod_deduped.filter(\n",
    "    col(\"product_id\").isNotNull() &\n",
    "    col(\"price\").isNotNull() &\n",
    "    (col(\"price\") > 0)\n",
    ")\n",
    "\n",
    "df_prod_bad = df_prod_deduped.filter(\n",
    "    col(\"product_id\").isNull() |\n",
    "    col(\"price\").isNull() |\n",
    "    (col(\"price\") <= 0)\n",
    ")\n",
    "\n",
    "good_count = df_prod_good.count()\n",
    "bad_count = df_prod_bad.count()\n",
    "\n",
    "print(\"STEP 5: Quality check\")\n",
    "print(\"  Duplicates removed: \" + str(dupes))\n",
    "print(\"  Good records:       \" + str(good_count))\n",
    "print(\"  Bad records:        \" + str(bad_count))\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 6: Add metadata and write\n",
    "# ----------------------------------------------------------\n",
    "df_prod_silver = df_prod_good \\\n",
    "    .withColumn(\"_silver_processed_at\", current_timestamp()) \\\n",
    "    .withColumn(\"_silver_version\", lit(\"1.0\"))\n",
    "\n",
    "silver_products_path = SILVER + \"/products\"\n",
    "\n",
    "df_prod_silver.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", True) \\\n",
    "    .save(silver_products_path)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 7: Verify\n",
    "# ----------------------------------------------------------\n",
    "df_verify = spark.read.format(\"delta\").load(silver_products_path)\n",
    "final_count = df_verify.count()\n",
    "\n",
    "print(\"\")\n",
    "print(\"=\" * 65)\n",
    "print(\"SILVER PRODUCTS - COMPLETE\")\n",
    "print(\"=\" * 65)\n",
    "print(\"  Source (Bronze):    \" + str(bronze_count) + \" rows\")\n",
    "print(\"  Duplicates removed: \" + str(dupes))\n",
    "print(\"  Quality rejected:   \" + str(bad_count))\n",
    "print(\"  Final Silver:       \" + str(final_count) + \" rows\")\n",
    "print(\"  Path:               \" + silver_products_path)\n",
    "\n",
    "print(\"\\n  Schema:\")\n",
    "df_verify.printSchema()\n",
    "\n",
    "print(\"\\n  Sample data:\")\n",
    "df_verify.select(\n",
    "    \"product_id\", \"product_name\", \"category\", \"brand\",\n",
    "    \"price\", \"cost_price\", \"profit_margin\", \"margin_pct\", \"price_tier\"\n",
    ").show(5, truncate=False)\n",
    "\n",
    "# Flattened attributes\n",
    "print(\"\\n  Flattened attributes sample:\")\n",
    "df_verify.select(\n",
    "    \"product_id\", \"attr_battery_life\", \"attr_colors\", \"attr_connectivity\"\n",
    ").show(5, truncate=False)\n",
    "\n",
    "# Category distribution\n",
    "print(\"\\n  Category distribution:\")\n",
    "df_verify.groupBy(\"category\").agg(\n",
    "    count(\"*\").alias(\"product_count\"),\n",
    "    round(avg(\"price\"), 2).alias(\"avg_price\"),\n",
    "    round(avg(\"margin_pct\"), 2).alias(\"avg_margin_pct\")\n",
    ").orderBy(desc(\"product_count\")).show()\n",
    "\n",
    "# Price tier distribution\n",
    "print(\"\\n  Price tier distribution:\")\n",
    "df_verify.groupBy(\"price_tier\").agg(\n",
    "    count(\"*\").alias(\"count\"),\n",
    "    round(avg(\"price\"), 2).alias(\"avg_price\"),\n",
    "    round(avg(\"rating\"), 2).alias(\"avg_rating\")\n",
    ").orderBy(\"avg_price\").show()\n",
    "\n",
    "# Rating distribution\n",
    "print(\"\\n  Rating category distribution:\")\n",
    "df_verify.groupBy(\"rating_category\").count().orderBy(desc(\"count\")).show()\n",
    "\n",
    "# Active vs Inactive\n",
    "print(\"\\n  Active status:\")\n",
    "df_verify.groupBy(\"is_active\").count().show()\n",
    "\n",
    "print(\"[DONE] Silver Products complete!\")\n",
    "print(\"[NEXT] Cell 8 - Silver Inventory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7d9cf0d-7cdd-4cb2-ad41-df32997ee8c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1: Bronze Inventory read - 150 rows\nSTEP 2: Found 4 negative stock rows\n  Negative values set to 0 and flagged\nSTEP 3: Data types standardized\nSTEP 4: Business columns derived\n  quantity_available, stock_status, needs_reorder\n  days_since_restock, stock_value\nSTEP 5: Deduplication - 0 duplicates removed\nSTEP 6: Quality check - 0 bad records\n\n=================================================================\nSILVER INVENTORY - COMPLETE\n=================================================================\n  Source (Bronze):     150 rows\n  Negative stock fixed: 4 rows\n  Duplicates removed:  0\n  Quality rejected:    0\n  Final Silver:        150 rows\n  Path:                abfss://silver@dlsshopsmartdev123.dfs.core.windows.net/inventory\n\n  Schema:\nroot\n |-- product_id: string (nullable = true)\n |-- warehouse_id: string (nullable = true)\n |-- quantity_on_hand: integer (nullable = true)\n |-- quantity_reserved: integer (nullable = true)\n |-- reorder_point: integer (nullable = true)\n |-- reorder_quantity: integer (nullable = true)\n |-- last_restock_date: date (nullable = true)\n |-- snapshot_date: date (nullable = true)\n |-- _had_negative_stock: boolean (nullable = true)\n |-- quantity_available: integer (nullable = true)\n |-- stock_status: string (nullable = true)\n |-- needs_reorder: boolean (nullable = true)\n |-- days_since_restock: integer (nullable = true)\n |-- stock_value: double (nullable = true)\n |-- _silver_processed_at: timestamp (nullable = true)\n |-- _silver_version: string (nullable = true)\n\n\n  Sample data:\n+----------+------------+----------------+-----------------+------------------+------------+-------------+\n|product_id|warehouse_id|quantity_on_hand|quantity_reserved|quantity_available|stock_status|needs_reorder|\n+----------+------------+----------------+-----------------+------------------+------------+-------------+\n|PROD036   |WH001       |240             |47               |193               |Medium Stock|false        |\n|PROD041   |WH001       |651             |171              |480               |In Stock    |false        |\n|PROD023   |WH002       |715             |111              |604               |In Stock    |false        |\n|PROD043   |WH003       |488             |43               |445               |In Stock    |false        |\n|PROD018   |WH003       |753             |54               |699               |In Stock    |false        |\n|PROD026   |WH003       |276             |66               |210               |In Stock    |false        |\n|PROD005   |WH002       |164             |19               |145               |Medium Stock|false        |\n|PROD048   |WH002       |0               |0                |0                 |Out of Stock|true         |\n|PROD015   |WH001       |882             |7                |875               |In Stock    |false        |\n|PROD021   |WH001       |902             |242              |660               |In Stock    |false        |\n+----------+------------+----------------+-----------------+------------------+------------+-------------+\nonly showing top 10 rows\n\n  Previously negative stock (now fixed to 0):\n+----------+------------+----------------+------------+-------------------+\n|product_id|warehouse_id|quantity_on_hand|stock_status|_had_negative_stock|\n+----------+------------+----------------+------------+-------------------+\n|PROD048   |WH002       |0               |Out of Stock|true               |\n|PROD020   |WH002       |0               |Out of Stock|true               |\n|PROD042   |WH003       |0               |Out of Stock|true               |\n|PROD039   |WH002       |0               |Out of Stock|true               |\n+----------+------------+----------------+------------+-------------------+\n\n\n  Stock status distribution:\n+------------+-----+-------------+\n|stock_status|count|avg_available|\n+------------+-----+-------------+\n|Out of Stock|    4|          0.0|\n|   Low Stock|   15|         46.0|\n|Medium Stock|   17|        163.0|\n|    In Stock|  114|        534.0|\n+------------+-----+-------------+\n\n\n  Stock by warehouse:\n+------------+--------+-------------+---------------+------------------+\n|warehouse_id|products|total_on_hand|total_available|need_reorder_count|\n+------------+--------+-------------+---------------+------------------+\n|       WH001|      50|        28912|          25081|                 2|\n|       WH002|      50|        24969|          21266|                 8|\n|       WH003|      50|        21007|          18024|                 6|\n+------------+--------+-------------+---------------+------------------+\n\n  REORDER ALERT: 16 product-warehouse combos need restocking!\n\n[DONE] Silver Inventory complete!\n[NEXT] Cell 9 - Silver Clickstream\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 8: SILVER LAYER - INVENTORY TRANSFORMATION\n",
    "# ============================================================\n",
    "#\n",
    "# WHAT IS THIS TABLE?\n",
    "# -------------------\n",
    "# Daily snapshot of stock levels per product per warehouse.\n",
    "# Each row says: \"On this date, warehouse WH001 had 500 \n",
    "# units of PROD001, with 45 reserved for pending orders.\"\n",
    "#\n",
    "# This is a SNAPSHOT table (not transactional).\n",
    "# Every day, a new snapshot is taken of current stock levels.\n",
    "#\n",
    "# WHY IS INVENTORY DATA IMPORTANT?\n",
    "# - \"Which products are out of stock?\" -> lost revenue\n",
    "# - \"Which warehouses need restocking?\" -> supply chain\n",
    "# - \"What's our inventory turnover rate?\" -> efficiency\n",
    "# - Demand forecasting needs inventory context\n",
    "#\n",
    "# BRONZE ISSUES TO FIX:\n",
    "#   - 4 rows with NEGATIVE stock levels (data error from source)\n",
    "#     Negative stock is physically impossible.\n",
    "#     Could be caused by:\n",
    "#       a) Overselling (sold more than available)\n",
    "#       b) System sync delay between warehouses\n",
    "#       c) Data entry error\n",
    "#     Our fix: Set to 0 and FLAG the row\n",
    "#\n",
    "#   - Need derived columns:\n",
    "#     quantity_available = on_hand - reserved\n",
    "#     stock_status (Out of Stock / Low / Medium / In Stock)\n",
    "#     needs_reorder flag\n",
    "# ============================================================\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 1: Read Bronze Inventory\n",
    "# ----------------------------------------------------------\n",
    "df_inv_bronze = spark.read \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"inferSchema\", True) \\\n",
    "    .csv(BRONZE + \"/source5_inventory_csv/inventory.csv\")\n",
    "\n",
    "bronze_count = df_inv_bronze.count()\n",
    "print(\"STEP 1: Bronze Inventory read - \" + str(bronze_count) + \" rows\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 2: Fix negative stock levels\n",
    "# ----------------------------------------------------------\n",
    "# WHY NOT JUST DELETE NEGATIVE ROWS?\n",
    "# Deleting loses information. Instead, we:\n",
    "#   1. FLAG the row (_had_negative_stock = true)\n",
    "#   2. SET the value to 0 (physically correct)\n",
    "#   3. Keep the row (the product+warehouse combo is valid)\n",
    "#\n",
    "# This way, analysts can:\n",
    "#   - Find all flagged rows: WHERE _had_negative_stock = true\n",
    "#   - Investigate root cause with the operations team\n",
    "#   - Still get accurate stock totals (0 instead of -28)\n",
    "#\n",
    "# greatest(value, 0) returns whichever is larger.\n",
    "#   greatest(-28, 0) -> 0\n",
    "#   greatest(500, 0) -> 500\n",
    "# This is a clean way to \"floor\" a value at 0.\n",
    "\n",
    "neg_count = df_inv_bronze.filter(col(\"quantity_on_hand\") < 0).count()\n",
    "print(\"STEP 2: Found \" + str(neg_count) + \" negative stock rows\")\n",
    "\n",
    "df_inv_fixed = df_inv_bronze \\\n",
    "    .withColumn(\"_had_negative_stock\",\n",
    "        when(col(\"quantity_on_hand\") < 0, lit(True)).otherwise(lit(False))) \\\n",
    "    .withColumn(\"quantity_on_hand\",\n",
    "        when(col(\"quantity_on_hand\") < 0, lit(0))\n",
    "        .otherwise(col(\"quantity_on_hand\")))\n",
    "\n",
    "print(\"  Negative values set to 0 and flagged\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 3: Clean and standardize\n",
    "# ----------------------------------------------------------\n",
    "# WHY TRIM product_id?\n",
    "# Your data generator doesn't add trailing spaces, but in \n",
    "# real systems, CSV files often have trailing spaces.\n",
    "# If product_id = \"PROD001 \" (with space), then joining to \n",
    "# products table where product_id = \"PROD001\" would FAIL.\n",
    "# trim() is defensive coding - costs nothing, prevents bugs.\n",
    "\n",
    "df_inv_clean = df_inv_fixed \\\n",
    "    .withColumn(\"product_id\", trim(col(\"product_id\"))) \\\n",
    "    .withColumn(\"warehouse_id\", trim(col(\"warehouse_id\"))) \\\n",
    "    .withColumn(\"quantity_on_hand\", col(\"quantity_on_hand\").cast(\"int\")) \\\n",
    "    .withColumn(\"quantity_reserved\", col(\"quantity_reserved\").cast(\"int\")) \\\n",
    "    .withColumn(\"reorder_point\", col(\"reorder_point\").cast(\"int\")) \\\n",
    "    .withColumn(\"reorder_quantity\", col(\"reorder_quantity\").cast(\"int\")) \\\n",
    "    .withColumn(\"last_restock_date\", col(\"last_restock_date\").cast(\"date\")) \\\n",
    "    .withColumn(\"snapshot_date\", col(\"snapshot_date\").cast(\"date\"))\n",
    "\n",
    "print(\"STEP 3: Data types standardized\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 4: Derive business columns\n",
    "# ----------------------------------------------------------\n",
    "# QUANTITY AVAILABLE:\n",
    "#   = on_hand - reserved\n",
    "#   \"Reserved\" means items allocated to pending orders\n",
    "#   but not yet shipped. They're physically in the warehouse\n",
    "#   but spoken for.\n",
    "#   Available = what we can actually sell right now.\n",
    "#\n",
    "#   greatest(..., 0) prevents negative available \n",
    "#   (if reserved > on_hand due to timing)\n",
    "#\n",
    "# STOCK STATUS:\n",
    "#   Business teams don't think in numbers.\n",
    "#   They think: \"Is it in stock or not?\"\n",
    "#   We categorize into 4 buckets:\n",
    "#     Out of Stock: available = 0 -> URGENT, losing sales\n",
    "#     Low Stock: available <= reorder_point -> ORDER NOW\n",
    "#     Medium Stock: available <= 2x reorder_point -> PLAN ORDER\n",
    "#     In Stock: plenty of stock -> NO ACTION\n",
    "#\n",
    "# NEEDS REORDER:\n",
    "#   Simple boolean: should purchasing team place an order?\n",
    "#   When on_hand drops to/below reorder_point, flag it.\n",
    "#   In a real system, this could trigger an automated \n",
    "#   purchase order via an API.\n",
    "#\n",
    "# DAYS SINCE RESTOCK:\n",
    "#   How many days since the last restock?\n",
    "#   If it's been 90+ days and stock is low, something \n",
    "#   is wrong with the supply chain.\n",
    "\n",
    "df_inv_enriched = df_inv_clean \\\n",
    "    .withColumn(\"quantity_available\",\n",
    "        greatest(col(\"quantity_on_hand\") - col(\"quantity_reserved\"), lit(0))) \\\n",
    "    .withColumn(\"stock_status\",\n",
    "        when(col(\"quantity_available\") == 0, lit(\"Out of Stock\"))\n",
    "        .when(col(\"quantity_available\") <= col(\"reorder_point\"), lit(\"Low Stock\"))\n",
    "        .when(col(\"quantity_available\") <= col(\"reorder_point\") * 2, lit(\"Medium Stock\"))\n",
    "        .otherwise(lit(\"In Stock\"))) \\\n",
    "    .withColumn(\"needs_reorder\",\n",
    "        when(col(\"quantity_on_hand\") <= col(\"reorder_point\"), lit(True))\n",
    "        .otherwise(lit(False))) \\\n",
    "    .withColumn(\"days_since_restock\",\n",
    "        datediff(current_date(), col(\"last_restock_date\"))) \\\n",
    "    .withColumn(\"stock_value\",\n",
    "        col(\"quantity_on_hand\").cast(\"double\") * lit(50.0))\n",
    "\n",
    "# NOTE on stock_value: In a real system, we'd JOIN with products\n",
    "# to get actual cost_price. Here we use a placeholder average\n",
    "# cost of $50. In the Gold layer, we'll do the proper JOIN.\n",
    "\n",
    "print(\"STEP 4: Business columns derived\")\n",
    "print(\"  quantity_available, stock_status, needs_reorder\")\n",
    "print(\"  days_since_restock, stock_value\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 5: Deduplicate\n",
    "# ----------------------------------------------------------\n",
    "# Inventory has a COMPOSITE KEY:\n",
    "#   product_id + warehouse_id + snapshot_date\n",
    "# This means \"one row per product per warehouse per day\"\n",
    "# If we get duplicates, keep just one.\n",
    "\n",
    "df_inv_deduped = df_inv_enriched.dropDuplicates(\n",
    "    [\"product_id\", \"warehouse_id\", \"snapshot_date\"]\n",
    ")\n",
    "dedup_count = df_inv_deduped.count()\n",
    "dupes = bronze_count - dedup_count + 0  # adding 0 to avoid issues\n",
    "print(\"STEP 5: Deduplication - \" + str(dupes) + \" duplicates removed\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 6: Quality check\n",
    "# ----------------------------------------------------------\n",
    "df_inv_good = df_inv_deduped.filter(\n",
    "    col(\"product_id\").isNotNull() &\n",
    "    col(\"warehouse_id\").isNotNull() &\n",
    "    col(\"snapshot_date\").isNotNull()\n",
    ")\n",
    "\n",
    "bad_count = df_inv_deduped.count() - df_inv_good.count()\n",
    "print(\"STEP 6: Quality check - \" + str(bad_count) + \" bad records\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 7: Add metadata and write\n",
    "# ----------------------------------------------------------\n",
    "df_inv_silver = df_inv_good \\\n",
    "    .withColumn(\"_silver_processed_at\", current_timestamp()) \\\n",
    "    .withColumn(\"_silver_version\", lit(\"1.0\"))\n",
    "\n",
    "silver_inventory_path = SILVER + \"/inventory\"\n",
    "\n",
    "df_inv_silver.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", True) \\\n",
    "    .save(silver_inventory_path)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 8: Verify\n",
    "# ----------------------------------------------------------\n",
    "df_verify = spark.read.format(\"delta\").load(silver_inventory_path)\n",
    "final_count = df_verify.count()\n",
    "\n",
    "print(\"\")\n",
    "print(\"=\" * 65)\n",
    "print(\"SILVER INVENTORY - COMPLETE\")\n",
    "print(\"=\" * 65)\n",
    "print(\"  Source (Bronze):     \" + str(bronze_count) + \" rows\")\n",
    "print(\"  Negative stock fixed: \" + str(neg_count) + \" rows\")\n",
    "print(\"  Duplicates removed:  \" + str(dupes))\n",
    "print(\"  Quality rejected:    \" + str(bad_count))\n",
    "print(\"  Final Silver:        \" + str(final_count) + \" rows\")\n",
    "print(\"  Path:                \" + silver_inventory_path)\n",
    "\n",
    "print(\"\\n  Schema:\")\n",
    "df_verify.printSchema()\n",
    "\n",
    "print(\"\\n  Sample data:\")\n",
    "df_verify.select(\n",
    "    \"product_id\", \"warehouse_id\", \"quantity_on_hand\",\n",
    "    \"quantity_reserved\", \"quantity_available\",\n",
    "    \"stock_status\", \"needs_reorder\"\n",
    ").show(10, truncate=False)\n",
    "\n",
    "# Show the fixed negative stock rows\n",
    "print(\"\\n  Previously negative stock (now fixed to 0):\")\n",
    "df_verify.filter(col(\"_had_negative_stock\") == True).select(\n",
    "    \"product_id\", \"warehouse_id\", \"quantity_on_hand\",\n",
    "    \"stock_status\", \"_had_negative_stock\"\n",
    ").show(truncate=False)\n",
    "\n",
    "# Stock status summary\n",
    "print(\"\\n  Stock status distribution:\")\n",
    "df_verify.groupBy(\"stock_status\").agg(\n",
    "    count(\"*\").alias(\"count\"),\n",
    "    round(avg(\"quantity_available\"), 0).alias(\"avg_available\")\n",
    ").orderBy(\"count\").show()\n",
    "\n",
    "# Warehouse summary\n",
    "print(\"\\n  Stock by warehouse:\")\n",
    "df_verify.groupBy(\"warehouse_id\").agg(\n",
    "    count(\"*\").alias(\"products\"),\n",
    "    sum(\"quantity_on_hand\").alias(\"total_on_hand\"),\n",
    "    sum(\"quantity_available\").alias(\"total_available\"),\n",
    "    sum(col(\"needs_reorder\").cast(\"int\")).alias(\"need_reorder_count\")\n",
    ").orderBy(\"warehouse_id\").show()\n",
    "\n",
    "# Reorder alert\n",
    "reorder_count = df_verify.filter(col(\"needs_reorder\") == True).count()\n",
    "print(\"  REORDER ALERT: \" + str(reorder_count) + \" product-warehouse combos need restocking!\")\n",
    "\n",
    "print(\"\\n[DONE] Silver Inventory complete!\")\n",
    "print(\"[NEXT] Cell 9 - Silver Clickstream\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b39cd8a-ed2a-444e-8cb9-1e93ec1c5972",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1: Bronze Clickstream read - 3000 rows\nSTEP 2: geo_location flattened -> geo_city, geo_country\nSTEP 3: Data types standardized\nSTEP 4: Event-level columns derived\n  event_date, event_hour, day_name, is_weekend\n  is_anonymous, is_purchase_intent, funnel_stage\nSTEP 5: Deduplication - 0 duplicates removed\nSTEP 6: Event-level table written - 3000 rows\nSTEP 7: Session-level table written - 3000 sessions\n\n=================================================================\nSILVER CLICKSTREAM - COMPLETE (2 tables)\n=================================================================\n  TABLE 1: Event-Level\n    Rows:     3000\n    Path:     abfss://silver@dlsshopsmartdev123.dfs.core.windows.net/clickstream\n  TABLE 2: Session-Level\n    Rows:     3000\n    Path:     abfss://silver@dlsshopsmartdev123.dfs.core.windows.net/sessions\n\n  Event-level schema:\nroot\n |-- browser: string (nullable = true)\n |-- customer_id: string (nullable = true)\n |-- device_type: string (nullable = true)\n |-- event_id: string (nullable = true)\n |-- event_timestamp: timestamp (nullable = true)\n |-- event_type: string (nullable = true)\n |-- ip_address: string (nullable = true)\n |-- os: string (nullable = true)\n |-- page_url: string (nullable = true)\n |-- product_id: string (nullable = true)\n |-- referrer: string (nullable = true)\n |-- search_query: string (nullable = true)\n |-- session_id: string (nullable = true)\n |-- geo_city: string (nullable = true)\n |-- geo_country: string (nullable = true)\n |-- event_date: date (nullable = true)\n |-- event_hour: integer (nullable = true)\n |-- event_day_of_week: integer (nullable = true)\n |-- day_name: string (nullable = true)\n |-- is_weekend: boolean (nullable = true)\n |-- is_anonymous: boolean (nullable = true)\n |-- is_purchase_intent: boolean (nullable = true)\n |-- funnel_stage: string (nullable = true)\n |-- _silver_processed_at: timestamp (nullable = true)\n |-- _silver_version: string (nullable = true)\n\n\n  Event sample:\n+-------------+----------+-----------+----------------+----------+-----------+---------------+------------+\n|event_id     |session_id|customer_id|event_type      |product_id|device_type|funnel_stage   |is_anonymous|\n+-------------+----------+-----------+----------------+----------+-----------+---------------+------------+\n|EVTD0F15823-F|SESS8121  |CUST100    |remove_from_cart|PROD007   |tablet     |3-Consideration|false       |\n|EVT703BEF62-B|SESS2230  |CUST256    |add_to_cart     |PROD049   |desktop    |3-Consideration|false       |\n|EVTE0FAA31F-B|SESS8537  |CUST350    |checkout        |PROD027   |mobile     |4-Purchase     |false       |\n|EVT15C35858-4|SESS4733  |NULL       |remove_from_cart|PROD018   |mobile     |3-Consideration|true        |\n|EVT243C8CEB-C|SESS8192  |CUST240    |remove_from_cart|PROD035   |mobile     |3-Consideration|false       |\n+-------------+----------+-----------+----------------+----------+-----------+---------------+------------+\nonly showing top 5 rows\n\n  Event type distribution:\n+----------------+---------------+-----+\n|      event_type|   funnel_stage|count|\n+----------------+---------------+-----+\n|       page_view|    1-Awareness|  544|\n|    product_view|     2-Interest|  485|\n|     add_to_cart|3-Consideration|  470|\n|          search|3-Consideration|  511|\n|remove_from_cart|3-Consideration|  492|\n|        checkout|     4-Purchase|  498|\n+----------------+---------------+-----+\n\n\n  Device distribution:\n+-----------+-----+\n|device_type|count|\n+-----------+-----+\n|     tablet| 1052|\n|     mobile|  975|\n|    desktop|  973|\n+-----------+-----+\n\n\n  Anonymous vs logged-in events:\n+------------+-----+\n|is_anonymous|count|\n+------------+-----+\n|        true|  626|\n|       false| 2374|\n+------------+-----+\n\n\n  Session-level schema:\nroot\n |-- session_id: string (nullable = true)\n |-- customer_id: string (nullable = true)\n |-- device_type: string (nullable = true)\n |-- browser: string (nullable = true)\n |-- os: string (nullable = true)\n |-- total_events: long (nullable = true)\n |-- unique_event_types: long (nullable = true)\n |-- products_viewed: long (nullable = true)\n |-- session_start: timestamp (nullable = true)\n |-- session_end: timestamp (nullable = true)\n |-- page_views: long (nullable = true)\n |-- product_views: long (nullable = true)\n |-- cart_adds: long (nullable = true)\n |-- cart_removes: long (nullable = true)\n |-- checkouts: long (nullable = true)\n |-- searches: long (nullable = true)\n |-- entry_referrer: string (nullable = true)\n |-- geo_city: string (nullable = true)\n |-- geo_country: string (nullable = true)\n |-- session_duration_sec: long (nullable = true)\n |-- session_duration_min: double (nullable = true)\n |-- has_cart_activity: boolean (nullable = true)\n |-- has_checkout: boolean (nullable = true)\n |-- is_bounce: boolean (nullable = true)\n |-- is_anonymous: boolean (nullable = true)\n |-- engagement_level: string (nullable = true)\n |-- session_date: date (nullable = true)\n |-- _silver_processed_at: timestamp (nullable = true)\n |-- _silver_version: string (nullable = true)\n\n\n  Session sample:\n+----------+-----------+------------+---------------+--------------------+------------+----------------+\n|session_id|customer_id|total_events|products_viewed|session_duration_min|has_checkout|engagement_level|\n+----------+-----------+------------+---------------+--------------------+------------+----------------+\n|SESS5388  |CUST279    |1           |1              |0.0                 |false       |Bounce          |\n|SESS7148  |CUST383    |1           |1              |0.0                 |false       |Bounce          |\n|SESS2429  |NULL       |1           |1              |0.0                 |false       |Bounce          |\n|SESS2368  |CUST232    |1           |1              |0.0                 |true        |Bounce          |\n|SESS5224  |CUST332    |1           |1              |0.0                 |false       |Bounce          |\n+----------+-----------+------------+---------------+--------------------+------------+----------------+\nonly showing top 5 rows\n\n  Engagement distribution:\n+----------------+--------+----------+----------------+------------+\n|engagement_level|sessions|avg_events|avg_duration_min|avg_products|\n+----------------+--------+----------+----------------+------------+\n|          Bounce|    3000|       1.0|             0.0|         1.0|\n+----------------+--------+----------+----------------+------------+\n\n\n  CONVERSION FUNNEL:\n    Total Sessions:     3000\n    With Cart Activity: 470 (15 pct)\n    With Checkout:      498 (16 pct)\n\n[DONE] Silver Clickstream complete!\n[NEXT] Cell 10 - Silver Payments\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 9: SILVER LAYER - CLICKSTREAM TRANSFORMATION\n",
    "# ============================================================\n",
    "#\n",
    "# WHAT IS CLICKSTREAM DATA?\n",
    "# -------------------------\n",
    "# Every click, scroll, search, and action a user takes on\n",
    "# the website/app is recorded as an \"event\".\n",
    "#\n",
    "# This is the HIGHEST VOLUME data source (3000 rows here,\n",
    "# but in production: millions per day).\n",
    "#\n",
    "# WHY IS IT VALUABLE?\n",
    "# - Understanding user behavior (what do people browse?)\n",
    "# - Conversion funnel analysis (browse -> cart -> buy)\n",
    "# - Personalization (recommend based on browsing history)\n",
    "# - A/B testing (which page layout gets more clicks?)\n",
    "# - Fraud detection (bot-like clicking patterns)\n",
    "#\n",
    "# BRONZE ISSUES TO FIX:\n",
    "#   - 626 NULL customer_ids (20%) -> anonymous/not-logged-in users\n",
    "#     These are VALID events, not errors. Many users browse\n",
    "#     without logging in. We keep them but flag them.\n",
    "#   - Nested geo_location struct -> flatten\n",
    "#   - event_timestamp is string -> convert to timestamp\n",
    "#   - Derive: session metrics, time parts, funnel flags\n",
    "#\n",
    "# TWO OUTPUT TABLES:\n",
    "#   1. silver/clickstream  -> event-level (every single click)\n",
    "#   2. silver/sessions     -> session-level (aggregated per visit)\n",
    "#   Session aggregation is key for funnel analysis.\n",
    "# ============================================================\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 1: Read Bronze Clickstream\n",
    "# ----------------------------------------------------------\n",
    "# WHY NO multiLine=True?\n",
    "# Your clickstream.json is JSON-Lines format (one JSON per line):\n",
    "#   {\"event_id\": \"EVT001\", ...}\n",
    "#   {\"event_id\": \"EVT002\", ...}\n",
    "# This is the standard format for streaming/high-volume data\n",
    "# because each line can be parsed independently (parallelizable).\n",
    "# JSON array format (used by customers/products) requires\n",
    "# reading the entire file to find the closing bracket.\n",
    "\n",
    "df_clicks_bronze = spark.read \\\n",
    "    .json(BRONZE + \"/source4_clickstream_eventhub/clickstream.json\")\n",
    "\n",
    "bronze_count = df_clicks_bronze.count()\n",
    "print(\"STEP 1: Bronze Clickstream read - \" + str(bronze_count) + \" rows\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 2: Flatten nested geo_location struct\n",
    "# ----------------------------------------------------------\n",
    "# geo_location: struct\n",
    "#   |-- city: string\n",
    "#   |-- country: string\n",
    "#\n",
    "# Same technique as customers.address - dot notation to extract.\n",
    "\n",
    "df_clicks_flat = df_clicks_bronze \\\n",
    "    .withColumn(\"geo_city\", col(\"geo_location.city\")) \\\n",
    "    .withColumn(\"geo_country\", col(\"geo_location.country\")) \\\n",
    "    .drop(\"geo_location\")\n",
    "\n",
    "print(\"STEP 2: geo_location flattened -> geo_city, geo_country\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 3: Data type conversions and cleaning\n",
    "# ----------------------------------------------------------\n",
    "# WHY CONVERT event_timestamp FROM STRING?\n",
    "# As string: \"2025-01-15T10:30:00Z\" -> can't do time math\n",
    "# As timestamp: can calculate session duration, hourly patterns\n",
    "#\n",
    "# WHY TRIM all string columns?\n",
    "# Clickstream data comes from web browsers and mobile apps.\n",
    "# User agents, URLs, and other fields often have extra spaces\n",
    "# or inconsistent formatting. trim() normalizes everything.\n",
    "\n",
    "df_clicks_clean = df_clicks_flat \\\n",
    "    .withColumn(\"event_id\", trim(col(\"event_id\"))) \\\n",
    "    .withColumn(\"session_id\", trim(col(\"session_id\"))) \\\n",
    "    .withColumn(\"customer_id\", trim(col(\"customer_id\"))) \\\n",
    "    .withColumn(\"event_type\", lower(trim(col(\"event_type\")))) \\\n",
    "    .withColumn(\"event_timestamp\", to_timestamp(col(\"event_timestamp\"))) \\\n",
    "    .withColumn(\"product_id\", trim(col(\"product_id\"))) \\\n",
    "    .withColumn(\"device_type\", lower(trim(col(\"device_type\")))) \\\n",
    "    .withColumn(\"browser\", initcap(trim(col(\"browser\")))) \\\n",
    "    .withColumn(\"os\", initcap(trim(col(\"os\")))) \\\n",
    "    .withColumn(\"page_url\", trim(col(\"page_url\"))) \\\n",
    "    .withColumn(\"referrer\", lower(trim(col(\"referrer\")))) \\\n",
    "    .withColumn(\"search_query\", lower(trim(col(\"search_query\"))))\n",
    "\n",
    "print(\"STEP 3: Data types standardized\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 4: Derive event-level columns\n",
    "# ----------------------------------------------------------\n",
    "# EVENT DATE & TIME PARTS:\n",
    "#   Breaking timestamp into parts enables fast filtering:\n",
    "#   \"Show me events from last Monday\" -> WHERE day_name = 'Monday'\n",
    "#   \"Peak hours analysis\" -> GROUP BY event_hour\n",
    "#\n",
    "# IS_ANONYMOUS:\n",
    "#   Flag for events without a customer_id.\n",
    "#   20% of our events are anonymous (not logged in).\n",
    "#   This is normal for e-commerce - people browse before logging in.\n",
    "#\n",
    "# IS_PURCHASE_INTENT:\n",
    "#   Which event types signal buying intent?\n",
    "#   add_to_cart and checkout = high intent\n",
    "#   page_view = low intent (just browsing)\n",
    "#   This flag helps conversion funnel analysis:\n",
    "#   \"What % of sessions show purchase intent?\"\n",
    "#\n",
    "# FUNNEL STAGE:\n",
    "#   E-commerce conversion funnel:\n",
    "#     Awareness  -> user visits site (page_view)\n",
    "#     Interest   -> user looks at products (product_view)\n",
    "#     Desire     -> user adds to cart or searches (add_to_cart, search)\n",
    "#     Action     -> user buys (checkout)\n",
    "#   Mapping events to funnel stages enables funnel analysis:\n",
    "#   \"Where do we lose the most customers?\"\n",
    "\n",
    "df_clicks_enriched = df_clicks_clean \\\n",
    "    .withColumn(\"event_date\", to_date(col(\"event_timestamp\"))) \\\n",
    "    .withColumn(\"event_hour\", hour(col(\"event_timestamp\"))) \\\n",
    "    .withColumn(\"event_day_of_week\", dayofweek(col(\"event_timestamp\"))) \\\n",
    "    .withColumn(\"day_name\", date_format(col(\"event_timestamp\"), \"EEEE\")) \\\n",
    "    .withColumn(\"is_weekend\",\n",
    "        when(dayofweek(col(\"event_timestamp\")).isin(1, 7), lit(True))\n",
    "        .otherwise(lit(False))) \\\n",
    "    .withColumn(\"is_anonymous\",\n",
    "        when(col(\"customer_id\").isNull(), lit(True)).otherwise(lit(False))) \\\n",
    "    .withColumn(\"is_purchase_intent\",\n",
    "        when(col(\"event_type\").isin(\"add_to_cart\", \"checkout\"), lit(True))\n",
    "        .otherwise(lit(False))) \\\n",
    "    .withColumn(\"funnel_stage\",\n",
    "        when(col(\"event_type\") == \"page_view\", lit(\"1-Awareness\"))\n",
    "        .when(col(\"event_type\") == \"product_view\", lit(\"2-Interest\"))\n",
    "        .when(col(\"event_type\").isin(\"add_to_cart\", \"search\", \"remove_from_cart\"), lit(\"3-Consideration\"))\n",
    "        .when(col(\"event_type\") == \"checkout\", lit(\"4-Purchase\"))\n",
    "        .otherwise(lit(\"Other\")))\n",
    "\n",
    "print(\"STEP 4: Event-level columns derived\")\n",
    "print(\"  event_date, event_hour, day_name, is_weekend\")\n",
    "print(\"  is_anonymous, is_purchase_intent, funnel_stage\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 5: Deduplicate on event_id\n",
    "# ----------------------------------------------------------\n",
    "df_clicks_deduped = df_clicks_enriched.dropDuplicates([\"event_id\"])\n",
    "dedup_count = df_clicks_deduped.count()\n",
    "dupes = bronze_count - dedup_count\n",
    "print(\"STEP 5: Deduplication - \" + str(dupes) + \" duplicates removed\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 6: Write event-level Silver table\n",
    "# ----------------------------------------------------------\n",
    "df_clicks_silver = df_clicks_deduped \\\n",
    "    .withColumn(\"_silver_processed_at\", current_timestamp()) \\\n",
    "    .withColumn(\"_silver_version\", lit(\"1.0\"))\n",
    "\n",
    "silver_clicks_path = SILVER + \"/clickstream\"\n",
    "\n",
    "df_clicks_silver.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"event_date\") \\\n",
    "    .option(\"overwriteSchema\", True) \\\n",
    "    .save(silver_clicks_path)\n",
    "\n",
    "events_final = spark.read.format(\"delta\").load(silver_clicks_path).count()\n",
    "print(\"STEP 6: Event-level table written - \" + str(events_final) + \" rows\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 7: Build SESSION-level aggregation\n",
    "# ----------------------------------------------------------\n",
    "# WHAT IS A SESSION?\n",
    "# A session is one \"visit\" to the website.\n",
    "# Identified by session_id (set by the browser/app).\n",
    "# One session can have many events (page views, clicks, etc.)\n",
    "#\n",
    "# WHY AGGREGATE TO SESSION LEVEL?\n",
    "# Event-level data is too granular for most analysis.\n",
    "# Business questions are at the session level:\n",
    "#   \"Average session duration?\" -> session level\n",
    "#   \"Conversion rate?\" -> sessions with checkout / total sessions\n",
    "#   \"Bounce rate?\" -> sessions with only 1 event / total sessions\n",
    "#\n",
    "# WHAT WE CALCULATE PER SESSION:\n",
    "#   - total_events: how many clicks in this visit\n",
    "#   - products_viewed: how many unique products they looked at\n",
    "#   - session_start/end: first and last event timestamp\n",
    "#   - session_duration: end - start (in seconds)\n",
    "#   - has_cart_activity: did they add anything to cart?\n",
    "#   - has_checkout: did they complete a purchase?\n",
    "#   - bounce: only 1 event = left immediately\n",
    "#   - engagement_level: High/Medium/Low/Bounce based on event count\n",
    "\n",
    "df_sessions = df_clicks_deduped \\\n",
    "    .groupBy(\"session_id\", \"customer_id\", \"device_type\", \"browser\", \"os\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"total_events\"),\n",
    "        countDistinct(\"event_type\").alias(\"unique_event_types\"),\n",
    "        countDistinct(\"product_id\").alias(\"products_viewed\"),\n",
    "        min(\"event_timestamp\").alias(\"session_start\"),\n",
    "        max(\"event_timestamp\").alias(\"session_end\"),\n",
    "        sum(when(col(\"event_type\") == \"page_view\", lit(1)).otherwise(lit(0))).alias(\"page_views\"),\n",
    "        sum(when(col(\"event_type\") == \"product_view\", lit(1)).otherwise(lit(0))).alias(\"product_views\"),\n",
    "        sum(when(col(\"event_type\") == \"add_to_cart\", lit(1)).otherwise(lit(0))).alias(\"cart_adds\"),\n",
    "        sum(when(col(\"event_type\") == \"remove_from_cart\", lit(1)).otherwise(lit(0))).alias(\"cart_removes\"),\n",
    "        sum(when(col(\"event_type\") == \"checkout\", lit(1)).otherwise(lit(0))).alias(\"checkouts\"),\n",
    "        sum(when(col(\"event_type\") == \"search\", lit(1)).otherwise(lit(0))).alias(\"searches\"),\n",
    "        first(\"referrer\").alias(\"entry_referrer\"),\n",
    "        first(\"geo_city\").alias(\"geo_city\"),\n",
    "        first(\"geo_country\").alias(\"geo_country\")\n",
    "    ) \\\n",
    "    .withColumn(\"session_duration_sec\",\n",
    "        unix_timestamp(col(\"session_end\")) - unix_timestamp(col(\"session_start\"))) \\\n",
    "    .withColumn(\"session_duration_min\",\n",
    "        round(col(\"session_duration_sec\") / 60, 2)) \\\n",
    "    .withColumn(\"has_cart_activity\",\n",
    "        when(col(\"cart_adds\") > 0, lit(True)).otherwise(lit(False))) \\\n",
    "    .withColumn(\"has_checkout\",\n",
    "        when(col(\"checkouts\") > 0, lit(True)).otherwise(lit(False))) \\\n",
    "    .withColumn(\"is_bounce\",\n",
    "        when(col(\"total_events\") == 1, lit(True)).otherwise(lit(False))) \\\n",
    "    .withColumn(\"is_anonymous\",\n",
    "        when(col(\"customer_id\").isNull(), lit(True)).otherwise(lit(False))) \\\n",
    "    .withColumn(\"engagement_level\",\n",
    "        when(col(\"total_events\") >= 10, lit(\"High\"))\n",
    "        .when(col(\"total_events\") >= 5, lit(\"Medium\"))\n",
    "        .when(col(\"total_events\") >= 2, lit(\"Low\"))\n",
    "        .otherwise(lit(\"Bounce\"))) \\\n",
    "    .withColumn(\"session_date\", to_date(col(\"session_start\"))) \\\n",
    "    .withColumn(\"_silver_processed_at\", current_timestamp()) \\\n",
    "    .withColumn(\"_silver_version\", lit(\"1.0\"))\n",
    "\n",
    "# Write sessions\n",
    "silver_sessions_path = SILVER + \"/sessions\"\n",
    "\n",
    "df_sessions.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", True) \\\n",
    "    .save(silver_sessions_path)\n",
    "\n",
    "sessions_final = spark.read.format(\"delta\").load(silver_sessions_path).count()\n",
    "print(\"STEP 7: Session-level table written - \" + str(sessions_final) + \" sessions\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 8: Verify both tables\n",
    "# ----------------------------------------------------------\n",
    "df_events = spark.read.format(\"delta\").load(silver_clicks_path)\n",
    "df_sess = spark.read.format(\"delta\").load(silver_sessions_path)\n",
    "\n",
    "print(\"\")\n",
    "print(\"=\" * 65)\n",
    "print(\"SILVER CLICKSTREAM - COMPLETE (2 tables)\")\n",
    "print(\"=\" * 65)\n",
    "print(\"  TABLE 1: Event-Level\")\n",
    "print(\"    Rows:     \" + str(df_events.count()))\n",
    "print(\"    Path:     \" + silver_clicks_path)\n",
    "print(\"  TABLE 2: Session-Level\")\n",
    "print(\"    Rows:     \" + str(df_sess.count()))\n",
    "print(\"    Path:     \" + silver_sessions_path)\n",
    "\n",
    "print(\"\\n  Event-level schema:\")\n",
    "df_events.printSchema()\n",
    "\n",
    "print(\"\\n  Event sample:\")\n",
    "df_events.select(\n",
    "    \"event_id\", \"session_id\", \"customer_id\", \"event_type\",\n",
    "    \"product_id\", \"device_type\", \"funnel_stage\", \"is_anonymous\"\n",
    ").show(5, truncate=False)\n",
    "\n",
    "print(\"\\n  Event type distribution:\")\n",
    "df_events.groupBy(\"event_type\", \"funnel_stage\").count().orderBy(\"funnel_stage\").show()\n",
    "\n",
    "print(\"\\n  Device distribution:\")\n",
    "df_events.groupBy(\"device_type\").count().orderBy(desc(\"count\")).show()\n",
    "\n",
    "print(\"\\n  Anonymous vs logged-in events:\")\n",
    "df_events.groupBy(\"is_anonymous\").count().show()\n",
    "\n",
    "# Session-level stats\n",
    "print(\"\\n  Session-level schema:\")\n",
    "df_sess.printSchema()\n",
    "\n",
    "print(\"\\n  Session sample:\")\n",
    "df_sess.select(\n",
    "    \"session_id\", \"customer_id\", \"total_events\", \"products_viewed\",\n",
    "    \"session_duration_min\", \"has_checkout\", \"engagement_level\"\n",
    ").show(5, truncate=False)\n",
    "\n",
    "print(\"\\n  Engagement distribution:\")\n",
    "df_sess.groupBy(\"engagement_level\").agg(\n",
    "    count(\"*\").alias(\"sessions\"),\n",
    "    round(avg(\"total_events\"), 1).alias(\"avg_events\"),\n",
    "    round(avg(\"session_duration_min\"), 1).alias(\"avg_duration_min\"),\n",
    "    round(avg(\"products_viewed\"), 1).alias(\"avg_products\")\n",
    ").orderBy(\"engagement_level\").show()\n",
    "\n",
    "# Conversion funnel\n",
    "total_sessions = df_sess.count()\n",
    "cart_sessions = df_sess.filter(col(\"has_cart_activity\") == True).count()\n",
    "checkout_sessions = df_sess.filter(col(\"has_checkout\") == True).count()\n",
    "\n",
    "print(\"\\n  CONVERSION FUNNEL:\")\n",
    "print(\"    Total Sessions:     \" + str(total_sessions))\n",
    "print(\"    With Cart Activity: \" + str(cart_sessions) + \" (\" + str(int(cart_sessions * 100 / total_sessions)) + \" pct)\")\n",
    "print(\"    With Checkout:      \" + str(checkout_sessions) + \" (\" + str(int(checkout_sessions * 100 / total_sessions)) + \" pct)\")\n",
    "\n",
    "print(\"\\n[DONE] Silver Clickstream complete!\")\n",
    "print(\"[NEXT] Cell 10 - Silver Payments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9294aba2-5b4d-4d67-a08d-716ae2c60405",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1: Bronze Payments read - 2000 rows\nSTEP 2: Data types converted\n  Unparseable timestamps: 0\nSTEP 3: Payment status standardized\nSTEP 4: Risk and fraud signals added\n  risk_level, is_high_risk, is_off_hours\n  is_high_amount, fraud_signal_count, fraud_risk_label\nSTEP 5: Deduplication - 0 duplicates removed\nSTEP 6: Quality check - 0 bad records\n\n=================================================================\nSILVER PAYMENTS - COMPLETE\n=================================================================\n  Source (Bronze):     2000 rows\n  Duplicates removed:  0\n  Quality rejected:    0\n  Final Silver:        2000 rows\n  Path:                abfss://silver@dlsshopsmartdev123.dfs.core.windows.net/payments\n\n  Schema:\nroot\n |-- amount: double (nullable = true)\n |-- card_type: string (nullable = true)\n |-- currency: string (nullable = true)\n |-- device_fingerprint: string (nullable = true)\n |-- gateway_response_code: string (nullable = true)\n |-- ip_address: string (nullable = true)\n |-- is_international: boolean (nullable = true)\n |-- order_id: string (nullable = true)\n |-- payment_method: string (nullable = true)\n |-- risk_score: integer (nullable = true)\n |-- status: string (nullable = true)\n |-- transaction_id: string (nullable = true)\n |-- transaction_timestamp: timestamp (nullable = true)\n |-- status_original: string (nullable = true)\n |-- risk_level: string (nullable = true)\n |-- is_high_risk: boolean (nullable = true)\n |-- transaction_hour: integer (nullable = true)\n |-- is_off_hours: boolean (nullable = true)\n |-- is_high_amount: boolean (nullable = true)\n |-- fraud_signal_count: integer (nullable = true)\n |-- fraud_risk_label: string (nullable = true)\n |-- transaction_date: date (nullable = true)\n |-- transaction_day_of_week: integer (nullable = true)\n |-- day_name: string (nullable = true)\n |-- _silver_processed_at: timestamp (nullable = true)\n |-- _silver_version: string (nullable = true)\n\n\n  Sample data:\n+--------------+--------+-------+-------+--------------+----------+----------+----------------+\n|transaction_id|order_id|amount |status |payment_method|risk_score|risk_level|fraud_risk_label|\n+--------------+--------+-------+-------+--------------+----------+----------+----------------+\n|TXN814B2773   |ORD00286|4417.39|SUCCESS|cod           |95        |CRITICAL  |CRITICAL        |\n|TXN3F55C520   |ORD00438|450.52 |SUCCESS|credit_card   |59        |MEDIUM    |LOW             |\n|TXN84651203   |ORD00593|1204.92|SUCCESS|debit_card    |94        |CRITICAL  |CRITICAL        |\n|TXN89D75557   |ORD00829|2093.51|SUCCESS|cod           |22        |LOW       |MEDIUM          |\n|TXN86015C93   |ORD01004|598.64 |SUCCESS|cod           |71        |HIGH      |HIGH            |\n+--------------+--------+-------+-------+--------------+----------+----------+----------------+\nonly showing top 5 rows\n\n  Payment status distribution:\n+-------+-----+------------+----------+\n| status|count|total_amount|avg_amount|\n+-------+-----+------------+----------+\n|SUCCESS| 1693|  4248875.87|   2509.67|\n| FAILED|  307|   758478.37|   2470.61|\n+-------+-----+------------+----------+\n\n\n  Risk level distribution:\n+----------+-----+--------------+\n|risk_level|count|avg_risk_score|\n+----------+-----+--------------+\n|  VERY_LOW|  395|          10.1|\n|       LOW|  372|          29.3|\n|    MEDIUM|  412|          50.0|\n|      HIGH|  401|          69.4|\n|  CRITICAL|  420|          90.1|\n+----------+-----+--------------+\n\n\n  Fraud risk label distribution:\n+----------------+-----+-----------+\n|fraud_risk_label|count|avg_signals|\n+----------------+-----+-----------+\n|        CRITICAL|  376|        3.1|\n|            HIGH|  750|        2.0|\n|          MEDIUM|  655|        1.0|\n|             LOW|  219|        0.0|\n+----------------+-----+-----------+\n\n\n  Payment method distribution:\n+--------------+-----+------------+\n|payment_method|count|total_amount|\n+--------------+-----+------------+\n|    debit_card|  415|   999332.69|\n|        wallet|  407|   994227.24|\n|           cod|  399|  1032451.51|\n|           upi|  391|   989781.75|\n|   credit_card|  388|   991561.05|\n+--------------+-----+------------+\n\n  FRAUD ALERT: 1126 transactions flagged as HIGH/CRITICAL risk\n\n  Sample HIGH/CRITICAL risk transactions:\n+--------------+-------+----------+------------+--------------+----------------+------------------+\n|transaction_id|amount |risk_score|is_off_hours|is_high_amount|is_international|fraud_signal_count|\n+--------------+-------+----------+------------+--------------+----------------+------------------+\n|TXN814B2773   |4417.39|95        |false       |true          |true            |3                 |\n|TXN84651203   |1204.92|94        |true        |false         |true            |3                 |\n|TXN86015C93   |598.64 |71        |false       |false         |true            |2                 |\n|TXN2469281D   |138.93 |72        |true        |false         |true            |3                 |\n|TXN431DC0C9   |2643.7 |50        |true        |true          |false           |2                 |\n+--------------+-------+----------+------------+--------------+----------------+------------------+\nonly showing top 5 rows\n\n[DONE] Silver Payments complete!\n=================================================================\nALL SILVER TABLES COMPLETE!\n=================================================================\n  1. silver/orders       - 1948 rows (52 quarantined)\n  2. silver/order_items  - 4904 rows\n  3. silver/customers    - 500 rows (PII masked)\n  4. silver/products     - 50 rows (attributes flattened)\n  5. silver/inventory    - 150 rows (neg stock fixed)\n  6. silver/clickstream  - 3000 rows (events)\n  7. silver/sessions     - 3000 rows (session aggregates)\n  8. silver/payments     - 2000 rows (fraud signals added)\n\n[NEXT] Cell 11 - GOLD LAYER (Star Schema: Dimensions + Facts)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 10: SILVER LAYER - PAYMENTS TRANSFORMATION\n",
    "# ============================================================\n",
    "#\n",
    "# WHAT IS THIS TABLE?\n",
    "# -------------------\n",
    "# Every payment transaction linked to an order.\n",
    "# One order = one payment (in our data).\n",
    "# In real systems, one order could have:\n",
    "#   - Multiple payment attempts (first failed, retry succeeded)\n",
    "#   - Split payments (half credit card, half wallet)\n",
    "#   - Refund transactions\n",
    "#\n",
    "# WHY IS PAYMENTS DATA IMPORTANT?\n",
    "# - Revenue recognition: only \"success\" payments = real revenue\n",
    "# - Fraud detection: high risk scores, unusual patterns\n",
    "# - Payment method analysis: which methods have highest failure?\n",
    "# - Reconciliation: match payments to orders to bank statements\n",
    "#\n",
    "# BRONZE ISSUES TO FIX:\n",
    "#   - transaction_timestamp is string -> convert to timestamp\n",
    "#   - Standardize status values\n",
    "#   - Add risk level categorization\n",
    "#   - Add fraud signal flags\n",
    "#   - Link validation (every payment should have a valid order)\n",
    "#\n",
    "# INTERVIEW GOLD: This table enables FRAUD DETECTION ML later.\n",
    "# We're building features now that the ML model will use.\n",
    "# ============================================================\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 1: Read Bronze Payments\n",
    "# ----------------------------------------------------------\n",
    "df_pay_bronze = spark.read \\\n",
    "    .option(\"multiLine\", True) \\\n",
    "    .json(BRONZE + \"/source6_payments_api/payments.json\")\n",
    "\n",
    "bronze_count = df_pay_bronze.count()\n",
    "print(\"STEP 1: Bronze Payments read - \" + str(bronze_count) + \" rows\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 2: Data type conversions\n",
    "# ----------------------------------------------------------\n",
    "# WHY IS transaction_timestamp TRICKY?\n",
    "# Payment systems often use different date formats:\n",
    "#   - \"2025-01-15T10:30:00Z\" (ISO 8601 with Z)\n",
    "#   - \"2025-01-15T10:30:00\" (ISO without timezone)\n",
    "#   - \"01/15/2025 10:30:00\" (US format)\n",
    "#   - \"15-01-2025 10:30:00\" (EU format)\n",
    "#\n",
    "# coalesce() with multiple to_timestamp() patterns tries\n",
    "# each format until one works. This handles inconsistencies\n",
    "# WITHOUT failing the pipeline.\n",
    "#\n",
    "# In your data, it's consistent ISO format, but defensive\n",
    "# coding protects against future source changes.\n",
    "\n",
    "df_pay_typed = df_pay_bronze \\\n",
    "    .withColumn(\"transaction_id\", trim(col(\"transaction_id\"))) \\\n",
    "    .withColumn(\"order_id\", trim(col(\"order_id\"))) \\\n",
    "    .withColumn(\"payment_method\", lower(trim(col(\"payment_method\")))) \\\n",
    "    .withColumn(\"card_type\", lower(trim(col(\"card_type\")))) \\\n",
    "    .withColumn(\"amount\", col(\"amount\").cast(\"double\")) \\\n",
    "    .withColumn(\"currency\", upper(trim(coalesce(col(\"currency\"), lit(\"USD\"))))) \\\n",
    "    .withColumn(\"status\", lower(trim(col(\"status\")))) \\\n",
    "    .withColumn(\"gateway_response_code\", trim(col(\"gateway_response_code\"))) \\\n",
    "    .withColumn(\"is_international\", col(\"is_international\").cast(\"boolean\")) \\\n",
    "    .withColumn(\"risk_score\", col(\"risk_score\").cast(\"int\")) \\\n",
    "    .withColumn(\"ip_address\", trim(col(\"ip_address\"))) \\\n",
    "    .withColumn(\"device_fingerprint\", trim(col(\"device_fingerprint\"))) \\\n",
    "    .withColumn(\"transaction_timestamp\",\n",
    "        coalesce(\n",
    "            to_timestamp(col(\"transaction_timestamp\"), \"yyyy-MM-dd'T'HH:mm:ss'Z'\"),\n",
    "            to_timestamp(col(\"transaction_timestamp\"), \"yyyy-MM-dd'T'HH:mm:ss\"),\n",
    "            to_timestamp(col(\"transaction_timestamp\"), \"yyyy-MM-dd HH:mm:ss\"),\n",
    "            to_timestamp(col(\"transaction_timestamp\"))\n",
    "        ))\n",
    "\n",
    "# Check how many timestamps parsed successfully\n",
    "null_ts = df_pay_typed.filter(col(\"transaction_timestamp\").isNull()).count()\n",
    "print(\"STEP 2: Data types converted\")\n",
    "print(\"  Unparseable timestamps: \" + str(null_ts))\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 3: Standardize payment status\n",
    "# ----------------------------------------------------------\n",
    "# WHY STANDARDIZE?\n",
    "# Different payment gateways return different status strings:\n",
    "#   Stripe: \"succeeded\", \"failed\"\n",
    "#   PayPal: \"COMPLETED\", \"DENIED\"\n",
    "#   Razorpay: \"captured\", \"failed\"\n",
    "#\n",
    "# We normalize to: SUCCESS, FAILED, PENDING, REFUNDED\n",
    "# This makes reporting consistent regardless of gateway.\n",
    "\n",
    "df_pay_std = df_pay_typed \\\n",
    "    .withColumn(\"status_original\", col(\"status\")) \\\n",
    "    .withColumn(\"status\",\n",
    "        when(col(\"status\").isin(\"success\", \"succeeded\", \"completed\", \"captured\", \"approved\"),\n",
    "            lit(\"SUCCESS\"))\n",
    "        .when(col(\"status\").isin(\"failed\", \"failure\", \"declined\", \"denied\", \"rejected\"),\n",
    "            lit(\"FAILED\"))\n",
    "        .when(col(\"status\").isin(\"pending\", \"processing\", \"initiated\"),\n",
    "            lit(\"PENDING\"))\n",
    "        .when(col(\"status\").isin(\"refunded\", \"reversed\", \"voided\"),\n",
    "            lit(\"REFUNDED\"))\n",
    "        .otherwise(upper(col(\"status\"))))\n",
    "\n",
    "print(\"STEP 3: Payment status standardized\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 4: Add risk categorization and fraud signals\n",
    "# ----------------------------------------------------------\n",
    "# RISK LEVEL:\n",
    "#   Based on the numeric risk_score (1-99) from payment gateway.\n",
    "#   We categorize into business-friendly labels.\n",
    "#   These become features for our ML fraud model later.\n",
    "#\n",
    "# FRAUD SIGNAL FLAGS:\n",
    "#   Each flag represents a suspicious pattern:\n",
    "#\n",
    "#   is_high_risk: risk_score >= 70\n",
    "#     Gateway's own risk assessment is high\n",
    "#\n",
    "#   is_off_hours: transaction between 11 PM and 5 AM\n",
    "#     Legitimate purchases happen less at 3 AM\n",
    "#     Fraudsters often operate during off-hours\n",
    "#\n",
    "#   is_high_amount: amount > $2000\n",
    "#     Large purchases have higher fraud risk\n",
    "#     Card testing often uses small amounts, but\n",
    "#     actual fraud uses large amounts\n",
    "#\n",
    "#   is_international: cross-border transaction\n",
    "#     Higher fraud risk than domestic\n",
    "#\n",
    "#   fraud_signal_count: how many flags are TRUE\n",
    "#     0 signals = very safe\n",
    "#     1 signal = monitor\n",
    "#     2 signals = review\n",
    "#     3+ signals = likely fraud, block and investigate\n",
    "#\n",
    "# WHY BUILD THESE IN SILVER (not Gold)?\n",
    "#   These flags are reusable across multiple Gold tables\n",
    "#   and ML models. Computing them once in Silver ensures\n",
    "#   consistency. The ML model in Cell 16+ will use these\n",
    "#   as features.\n",
    "\n",
    "df_pay_enriched = df_pay_std \\\n",
    "    .withColumn(\"risk_level\",\n",
    "        when(col(\"risk_score\") >= 80, lit(\"CRITICAL\"))\n",
    "        .when(col(\"risk_score\") >= 60, lit(\"HIGH\"))\n",
    "        .when(col(\"risk_score\") >= 40, lit(\"MEDIUM\"))\n",
    "        .when(col(\"risk_score\") >= 20, lit(\"LOW\"))\n",
    "        .otherwise(lit(\"VERY_LOW\"))) \\\n",
    "    .withColumn(\"is_high_risk\",\n",
    "        when(col(\"risk_score\") >= 70, lit(True)).otherwise(lit(False))) \\\n",
    "    .withColumn(\"transaction_hour\", hour(col(\"transaction_timestamp\"))) \\\n",
    "    .withColumn(\"is_off_hours\",\n",
    "        when((hour(col(\"transaction_timestamp\")) >= 23) |\n",
    "             (hour(col(\"transaction_timestamp\")) <= 5),\n",
    "            lit(True)).otherwise(lit(False))) \\\n",
    "    .withColumn(\"is_high_amount\",\n",
    "        when(col(\"amount\") > 2000, lit(True)).otherwise(lit(False))) \\\n",
    "    .withColumn(\"fraud_signal_count\",\n",
    "        col(\"is_high_risk\").cast(\"int\") +\n",
    "        col(\"is_off_hours\").cast(\"int\") +\n",
    "        col(\"is_high_amount\").cast(\"int\") +\n",
    "        col(\"is_international\").cast(\"int\")) \\\n",
    "    .withColumn(\"fraud_risk_label\",\n",
    "        when(col(\"fraud_signal_count\") >= 3, lit(\"CRITICAL\"))\n",
    "        .when(col(\"fraud_signal_count\") >= 2, lit(\"HIGH\"))\n",
    "        .when(col(\"fraud_signal_count\") >= 1, lit(\"MEDIUM\"))\n",
    "        .otherwise(lit(\"LOW\"))) \\\n",
    "    .withColumn(\"transaction_date\", to_date(col(\"transaction_timestamp\"))) \\\n",
    "    .withColumn(\"transaction_day_of_week\", dayofweek(col(\"transaction_timestamp\"))) \\\n",
    "    .withColumn(\"day_name\", date_format(col(\"transaction_timestamp\"), \"EEEE\"))\n",
    "\n",
    "print(\"STEP 4: Risk and fraud signals added\")\n",
    "print(\"  risk_level, is_high_risk, is_off_hours\")\n",
    "print(\"  is_high_amount, fraud_signal_count, fraud_risk_label\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 5: Deduplicate on transaction_id\n",
    "# ----------------------------------------------------------\n",
    "df_pay_deduped = df_pay_enriched.dropDuplicates([\"transaction_id\"])\n",
    "dedup_count = df_pay_deduped.count()\n",
    "dupes = bronze_count - dedup_count\n",
    "print(\"STEP 5: Deduplication - \" + str(dupes) + \" duplicates removed\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 6: Quality check\n",
    "# ----------------------------------------------------------\n",
    "df_pay_good = df_pay_deduped.filter(\n",
    "    col(\"transaction_id\").isNotNull() &\n",
    "    col(\"order_id\").isNotNull() &\n",
    "    col(\"amount\").isNotNull() &\n",
    "    (col(\"amount\") > 0) &\n",
    "    col(\"transaction_timestamp\").isNotNull()\n",
    ")\n",
    "\n",
    "bad_count = df_pay_deduped.count() - df_pay_good.count()\n",
    "print(\"STEP 6: Quality check - \" + str(bad_count) + \" bad records\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 7: Add metadata and write\n",
    "# ----------------------------------------------------------\n",
    "df_pay_silver = df_pay_good \\\n",
    "    .withColumn(\"_silver_processed_at\", current_timestamp()) \\\n",
    "    .withColumn(\"_silver_version\", lit(\"1.0\"))\n",
    "\n",
    "silver_payments_path = SILVER + \"/payments\"\n",
    "\n",
    "df_pay_silver.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", True) \\\n",
    "    .save(silver_payments_path)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 8: Verify\n",
    "# ----------------------------------------------------------\n",
    "df_verify = spark.read.format(\"delta\").load(silver_payments_path)\n",
    "final_count = df_verify.count()\n",
    "\n",
    "print(\"\")\n",
    "print(\"=\" * 65)\n",
    "print(\"SILVER PAYMENTS - COMPLETE\")\n",
    "print(\"=\" * 65)\n",
    "print(\"  Source (Bronze):     \" + str(bronze_count) + \" rows\")\n",
    "print(\"  Duplicates removed:  \" + str(dupes))\n",
    "print(\"  Quality rejected:    \" + str(bad_count))\n",
    "print(\"  Final Silver:        \" + str(final_count) + \" rows\")\n",
    "print(\"  Path:                \" + silver_payments_path)\n",
    "\n",
    "print(\"\\n  Schema:\")\n",
    "df_verify.printSchema()\n",
    "\n",
    "print(\"\\n  Sample data:\")\n",
    "df_verify.select(\n",
    "    \"transaction_id\", \"order_id\", \"amount\", \"status\",\n",
    "    \"payment_method\", \"risk_score\", \"risk_level\", \"fraud_risk_label\"\n",
    ").show(5, truncate=False)\n",
    "\n",
    "# Payment status\n",
    "print(\"\\n  Payment status distribution:\")\n",
    "df_verify.groupBy(\"status\").agg(\n",
    "    count(\"*\").alias(\"count\"),\n",
    "    round(sum(\"amount\"), 2).alias(\"total_amount\"),\n",
    "    round(avg(\"amount\"), 2).alias(\"avg_amount\")\n",
    ").orderBy(desc(\"count\")).show()\n",
    "\n",
    "# Risk level\n",
    "print(\"\\n  Risk level distribution:\")\n",
    "df_verify.groupBy(\"risk_level\").agg(\n",
    "    count(\"*\").alias(\"count\"),\n",
    "    round(avg(\"risk_score\"), 1).alias(\"avg_risk_score\")\n",
    ").orderBy(\"avg_risk_score\").show()\n",
    "\n",
    "# Fraud signals\n",
    "print(\"\\n  Fraud risk label distribution:\")\n",
    "df_verify.groupBy(\"fraud_risk_label\").agg(\n",
    "    count(\"*\").alias(\"count\"),\n",
    "    round(avg(\"fraud_signal_count\"), 1).alias(\"avg_signals\")\n",
    ").orderBy(desc(\"avg_signals\")).show()\n",
    "\n",
    "# Payment method\n",
    "print(\"\\n  Payment method distribution:\")\n",
    "df_verify.groupBy(\"payment_method\").agg(\n",
    "    count(\"*\").alias(\"count\"),\n",
    "    round(sum(\"amount\"), 2).alias(\"total_amount\")\n",
    ").orderBy(desc(\"count\")).show()\n",
    "\n",
    "# High risk transactions\n",
    "high_risk_count = df_verify.filter(col(\"fraud_risk_label\").isin(\"HIGH\", \"CRITICAL\")).count()\n",
    "print(\"  FRAUD ALERT: \" + str(high_risk_count) + \" transactions flagged as HIGH/CRITICAL risk\")\n",
    "\n",
    "print(\"\\n  Sample HIGH/CRITICAL risk transactions:\")\n",
    "df_verify.filter(col(\"fraud_risk_label\").isin(\"HIGH\", \"CRITICAL\")).select(\n",
    "    \"transaction_id\", \"amount\", \"risk_score\", \"is_off_hours\",\n",
    "    \"is_high_amount\", \"is_international\", \"fraud_signal_count\"\n",
    ").show(5, truncate=False)\n",
    "\n",
    "print(\"\\n[DONE] Silver Payments complete!\")\n",
    "print(\"=\" * 65)\n",
    "print(\"ALL SILVER TABLES COMPLETE!\")\n",
    "print(\"=\" * 65)\n",
    "print(\"  1. silver/orders       - 1948 rows (52 quarantined)\")\n",
    "print(\"  2. silver/order_items  - 4904 rows\")\n",
    "print(\"  3. silver/customers    - 500 rows (PII masked)\")\n",
    "print(\"  4. silver/products     - 50 rows (attributes flattened)\")\n",
    "print(\"  5. silver/inventory    - 150 rows (neg stock fixed)\")\n",
    "print(\"  6. silver/clickstream  - 3000 rows (events)\")\n",
    "print(\"  7. silver/sessions     - 3000 rows (session aggregates)\")\n",
    "print(\"  8. silver/payments     - 2000 rows (fraud signals added)\")\n",
    "print(\"\")\n",
    "print(\"[NEXT] Cell 11 - GOLD LAYER (Star Schema: Dimensions + Facts)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "906b81f1-227c-4757-a10c-eea3191d1127",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1: Generated 1096 dates (2024-2026)\nSTEP 2: Date attributes built - 21 columns\n\n=================================================================\nGOLD dim_date - COMPLETE\n=================================================================\n  Total dates:  1096 rows\n  Date range:   2024-01-01 to 2026-12-31\n  Columns:      21\n  Path:         abfss://gold@dlsshopsmartdev123.dfs.core.windows.net/dim_date\n\n  Schema:\nroot\n |-- date_key: integer (nullable = true)\n |-- full_date: date (nullable = true)\n |-- year: integer (nullable = true)\n |-- quarter: integer (nullable = true)\n |-- month: integer (nullable = true)\n |-- day: integer (nullable = true)\n |-- month_name: string (nullable = true)\n |-- month_short: string (nullable = true)\n |-- day_of_week: integer (nullable = true)\n |-- day_name: string (nullable = true)\n |-- day_short: string (nullable = true)\n |-- week_of_year: integer (nullable = true)\n |-- day_of_year: integer (nullable = true)\n |-- is_weekend: boolean (nullable = true)\n |-- is_month_start: boolean (nullable = true)\n |-- is_month_end: boolean (nullable = true)\n |-- is_holiday_season: boolean (nullable = true)\n |-- quarter_label: string (nullable = true)\n |-- month_year_label: string (nullable = true)\n |-- half_year: integer (nullable = true)\n |-- half_year_label: string (nullable = true)\n\n\n  Sample (first 5 days of 2025):\n+--------+----------+----+-------+-----+---+----------+-----------+-----------+---------+---------+------------+-----------+----------+--------------+------------+-----------------+-------------+----------------+---------+---------------+\n|date_key|full_date |year|quarter|month|day|month_name|month_short|day_of_week|day_name |day_short|week_of_year|day_of_year|is_weekend|is_month_start|is_month_end|is_holiday_season|quarter_label|month_year_label|half_year|half_year_label|\n+--------+----------+----+-------+-----+---+----------+-----------+-----------+---------+---------+------------+-----------+----------+--------------+------------+-----------------+-------------+----------------+---------+---------------+\n|20250101|2025-01-01|2025|1      |1    |1  |January   |Jan        |4          |Wednesday|Wed      |1           |1          |false     |true          |false       |true             |Q1-2025      |Jan-2025        |1        |H1             |\n|20250102|2025-01-02|2025|1      |1    |2  |January   |Jan        |5          |Thursday |Thu      |1           |2          |false     |false         |false       |true             |Q1-2025      |Jan-2025        |1        |H1             |\n|20250103|2025-01-03|2025|1      |1    |3  |January   |Jan        |6          |Friday   |Fri      |1           |3          |false     |false         |false       |true             |Q1-2025      |Jan-2025        |1        |H1             |\n|20250104|2025-01-04|2025|1      |1    |4  |January   |Jan        |7          |Saturday |Sat      |1           |4          |true      |false         |false       |true             |Q1-2025      |Jan-2025        |1        |H1             |\n|20250105|2025-01-05|2025|1      |1    |5  |January   |Jan        |1          |Sunday   |Sun      |1           |5          |true      |false         |false       |true             |Q1-2025      |Jan-2025        |1        |H1             |\n+--------+----------+----+-------+-----+---+----------+-----------+-----------+---------+---------+------------+-----------+----------+--------------+------------+-----------------+-------------+----------------+---------+---------------+\n\n\n  Weekend vs Weekday count:\n+----------+-----+\n|is_weekend|count|\n+----------+-----+\n|      true|  312|\n|     false|  784|\n+----------+-----+\n\n\n  Records per year:\n+----+-----+\n|year|count|\n+----+-----+\n|2024|  366|\n|2025|  365|\n|2026|  365|\n+----+-----+\n\n\n  Quarter labels sample:\n+-------------+\n|quarter_label|\n+-------------+\n|      Q1-2024|\n|      Q1-2025|\n|      Q1-2026|\n|      Q2-2024|\n|      Q2-2025|\n|      Q2-2026|\n|      Q3-2024|\n|      Q3-2025|\n|      Q3-2026|\n|      Q4-2024|\n|      Q4-2025|\n|      Q4-2026|\n+-------------+\n\n[DONE] Gold dim_date complete!\n[NEXT] Cell 12 - Gold dim_customer\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 11: GOLD LAYER - dim_date (Date Dimension)\n",
    "# ============================================================\n",
    "#\n",
    "# WHAT IS A DATE DIMENSION?\n",
    "# -------------------------\n",
    "# A pre-built calendar table with one row per day.\n",
    "# Instead of calculating \"is this a weekend?\" or \n",
    "# \"what quarter is this?\" in every query, we calculate\n",
    "# it ONCE and store it.\n",
    "#\n",
    "# Every fact table joins to dim_date via a date key.\n",
    "# Example: fact_sales.order_date_key -> dim_date.date_key\n",
    "#\n",
    "# WHY NOT JUST USE THE DATE COLUMN DIRECTLY?\n",
    "# 1. Performance: pre-computed attributes avoid runtime functions\n",
    "# 2. Consistency: everyone uses the same fiscal year definition\n",
    "# 3. Filtering: \"Show Q4 2025\" is just WHERE quarter = 4\n",
    "# 4. Custom attributes: holidays, fiscal periods, pay days\n",
    "#    can't be derived from a raw date\n",
    "#\n",
    "# THIS IS ASKED IN EVERY DATA ENGINEERING INTERVIEW.\n",
    "# \"How would you design a date dimension?\" is a classic question.\n",
    "#\n",
    "# We generate dates from 2024-01-01 to 2026-12-31 (3 years)\n",
    "# to cover all historical and future data.\n",
    "# ============================================================\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 1: Generate a sequence of dates\n",
    "# ----------------------------------------------------------\n",
    "# HOW THIS WORKS:\n",
    "# 1. sequence() creates an array of dates from start to end\n",
    "#    sequence(2024-01-01, 2026-12-31) = [2024-01-01, 2024-01-02, ...]\n",
    "# 2. explode() converts the array into individual rows\n",
    "#    One array of 1096 dates -> 1096 rows\n",
    "#\n",
    "# This is a common Spark pattern for generating reference data\n",
    "# without needing an external source.\n",
    "\n",
    "df_dates = spark.sql(\"\"\"\n",
    "    SELECT explode(sequence(\n",
    "        to_date('2024-01-01'),\n",
    "        to_date('2026-12-31'),\n",
    "        interval 1 day\n",
    "    )) as date\n",
    "\"\"\")\n",
    "\n",
    "total_dates = df_dates.count()\n",
    "print(\"STEP 1: Generated \" + str(total_dates) + \" dates (2024-2026)\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 2: Build all date attributes\n",
    "# ----------------------------------------------------------\n",
    "# Each attribute serves a specific analytics purpose:\n",
    "#\n",
    "# date_key (int): 20250115 format\n",
    "#   Used as JOIN key with fact tables.\n",
    "#   Integer keys are faster than date JOINs.\n",
    "#   Format: YYYYMMDD\n",
    "#\n",
    "# year, quarter, month, day: Basic components\n",
    "#   \"Revenue by quarter\" -> GROUP BY quarter\n",
    "#\n",
    "# month_name, day_name: Human-readable labels\n",
    "#   Dashboards show \"January\" not \"1\"\n",
    "#\n",
    "# week_of_year: For weekly reporting\n",
    "#   \"Week-over-week growth\" needs this\n",
    "#\n",
    "# is_weekend: Saturday/Sunday flag\n",
    "#   \"Weekend vs weekday sales patterns\"\n",
    "#   dayofweek returns 1=Sunday, 7=Saturday\n",
    "#\n",
    "# is_month_start, is_month_end: Boundary flags\n",
    "#   Financial reporting often focuses on month boundaries\n",
    "#\n",
    "# quarter_label: \"Q1-2025\" format\n",
    "#   Clean label for dashboard filters\n",
    "#\n",
    "# day_of_year: 1-365\n",
    "#   Useful for year-over-year comparison at the day level\n",
    "\n",
    "df_dim_date = df_dates.select(\n",
    "    date_format(col(\"date\"), \"yyyyMMdd\").cast(\"int\").alias(\"date_key\"),\n",
    "    col(\"date\").alias(\"full_date\"),\n",
    "    year(\"date\").alias(\"year\"),\n",
    "    quarter(\"date\").alias(\"quarter\"),\n",
    "    month(\"date\").alias(\"month\"),\n",
    "    dayofmonth(\"date\").alias(\"day\"),\n",
    "    date_format(\"date\", \"MMMM\").alias(\"month_name\"),\n",
    "    date_format(\"date\", \"MMM\").alias(\"month_short\"),\n",
    "    dayofweek(\"date\").alias(\"day_of_week\"),\n",
    "    date_format(\"date\", \"EEEE\").alias(\"day_name\"),\n",
    "    date_format(\"date\", \"EEE\").alias(\"day_short\"),\n",
    "    weekofyear(\"date\").alias(\"week_of_year\"),\n",
    "    dayofyear(\"date\").alias(\"day_of_year\"),\n",
    "    when(dayofweek(\"date\").isin(1, 7), lit(True)).otherwise(lit(False)).alias(\"is_weekend\"),\n",
    "    when(dayofmonth(\"date\") == 1, lit(True)).otherwise(lit(False)).alias(\"is_month_start\"),\n",
    "    when(col(\"date\") == last_day(\"date\"), lit(True)).otherwise(lit(False)).alias(\"is_month_end\"),\n",
    "    when(month(\"date\").isin(11, 12, 1), lit(True)).otherwise(lit(False)).alias(\"is_holiday_season\"),\n",
    "    concat(lit(\"Q\"), quarter(\"date\").cast(\"string\"), lit(\"-\"), year(\"date\").cast(\"string\")).alias(\"quarter_label\"),\n",
    "    concat(date_format(\"date\", \"MMM\"), lit(\"-\"), year(\"date\").cast(\"string\")).alias(\"month_year_label\"),\n",
    "    when(month(\"date\") <= 6, lit(1)).otherwise(lit(2)).alias(\"half_year\"),\n",
    "    when(month(\"date\") <= 6, lit(\"H1\")).otherwise(lit(\"H2\")).alias(\"half_year_label\")\n",
    ")\n",
    "\n",
    "print(\"STEP 2: Date attributes built - \" + str(len(df_dim_date.columns)) + \" columns\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 3: Write to Gold layer\n",
    "# ----------------------------------------------------------\n",
    "gold_dim_date_path = GOLD + \"/dim_date\"\n",
    "\n",
    "df_dim_date.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", True) \\\n",
    "    .save(gold_dim_date_path)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 4: Verify\n",
    "# ----------------------------------------------------------\n",
    "df_verify = spark.read.format(\"delta\").load(gold_dim_date_path)\n",
    "final_count = df_verify.count()\n",
    "\n",
    "print(\"\")\n",
    "print(\"=\" * 65)\n",
    "print(\"GOLD dim_date - COMPLETE\")\n",
    "print(\"=\" * 65)\n",
    "print(\"  Total dates:  \" + str(final_count) + \" rows\")\n",
    "print(\"  Date range:   2024-01-01 to 2026-12-31\")\n",
    "print(\"  Columns:      \" + str(len(df_verify.columns)))\n",
    "print(\"  Path:         \" + gold_dim_date_path)\n",
    "\n",
    "print(\"\\n  Schema:\")\n",
    "df_verify.printSchema()\n",
    "\n",
    "print(\"\\n  Sample (first 5 days of 2025):\")\n",
    "df_verify.filter(\n",
    "    (col(\"year\") == 2025) & (col(\"month\") == 1) & (col(\"day\") <= 5)\n",
    ").orderBy(\"full_date\").show(truncate=False)\n",
    "\n",
    "print(\"\\n  Weekend vs Weekday count:\")\n",
    "df_verify.groupBy(\"is_weekend\").count().show()\n",
    "\n",
    "print(\"\\n  Records per year:\")\n",
    "df_verify.groupBy(\"year\").count().orderBy(\"year\").show()\n",
    "\n",
    "print(\"\\n  Quarter labels sample:\")\n",
    "df_verify.select(\"quarter_label\").distinct().orderBy(\"quarter_label\").show(12)\n",
    "\n",
    "print(\"[DONE] Gold dim_date complete!\")\n",
    "print(\"[NEXT] Cell 12 - Gold dim_customer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67787763-a312-46a2-9b0a-98724bfda11a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1: Silver Customers read - 500 rows\nSTEP 2: dim_customer built with surrogate key and SCD2 columns\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/spark/python/pyspark/sql/connect/expressions.py:1061: UserWarning: WARN WindowExpression: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Columns: 29\n\n=================================================================\nGOLD dim_customer - COMPLETE\n=================================================================\n  Source (Silver):  500 rows\n  Final Gold:       500 rows\n  Columns:          29\n  Path:             abfss://gold@dlsshopsmartdev123.dfs.core.windows.net/dim_customer\n\n  Schema:\nroot\n |-- customer_sk: integer (nullable = true)\n |-- customer_id: string (nullable = true)\n |-- first_name: string (nullable = true)\n |-- last_name: string (nullable = true)\n |-- first_name_initial: string (nullable = true)\n |-- last_name_initial: string (nullable = true)\n |-- email_hash: string (nullable = true)\n |-- email_domain: string (nullable = true)\n |-- phone_masked: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- date_of_birth: date (nullable = true)\n |-- age: integer (nullable = true)\n |-- age_group: string (nullable = true)\n |-- loyalty_tier: string (nullable = true)\n |-- registration_date: date (nullable = true)\n |-- customer_tenure_days: integer (nullable = true)\n |-- tenure_category: string (nullable = true)\n |-- address_city: string (nullable = true)\n |-- address_state: string (nullable = true)\n |-- address_zip: string (nullable = true)\n |-- address_country: string (nullable = true)\n |-- pref_categories: string (nullable = true)\n |-- pref_communication: string (nullable = true)\n |-- has_email: boolean (nullable = true)\n |-- effective_start_date: date (nullable = true)\n |-- effective_end_date: date (nullable = true)\n |-- is_current: boolean (nullable = true)\n |-- _gold_processed_at: timestamp (nullable = true)\n |-- _gold_version: string (nullable = true)\n\n\n  Surrogate key sample:\n+-----------+-----------+----------+---------+------+---------+------------+\n|customer_sk|customer_id|first_name|last_name|gender|age_group|loyalty_tier|\n+-----------+-----------+----------+---------+------+---------+------------+\n|1          |CUST001    |Kayla     |Gibson   |Male  |18-24    |Platinum    |\n|2          |CUST002    |Julia     |Wells    |Other |45-54    |Bronze      |\n|3          |CUST003    |Sophia    |Smith    |Other |25-34    |Platinum    |\n|4          |CUST004    |Matthew   |Hobbs    |Female|65+      |Gold        |\n|5          |CUST005    |Anthony   |Collins  |Male  |35-44    |Gold        |\n+-----------+-----------+----------+---------+------+---------+------------+\nonly showing top 5 rows\n\n  SCD Type 2 columns:\n+-----------+-----------+------------+--------------------+------------------+----------+\n|customer_sk|customer_id|loyalty_tier|effective_start_date|effective_end_date|is_current|\n+-----------+-----------+------------+--------------------+------------------+----------+\n|1          |CUST001    |Platinum    |2025-12-06          |9999-12-31        |true      |\n|2          |CUST002    |Bronze      |2025-08-09          |9999-12-31        |true      |\n|3          |CUST003    |Platinum    |2024-06-24          |9999-12-31        |true      |\n|4          |CUST004    |Gold        |2025-02-18          |9999-12-31        |true      |\n|5          |CUST005    |Gold        |2024-11-30          |9999-12-31        |true      |\n+-----------+-----------+------------+--------------------+------------------+----------+\nonly showing top 5 rows\n\n  Top 10 states by customer count:\n+-------------+-----+\n|address_state|count|\n+-------------+-----+\n|           NC|   15|\n|           LA|   14|\n|           HI|   14|\n|           ME|   14|\n|           NH|   13|\n|           CO|   13|\n|           NM|   12|\n|           VT|   12|\n|           UT|   12|\n|           MS|   12|\n+-------------+-----+\nonly showing top 10 rows\n\n  Loyalty tier by age group:\n+---------+------------+-----+\n|age_group|loyalty_tier|count|\n+---------+------------+-----+\n|    18-24|      Bronze|    9|\n|    18-24|        Gold|   14|\n|    18-24|    Platinum|   12|\n|    18-24|      Silver|   16|\n|    25-34|      Bronze|   12|\n|    25-34|        Gold|   20|\n|    25-34|    Platinum|   27|\n|    25-34|      Silver|   21|\n|    35-44|      Bronze|   22|\n|    35-44|        Gold|   19|\n|    35-44|    Platinum|   25|\n|    35-44|      Silver|   20|\n|    45-54|      Bronze|   25|\n|    45-54|        Gold|   17|\n|    45-54|    Platinum|   28|\n|    45-54|      Silver|   14|\n|    55-64|      Bronze|   17|\n|    55-64|        Gold|   21|\n|    55-64|    Platinum|   21|\n|    55-64|      Silver|   15|\n|      65+|      Bronze|   33|\n|      65+|        Gold|   32|\n|      65+|    Platinum|   32|\n|      65+|      Silver|   28|\n+---------+------------+-----+\n\n[DONE] Gold dim_customer complete!\n[NEXT] Cell 13 - Gold dim_product\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 12: GOLD LAYER - dim_customer (Customer Dimension)\n",
    "# ============================================================\n",
    "#\n",
    "# WHAT IS dim_customer?\n",
    "# ---------------------\n",
    "# The customer dimension describes WHO made the purchase.\n",
    "# Every row in fact_sales will JOIN to dim_customer to answer:\n",
    "#   \"Revenue by loyalty tier\"\n",
    "#   \"Orders by age group\"\n",
    "#   \"Customer count by state\"\n",
    "#\n",
    "# SOURCE: silver/customers (already cleaned & PII masked)\n",
    "#\n",
    "# WHAT WE ADD IN GOLD:\n",
    "# Gold layer is about BUSINESS PERSPECTIVE, not cleaning.\n",
    "# We select only the columns analysts need and add\n",
    "# surrogate keys.\n",
    "#\n",
    "# SURROGATE KEY vs NATURAL KEY:\n",
    "# - Natural key: customer_id (\"CUST001\") - from source system\n",
    "# - Surrogate key: customer_sk (1, 2, 3...) - generated by us\n",
    "#\n",
    "# WHY SURROGATE KEYS?\n",
    "# 1. Performance: integer JOINs faster than string JOINs\n",
    "# 2. Independence: if source changes CUST001 to C-001, \n",
    "#    our surrogate key stays the same\n",
    "# 3. History: enables SCD Type 2 (tracking changes over time)\n",
    "# 4. Standard practice: every data warehouse uses them\n",
    "#\n",
    "# SCD TYPE 2 (Slowly Changing Dimension):\n",
    "# When a customer moves from \"Silver\" to \"Gold\" loyalty tier,\n",
    "# we want to keep BOTH versions:\n",
    "#   customer_sk=1, CUST001, Silver, effective 2024-01-01 to 2025-06-01\n",
    "#   customer_sk=2, CUST001, Gold,   effective 2025-06-01 to 9999-12-31\n",
    "#\n",
    "# This lets us analyze: \"What was the customer's tier WHEN \n",
    "# they placed that order in March?\" — historical accuracy.\n",
    "#\n",
    "# For this project, we implement a SIMPLIFIED SCD Type 2\n",
    "# (single version per customer since we have point-in-time data).\n",
    "# In production, you'd run this incrementally with MERGE.\n",
    "# ============================================================\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 1: Read Silver Customers\n",
    "# ----------------------------------------------------------\n",
    "df_cust_silver = spark.read.format(\"delta\").load(SILVER + \"/customers\")\n",
    "\n",
    "silver_count = df_cust_silver.count()\n",
    "print(\"STEP 1: Silver Customers read - \" + str(silver_count) + \" rows\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 2: Build dim_customer with surrogate key\n",
    "# ----------------------------------------------------------\n",
    "# monotonically_increasing_id() generates unique IDs.\n",
    "# NOTE: These IDs are NOT sequential (1, 2, 3...).\n",
    "# They're globally unique across partitions.\n",
    "# For a true sequential key, we use row_number() instead.\n",
    "#\n",
    "# WHY row_number() OVER orderBy(customer_id)?\n",
    "# - Guarantees sequential: 1, 2, 3, 4, ...\n",
    "# - Deterministic: same input always gives same keys\n",
    "# - Sorted: customer_sk order matches customer_id order\n",
    "# This is important for debugging and testing.\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "window_sk = Window.orderBy(\"customer_id\")\n",
    "\n",
    "df_dim_customer = df_cust_silver \\\n",
    "    .withColumn(\"customer_sk\", row_number().over(window_sk)) \\\n",
    "    .select(\n",
    "        col(\"customer_sk\"),\n",
    "        col(\"customer_id\"),\n",
    "        col(\"first_name\"),\n",
    "        col(\"last_name\"),\n",
    "        col(\"first_name_initial\"),\n",
    "        col(\"last_name_initial\"),\n",
    "        col(\"email_hash\"),\n",
    "        col(\"email_domain\"),\n",
    "        col(\"phone_masked\"),\n",
    "        col(\"gender\"),\n",
    "        col(\"date_of_birth\"),\n",
    "        col(\"age\"),\n",
    "        col(\"age_group\"),\n",
    "        col(\"loyalty_tier\"),\n",
    "        col(\"registration_date\"),\n",
    "        col(\"customer_tenure_days\"),\n",
    "        col(\"tenure_category\"),\n",
    "        col(\"address_city\"),\n",
    "        col(\"address_state\"),\n",
    "        col(\"address_zip\"),\n",
    "        col(\"address_country\"),\n",
    "        col(\"pref_categories\"),\n",
    "        col(\"pref_communication\"),\n",
    "        col(\"has_email\"),\n",
    "        # SCD Type 2 columns\n",
    "        col(\"registration_date\").alias(\"effective_start_date\"),\n",
    "        to_date(lit(\"9999-12-31\")).alias(\"effective_end_date\"),\n",
    "        lit(True).alias(\"is_current\"),\n",
    "        current_timestamp().alias(\"_gold_processed_at\"),\n",
    "        lit(\"1.0\").alias(\"_gold_version\")\n",
    "    )\n",
    "\n",
    "print(\"STEP 2: dim_customer built with surrogate key and SCD2 columns\")\n",
    "print(\"  Columns: \" + str(len(df_dim_customer.columns)))\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 3: Write to Gold layer\n",
    "# ----------------------------------------------------------\n",
    "gold_dim_customer_path = GOLD + \"/dim_customer\"\n",
    "\n",
    "df_dim_customer.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", True) \\\n",
    "    .save(gold_dim_customer_path)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 4: Verify\n",
    "# ----------------------------------------------------------\n",
    "df_verify = spark.read.format(\"delta\").load(gold_dim_customer_path)\n",
    "final_count = df_verify.count()\n",
    "\n",
    "print(\"\")\n",
    "print(\"=\" * 65)\n",
    "print(\"GOLD dim_customer - COMPLETE\")\n",
    "print(\"=\" * 65)\n",
    "print(\"  Source (Silver):  \" + str(silver_count) + \" rows\")\n",
    "print(\"  Final Gold:       \" + str(final_count) + \" rows\")\n",
    "print(\"  Columns:          \" + str(len(df_verify.columns)))\n",
    "print(\"  Path:             \" + gold_dim_customer_path)\n",
    "\n",
    "print(\"\\n  Schema:\")\n",
    "df_verify.printSchema()\n",
    "\n",
    "# Show surrogate key assignment\n",
    "print(\"\\n  Surrogate key sample:\")\n",
    "df_verify.select(\n",
    "    \"customer_sk\", \"customer_id\", \"first_name\", \"last_name\",\n",
    "    \"gender\", \"age_group\", \"loyalty_tier\"\n",
    ").orderBy(\"customer_sk\").show(5, truncate=False)\n",
    "\n",
    "# Show SCD2 columns\n",
    "print(\"\\n  SCD Type 2 columns:\")\n",
    "df_verify.select(\n",
    "    \"customer_sk\", \"customer_id\", \"loyalty_tier\",\n",
    "    \"effective_start_date\", \"effective_end_date\", \"is_current\"\n",
    ").orderBy(\"customer_sk\").show(5, truncate=False)\n",
    "\n",
    "# Show geographic distribution\n",
    "print(\"\\n  Top 10 states by customer count:\")\n",
    "df_verify.groupBy(\"address_state\").count().orderBy(desc(\"count\")).show(10)\n",
    "\n",
    "# Loyalty by age group\n",
    "print(\"\\n  Loyalty tier by age group:\")\n",
    "df_verify.groupBy(\"age_group\", \"loyalty_tier\") \\\n",
    "    .count() \\\n",
    "    .orderBy(\"age_group\", \"loyalty_tier\") \\\n",
    "    .show(25)\n",
    "\n",
    "print(\"[DONE] Gold dim_customer complete!\")\n",
    "print(\"[NEXT] Cell 13 - Gold dim_product\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f461b48-bdb8-42d3-9d26-956d529550d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1: Silver Products read - 50 rows\nSTEP 2: dim_product built - 25 columns\n\n=================================================================\nGOLD dim_product - COMPLETE\n=================================================================\n  Source (Silver):  50 rows\n  Final Gold:       50 rows\n  Columns:          25\n  Path:             abfss://gold@dlsshopsmartdev123.dfs.core.windows.net/dim_product\n\n  Schema:\nroot\n |-- product_sk: integer (nullable = true)\n |-- product_id: string (nullable = true)\n |-- product_name: string (nullable = true)\n |-- category: string (nullable = true)\n |-- sub_category: string (nullable = true)\n |-- brand: string (nullable = true)\n |-- supplier_id: string (nullable = true)\n |-- current_price: double (nullable = true)\n |-- cost_price: double (nullable = true)\n |-- profit_margin: double (nullable = true)\n |-- margin_pct: double (nullable = true)\n |-- price_tier: string (nullable = true)\n |-- weight_kg: double (nullable = true)\n |-- rating: double (nullable = true)\n |-- review_count: integer (nullable = true)\n |-- rating_category: string (nullable = true)\n |-- is_active: boolean (nullable = true)\n |-- attr_battery_life: string (nullable = true)\n |-- attr_colors: string (nullable = true)\n |-- attr_connectivity: string (nullable = true)\n |-- product_created_at: timestamp (nullable = true)\n |-- product_updated_at: timestamp (nullable = true)\n |-- product_age_days: integer (nullable = true)\n |-- _gold_processed_at: timestamp (nullable = true)\n |-- _gold_version: string (nullable = true)\n\n\n  Surrogate key sample:\n+----------+----------+--------------------------------------+-----------+----------------+-------------+----------+\n|product_sk|product_id|product_name                          |category   |brand           |current_price|price_tier|\n+----------+----------+--------------------------------------+-----------+----------------+-------------+----------+\n|1         |PROD001   |Sharable bifurcated algorithm         |Electronics|Sanchez-taylor  |644.83       |Luxury    |\n|2         |PROD002   |Optimized 5thgeneration algorithm     |Electronics|Hall Plc        |46.31        |Budget    |\n|3         |PROD003   |Total needs-based instruction set     |Home       |Lawrence-pacheco|457.47       |Premium   |\n|4         |PROD004   |Mandatory even-keeled collaboration   |Home       |Garcia-james    |227.08       |Premium   |\n|5         |PROD005   |Balanced upward-trending knowledgebase|Beauty     |Dudley Group    |733.78       |Luxury    |\n+----------+----------+--------------------------------------+-----------+----------------+-------------+----------+\nonly showing top 5 rows\n\n  Category analysis:\n+-----------+--------+---------+--------------+----------+------------+\n|   category|products|avg_price|avg_margin_pct|avg_rating|active_count|\n+-----------+--------+---------+--------------+----------+------------+\n|       Home|      15|   478.27|         46.39|       4.0|          13|\n|    Fashion|      10|   480.67|         42.99|      3.71|          10|\n|Electronics|      10|   559.28|         46.07|      3.62|           7|\n|     Beauty|       9|   500.44|         42.19|       3.1|           8|\n|     Sports|       6|   604.22|         46.94|      3.78|           4|\n+-----------+--------+---------+--------------+----------+------------+\n\n\n  Price tier analysis:\n+----------+--------+---------+---------+--------------+\n|price_tier|products|min_price|max_price|avg_margin_pct|\n+----------+--------+---------+---------+--------------+\n|    Budget|       3|     18.2|    46.31|          39.3|\n| Mid-Range|       5|    60.14|   140.53|         45.12|\n|   Premium|      13|   227.08|   457.47|         42.77|\n|    Luxury|      29|   521.44|   995.21|         46.49|\n+----------+--------+---------+---------+--------------+\n\n[DONE] Gold dim_product complete!\n[NEXT] Cell 14 - Gold fact_sales (THE MAIN FACT TABLE)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 13: GOLD LAYER - dim_product (Product Dimension)\n",
    "# ============================================================\n",
    "#\n",
    "# WHAT IS dim_product?\n",
    "# --------------------\n",
    "# Describes WHAT was sold. Every fact_sales row joins here.\n",
    "# Business questions this enables:\n",
    "#   \"Revenue by category\" \n",
    "#   \"Top 10 brands by sales\"\n",
    "#   \"Average margin by price tier\"\n",
    "#   \"Products to discontinue (low rating + low sales)\"\n",
    "#\n",
    "# SOURCE: silver/products (already cleaned, attributes flattened)\n",
    "#\n",
    "# WHAT WE ADD IN GOLD:\n",
    "# - Surrogate key (product_sk)\n",
    "# - Only business-relevant columns (drop technical metadata)\n",
    "# - Consistent column ordering (keys first, then attributes)\n",
    "#\n",
    "# DESIGN PRINCIPLE:\n",
    "# Dimension tables should be WIDE (many columns) but SHORT \n",
    "# (few rows). Our dim_product has 50 rows and ~25 columns.\n",
    "# This is typical - a retail company might have 50,000 products\n",
    "# with 30+ attributes each.\n",
    "#\n",
    "# The \"width\" of dimensions is what makes star schema powerful.\n",
    "# One JOIN to dim_product gives you access to category, brand,\n",
    "# price tier, rating, margin - all in one hop.\n",
    "# ============================================================\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 1: Read Silver Products\n",
    "# ----------------------------------------------------------\n",
    "df_prod_silver = spark.read.format(\"delta\").load(SILVER + \"/products\")\n",
    "\n",
    "silver_count = df_prod_silver.count()\n",
    "print(\"STEP 1: Silver Products read - \" + str(silver_count) + \" rows\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 2: Build dim_product with surrogate key\n",
    "# ----------------------------------------------------------\n",
    "# Same row_number() approach as dim_customer.\n",
    "# Ordered by product_id for deterministic key assignment.\n",
    "\n",
    "window_sk = Window.orderBy(\"product_id\")\n",
    "\n",
    "df_dim_product = df_prod_silver \\\n",
    "    .withColumn(\"product_sk\", row_number().over(window_sk)) \\\n",
    "    .select(\n",
    "        # Keys (always first in a dimension)\n",
    "        col(\"product_sk\"),\n",
    "        col(\"product_id\"),\n",
    "        # Descriptive attributes\n",
    "        col(\"product_name\"),\n",
    "        col(\"category\"),\n",
    "        col(\"sub_category\"),\n",
    "        col(\"brand\"),\n",
    "        col(\"supplier_id\"),\n",
    "        # Price and cost\n",
    "        col(\"price\").alias(\"current_price\"),\n",
    "        col(\"cost_price\"),\n",
    "        col(\"profit_margin\"),\n",
    "        col(\"margin_pct\"),\n",
    "        col(\"price_tier\"),\n",
    "        # Product characteristics\n",
    "        col(\"weight_kg\"),\n",
    "        col(\"rating\"),\n",
    "        col(\"review_count\"),\n",
    "        col(\"rating_category\"),\n",
    "        col(\"is_active\"),\n",
    "        # Flattened attributes\n",
    "        col(\"attr_battery_life\"),\n",
    "        col(\"attr_colors\"),\n",
    "        col(\"attr_connectivity\"),\n",
    "        # Time attributes\n",
    "        col(\"created_at\").alias(\"product_created_at\"),\n",
    "        col(\"updated_at\").alias(\"product_updated_at\"),\n",
    "        col(\"product_age_days\"),\n",
    "        # Gold metadata\n",
    "        current_timestamp().alias(\"_gold_processed_at\"),\n",
    "        lit(\"1.0\").alias(\"_gold_version\")\n",
    "    )\n",
    "\n",
    "print(\"STEP 2: dim_product built - \" + str(len(df_dim_product.columns)) + \" columns\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 3: Write to Gold layer\n",
    "# ----------------------------------------------------------\n",
    "gold_dim_product_path = GOLD + \"/dim_product\"\n",
    "\n",
    "df_dim_product.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", True) \\\n",
    "    .save(gold_dim_product_path)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 4: Verify\n",
    "# ----------------------------------------------------------\n",
    "df_verify = spark.read.format(\"delta\").load(gold_dim_product_path)\n",
    "final_count = df_verify.count()\n",
    "\n",
    "print(\"\")\n",
    "print(\"=\" * 65)\n",
    "print(\"GOLD dim_product - COMPLETE\")\n",
    "print(\"=\" * 65)\n",
    "print(\"  Source (Silver):  \" + str(silver_count) + \" rows\")\n",
    "print(\"  Final Gold:       \" + str(final_count) + \" rows\")\n",
    "print(\"  Columns:          \" + str(len(df_verify.columns)))\n",
    "print(\"  Path:             \" + gold_dim_product_path)\n",
    "\n",
    "print(\"\\n  Schema:\")\n",
    "df_verify.printSchema()\n",
    "\n",
    "# Surrogate key sample\n",
    "print(\"\\n  Surrogate key sample:\")\n",
    "df_verify.select(\n",
    "    \"product_sk\", \"product_id\", \"product_name\",\n",
    "    \"category\", \"brand\", \"current_price\", \"price_tier\"\n",
    ").orderBy(\"product_sk\").show(5, truncate=False)\n",
    "\n",
    "# Category and margin analysis\n",
    "print(\"\\n  Category analysis:\")\n",
    "df_verify.groupBy(\"category\").agg(\n",
    "    count(\"*\").alias(\"products\"),\n",
    "    round(avg(\"current_price\"), 2).alias(\"avg_price\"),\n",
    "    round(avg(\"margin_pct\"), 2).alias(\"avg_margin_pct\"),\n",
    "    round(avg(\"rating\"), 2).alias(\"avg_rating\"),\n",
    "    sum(col(\"is_active\").cast(\"int\")).alias(\"active_count\")\n",
    ").orderBy(desc(\"products\")).show()\n",
    "\n",
    "# Price tier analysis\n",
    "print(\"\\n  Price tier analysis:\")\n",
    "df_verify.groupBy(\"price_tier\").agg(\n",
    "    count(\"*\").alias(\"products\"),\n",
    "    round(min(\"current_price\"), 2).alias(\"min_price\"),\n",
    "    round(max(\"current_price\"), 2).alias(\"max_price\"),\n",
    "    round(avg(\"margin_pct\"), 2).alias(\"avg_margin_pct\")\n",
    ").orderBy(\"min_price\").show()\n",
    "\n",
    "print(\"[DONE] Gold dim_product complete!\")\n",
    "print(\"[NEXT] Cell 14 - Gold fact_sales (THE MAIN FACT TABLE)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc578771-a35e-47db-9db7-609e2fb263eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1: Silver data read\n  Orders:      1948 rows\n  Order Items: 4904 rows\nSTEP 2: Orders JOIN Order Items = 4780 rows\n  Orphaned items (no matching order): 124\nSTEP 3: fact_sales built - 30 columns\n\n=================================================================\nGOLD fact_sales - COMPLETE\n=================================================================\n  Orders (Silver):    1948\n  Items (Silver):     4904\n  Joined rows:        4780\n  Final fact_sales:   4780 rows\n  Columns:            30\n  Partitioned by:     order_year, order_month\n  Path:               abfss://gold@dlsshopsmartdev123.dfs.core.windows.net/fact_sales\n\n  Schema:\nroot\n |-- sales_key: string (nullable = true)\n |-- order_date_key: integer (nullable = true)\n |-- customer_id: string (nullable = true)\n |-- product_id: string (nullable = true)\n |-- order_id: string (nullable = true)\n |-- item_id: string (nullable = true)\n |-- quantity: integer (nullable = true)\n |-- unit_price: double (nullable = true)\n |-- line_total: double (nullable = true)\n |-- discount_percent: double (nullable = true)\n |-- discount_amount: double (nullable = true)\n |-- net_line_total: double (nullable = true)\n |-- shipping_amount: double (nullable = true)\n |-- order_date: timestamp (nullable = true)\n |-- order_status: string (nullable = true)\n |-- payment_method: string (nullable = true)\n |-- channel: string (nullable = true)\n |-- item_status: string (nullable = true)\n |-- is_cancelled: boolean (nullable = true)\n |-- is_returned: boolean (nullable = true)\n |-- has_discount: boolean (nullable = true)\n |-- is_weekend: boolean (nullable = true)\n |-- has_free_shipping: boolean (nullable = true)\n |-- order_year: integer (nullable = true)\n |-- order_month: integer (nullable = true)\n |-- order_day: integer (nullable = true)\n |-- order_hour: integer (nullable = true)\n |-- day_name: string (nullable = true)\n |-- _gold_processed_at: timestamp (nullable = true)\n |-- _gold_version: string (nullable = true)\n\n\n  Sample data:\n+-------------+--------------+--------+-----------+----------+--------+----------+--------------+------------+-----------+\n|sales_key    |order_date_key|order_id|customer_id|product_id|quantity|unit_price|net_line_total|order_status|channel    |\n+-------------+--------------+--------+-----------+----------+--------+----------+--------------+------------+-----------+\n|ITMORD01976-0|20250218      |ORD01976|CUST351    |PROD005   |2       |733.78    |1432.63       |DELIVERED   |mobile_app |\n|ITMORD01969-0|20250222      |ORD01969|CUST024    |PROD007   |2       |641.14    |1214.06       |DELIVERED   |marketplace|\n|ITMORD01936-0|20250217      |ORD01936|CUST359    |PROD013   |2       |686.48    |1209.99       |SHIPPED     |mobile_app |\n|ITMORD01933-0|20250220      |ORD01933|CUST079    |PROD026   |1       |866.67    |770.3         |CANCELLED   |mobile_app |\n|ITMORD01927-2|20250221      |ORD01927|CUST266    |PROD028   |2       |71.31     |135.8         |CANCELLED   |in_store   |\n+-------------+--------------+--------+-----------+----------+--------+----------+--------------+------------+-----------+\nonly showing top 5 rows\n\n=================================================================\nBUSINESS KPIs FROM fact_sales\n=================================================================\n\n  KPI 1: Revenue Summary\n+----------------+------------+---------------+-------------+-------------------+---------------+\n|total_line_items|total_orders|total_customers|total_revenue|avg_line_item_value|total_discounts|\n+----------------+------------+---------------+-------------+-------------------+---------------+\n|4780            |1948        |488            |4509792.0    |943.47             |370844.13      |\n+----------------+------------+---------------+-------------+-------------------+---------------+\n\n\n  KPI 2: Revenue by Channel\n+-----------+------+----------+--------------+\n|    channel|orders|   revenue|avg_item_value|\n+-----------+------+----------+--------------+\n| mobile_app|   486| 1170497.0|        969.76|\n|marketplace|   494|1143852.81|        942.22|\n|   in_store|   496|1125645.29|        928.75|\n|        web|   472| 1069796.9|        932.69|\n+-----------+------+----------+--------------+\n\n\n  KPI 3: Monthly Revenue Trend\n+----------+-----------+------+---------+\n|order_year|order_month|orders|  revenue|\n+----------+-----------+------+---------+\n|      2025|          2|    79|214236.43|\n|      2025|          3|   172|372089.73|\n|      2025|          4|   172|437767.73|\n|      2025|          5|   177|390772.75|\n|      2025|          6|   154| 338388.9|\n|      2025|          7|   143|305368.27|\n|      2025|          8|   174|409339.58|\n|      2025|          9|   151|324951.37|\n|      2025|         10|   142|324940.33|\n|      2025|         11|   153|364104.19|\n|      2025|         12|   180|412927.27|\n|      2026|          1|   167|390645.03|\n|      2026|          2|    84|224260.42|\n+----------+-----------+------+---------+\n\n\n  KPI 4: Order Status Breakdown\n+------------+------+---------+\n|order_status|orders|  revenue|\n+------------+------+---------+\n|     PENDING|   347|776881.45|\n|   DELIVERED|   334|825738.57|\n|     SHIPPED|   329|737813.84|\n|    RETURNED|   328|780482.02|\n|   CANCELLED|   307|701257.18|\n|   CONFIRMED|   303|687618.94|\n+------------+------+---------+\n\n\n  KPI 5: Payment Method Analysis\n+--------------+------+---------+\n|payment_method|orders|  revenue|\n+--------------+------+---------+\n|    debit_card|   412|972523.17|\n|           cod|   385|912035.58|\n|   credit_card|   382|911255.17|\n|           upi|   381|873415.22|\n|        wallet|   388|840562.86|\n+--------------+------+---------+\n\n[DONE] Gold fact_sales complete!\n[NEXT] Cell 15 - Gold agg_daily_sales (Pre-aggregated metrics)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 14: GOLD LAYER - fact_sales (Main Fact Table)\n",
    "# ============================================================\n",
    "#\n",
    "# THIS IS THE HEART OF THE ENTIRE DATA PLATFORM.\n",
    "#\n",
    "# WHAT IS A FACT TABLE?\n",
    "# ---------------------\n",
    "# A fact table records BUSINESS EVENTS (things that happened).\n",
    "# Each row = one item sold in one order.\n",
    "# This is called the \"grain\" of the fact table.\n",
    "#\n",
    "# GRAIN = \"The most atomic level of detail in the fact table\"\n",
    "# Our grain: ONE ORDER ITEM (not one order!)\n",
    "#\n",
    "# WHY ORDER ITEM LEVEL (not order level)?\n",
    "# If we aggregate to order level, we lose product-level detail:\n",
    "#   - \"Which product sold the most?\" needs item-level\n",
    "#   - \"Revenue by category\" needs item + product JOIN\n",
    "#   - \"Average items per order\" needs item count per order\n",
    "#\n",
    "# Rule: Always choose the LOWEST useful grain. You can always\n",
    "# aggregate UP (items -> orders), but you can't disaggregate \n",
    "# DOWN (orders -> items) without the source data.\n",
    "#\n",
    "# HOW FACT TABLE CONNECTS TO DIMENSIONS:\n",
    "#\n",
    "#   dim_date -------- date_key -------- fact_sales\n",
    "#   dim_customer ---- customer_id ----- fact_sales  \n",
    "#   dim_product ----- product_id ------ fact_sales\n",
    "#\n",
    "# This is the STAR shape:\n",
    "#           dim_date\n",
    "#              |\n",
    "#   dim_customer -- fact_sales -- dim_product\n",
    "#\n",
    "# MEASURES (numeric values we analyze):\n",
    "#   quantity, unit_price, line_total, discount_amount,\n",
    "#   net_line_total, shipping_amount\n",
    "#\n",
    "# These are the numbers we SUM, AVG, COUNT in dashboards.\n",
    "#\n",
    "# DEGENERATE DIMENSIONS:\n",
    "#   order_id and item_id live in the fact table directly.\n",
    "#   They don't have their own dimension table because\n",
    "#   there's nothing more to describe about them.\n",
    "#   This is called a \"degenerate dimension\" - a dimension\n",
    "#   key without a dimension table.\n",
    "# ============================================================\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 1: Read Silver tables we need to JOIN\n",
    "# ----------------------------------------------------------\n",
    "df_orders = spark.read.format(\"delta\").load(SILVER + \"/orders\")\n",
    "df_items = spark.read.format(\"delta\").load(SILVER + \"/order_items\")\n",
    "\n",
    "orders_count = df_orders.count()\n",
    "items_count = df_items.count()\n",
    "print(\"STEP 1: Silver data read\")\n",
    "print(\"  Orders:      \" + str(orders_count) + \" rows\")\n",
    "print(\"  Order Items: \" + str(items_count) + \" rows\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 2: JOIN orders with order_items\n",
    "# ----------------------------------------------------------\n",
    "# WHY JOIN IN GOLD (not Silver)?\n",
    "# Silver tables are INDEPENDENT cleaned tables.\n",
    "# Gold is where we COMBINE them for analytics.\n",
    "#\n",
    "# JOIN TYPE: INNER JOIN\n",
    "# We only want items that belong to valid orders.\n",
    "# If an item has an order_id that doesn't exist in orders,\n",
    "# it's orphaned data and should not be in the fact table.\n",
    "#\n",
    "# WHAT COLUMNS COME FROM WHERE:\n",
    "# From orders: customer_id, order_date, order_status, \n",
    "#              payment_method, channel, shipping_amount\n",
    "# From items:  product_id, quantity, unit_price,\n",
    "#              discount_percent, line_total, net_line_total\n",
    "#\n",
    "# We prefix nothing because column names don't conflict\n",
    "# (we already designed Silver tables carefully).\n",
    "\n",
    "df_joined = df_orders.alias(\"o\").join(\n",
    "    df_items.alias(\"i\"),\n",
    "    col(\"o.order_id\") == col(\"i.order_id\"),\n",
    "    \"inner\"\n",
    ")\n",
    "\n",
    "joined_count = df_joined.count()\n",
    "print(\"STEP 2: Orders JOIN Order Items = \" + str(joined_count) + \" rows\")\n",
    "\n",
    "# Check for orphaned items (items without matching orders)\n",
    "orphan_items = df_items.join(df_orders, \"order_id\", \"left_anti\").count()\n",
    "print(\"  Orphaned items (no matching order): \" + str(orphan_items))\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 3: Build fact_sales\n",
    "# ----------------------------------------------------------\n",
    "# SELECT COLUMNS carefully:\n",
    "#\n",
    "# KEYS (for joining to dimensions):\n",
    "#   order_date_key -> joins to dim_date.date_key\n",
    "#   customer_id    -> joins to dim_customer.customer_id\n",
    "#   product_id     -> joins to dim_product.product_id\n",
    "#\n",
    "# DEGENERATE DIMENSIONS (identifiers without dim tables):\n",
    "#   order_id       -> for drill-down to specific order\n",
    "#   item_id        -> for uniqueness (primary key of fact)\n",
    "#\n",
    "# MEASURES (numbers we aggregate):\n",
    "#   quantity              -> SUM for total units sold\n",
    "#   unit_price            -> for price analysis\n",
    "#   line_total            -> quantity * unit_price (gross)\n",
    "#   discount_percent      -> for discount analysis\n",
    "#   discount_amount       -> SUM for total discounts given\n",
    "#   net_line_total        -> revenue after discount\n",
    "#   shipping_amount       -> allocated from order level\n",
    "#\n",
    "# ATTRIBUTES (for filtering/grouping):\n",
    "#   order_status, payment_method, channel\n",
    "#   item_status\n",
    "#\n",
    "# FLAGS (pre-computed boolean filters):\n",
    "#   is_cancelled, is_returned, has_discount\n",
    "#   is_weekend\n",
    "#\n",
    "# TIME ATTRIBUTES (from orders, for quick filtering):\n",
    "#   order_year, order_month, order_hour, day_name\n",
    "\n",
    "df_fact_sales = df_joined.select(\n",
    "    # Primary key\n",
    "    col(\"i.item_id\").alias(\"sales_key\"),\n",
    "    # Dimension keys\n",
    "    date_format(col(\"o.order_date\"), \"yyyyMMdd\").cast(\"int\").alias(\"order_date_key\"),\n",
    "    col(\"o.customer_id\"),\n",
    "    col(\"i.product_id\"),\n",
    "    # Degenerate dimensions\n",
    "    col(\"o.order_id\"),\n",
    "    col(\"i.item_id\"),\n",
    "    # Measures\n",
    "    col(\"i.quantity\"),\n",
    "    col(\"i.unit_price\"),\n",
    "    col(\"i.line_total\"),\n",
    "    col(\"i.discount_percent\"),\n",
    "    col(\"i.discount_amount\"),\n",
    "    col(\"i.net_line_total\"),\n",
    "    col(\"o.shipping_amount\"),\n",
    "    # Order attributes\n",
    "    col(\"o.order_date\"),\n",
    "    col(\"o.order_status\"),\n",
    "    col(\"o.payment_method\"),\n",
    "    col(\"o.channel\"),\n",
    "    col(\"i.item_status\"),\n",
    "    # Pre-computed flags\n",
    "    col(\"o.is_cancelled\"),\n",
    "    col(\"o.is_returned\"),\n",
    "    col(\"i.has_discount\"),\n",
    "    col(\"o.is_weekend\"),\n",
    "    col(\"o.has_free_shipping\"),\n",
    "    # Time parts (for quick filtering without joining dim_date)\n",
    "    col(\"o.order_year\"),\n",
    "    col(\"o.order_month\"),\n",
    "    col(\"o.order_day\"),\n",
    "    col(\"o.order_hour\"),\n",
    "    col(\"o.day_name\"),\n",
    "    # Metadata\n",
    "    current_timestamp().alias(\"_gold_processed_at\"),\n",
    "    lit(\"1.0\").alias(\"_gold_version\")\n",
    ")\n",
    "\n",
    "print(\"STEP 3: fact_sales built - \" + str(len(df_fact_sales.columns)) + \" columns\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 4: Write to Gold layer (partitioned by year, month)\n",
    "# ----------------------------------------------------------\n",
    "# WHY PARTITION fact_sales?\n",
    "# Fact tables are LARGE (millions of rows in production).\n",
    "# Partitioning by year/month means:\n",
    "#   - Query \"SELECT * WHERE order_year = 2025\" only reads \n",
    "#     2025 files, skipping all other years\n",
    "#   - This is called PARTITION PRUNING\n",
    "#   - Can reduce query time by 90%+\n",
    "#\n",
    "# WHY year + month (not just date)?\n",
    "# - Partitioning by date creates too many small files\n",
    "#   (365 partitions per year, each with few rows)\n",
    "# - year + month = 12 partitions per year (optimal)\n",
    "# - This is a common production pattern\n",
    "\n",
    "gold_fact_sales_path = GOLD + \"/fact_sales\"\n",
    "\n",
    "df_fact_sales.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"order_year\", \"order_month\") \\\n",
    "    .option(\"overwriteSchema\", True) \\\n",
    "    .save(gold_fact_sales_path)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 5: Verify and analyze\n",
    "# ----------------------------------------------------------\n",
    "df_verify = spark.read.format(\"delta\").load(gold_fact_sales_path)\n",
    "final_count = df_verify.count()\n",
    "\n",
    "print(\"\")\n",
    "print(\"=\" * 65)\n",
    "print(\"GOLD fact_sales - COMPLETE\")\n",
    "print(\"=\" * 65)\n",
    "print(\"  Orders (Silver):    \" + str(orders_count))\n",
    "print(\"  Items (Silver):     \" + str(items_count))\n",
    "print(\"  Joined rows:        \" + str(joined_count))\n",
    "print(\"  Final fact_sales:   \" + str(final_count) + \" rows\")\n",
    "print(\"  Columns:            \" + str(len(df_verify.columns)))\n",
    "print(\"  Partitioned by:     order_year, order_month\")\n",
    "print(\"  Path:               \" + gold_fact_sales_path)\n",
    "\n",
    "print(\"\\n  Schema:\")\n",
    "df_verify.printSchema()\n",
    "\n",
    "print(\"\\n  Sample data:\")\n",
    "df_verify.select(\n",
    "    \"sales_key\", \"order_date_key\", \"order_id\", \"customer_id\",\n",
    "    \"product_id\", \"quantity\", \"unit_price\", \"net_line_total\",\n",
    "    \"order_status\", \"channel\"\n",
    ").show(5, truncate=False)\n",
    "\n",
    "# ============================================================\n",
    "# BUSINESS KPI QUERIES (This is what the Gold layer enables!)\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 65)\n",
    "print(\"BUSINESS KPIs FROM fact_sales\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "# KPI 1: Total Revenue\n",
    "print(\"\\n  KPI 1: Revenue Summary\")\n",
    "df_verify.agg(\n",
    "    count(\"*\").alias(\"total_line_items\"),\n",
    "    countDistinct(\"order_id\").alias(\"total_orders\"),\n",
    "    countDistinct(\"customer_id\").alias(\"total_customers\"),\n",
    "    round(sum(\"net_line_total\"), 2).alias(\"total_revenue\"),\n",
    "    round(avg(\"net_line_total\"), 2).alias(\"avg_line_item_value\"),\n",
    "    round(sum(\"discount_amount\"), 2).alias(\"total_discounts\")\n",
    ").show(truncate=False)\n",
    "\n",
    "# KPI 2: Revenue by Channel\n",
    "print(\"\\n  KPI 2: Revenue by Channel\")\n",
    "df_verify.groupBy(\"channel\").agg(\n",
    "    countDistinct(\"order_id\").alias(\"orders\"),\n",
    "    round(sum(\"net_line_total\"), 2).alias(\"revenue\"),\n",
    "    round(avg(\"net_line_total\"), 2).alias(\"avg_item_value\")\n",
    ").orderBy(desc(\"revenue\")).show()\n",
    "\n",
    "# KPI 3: Revenue by Month\n",
    "print(\"\\n  KPI 3: Monthly Revenue Trend\")\n",
    "df_verify.groupBy(\"order_year\", \"order_month\").agg(\n",
    "    countDistinct(\"order_id\").alias(\"orders\"),\n",
    "    round(sum(\"net_line_total\"), 2).alias(\"revenue\")\n",
    ").orderBy(\"order_year\", \"order_month\").show(15)\n",
    "\n",
    "# KPI 4: Order Status Breakdown\n",
    "print(\"\\n  KPI 4: Order Status Breakdown\")\n",
    "df_verify.groupBy(\"order_status\").agg(\n",
    "    countDistinct(\"order_id\").alias(\"orders\"),\n",
    "    round(sum(\"net_line_total\"), 2).alias(\"revenue\")\n",
    ").orderBy(desc(\"orders\")).show()\n",
    "\n",
    "# KPI 5: Payment Method Analysis\n",
    "print(\"\\n  KPI 5: Payment Method Analysis\")\n",
    "df_verify.groupBy(\"payment_method\").agg(\n",
    "    countDistinct(\"order_id\").alias(\"orders\"),\n",
    "    round(sum(\"net_line_total\"), 2).alias(\"revenue\")\n",
    ").orderBy(desc(\"revenue\")).show()\n",
    "\n",
    "print(\"[DONE] Gold fact_sales complete!\")\n",
    "print(\"[NEXT] Cell 15 - Gold agg_daily_sales (Pre-aggregated metrics)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57cc07db-ab77-4364-9b78-03e5e27119d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1: fact_sales read - 4780 rows\nSTEP 2: Daily aggregation built - 1107 rows\n\n=================================================================\nGOLD agg_daily_sales - COMPLETE\n=================================================================\n  Fact rows aggregated: 4780\n  Aggregated rows:      1107\n  Compression ratio:    4780 -> 1107 rows\n  Path:                 abfss://gold@dlsshopsmartdev123.dfs.core.windows.net/agg_daily_sales\n\n  Schema:\nroot\n |-- order_year: integer (nullable = true)\n |-- order_month: integer (nullable = true)\n |-- order_day: integer (nullable = true)\n |-- order_date: date (nullable = true)\n |-- channel: string (nullable = true)\n |-- total_orders: long (nullable = true)\n |-- total_customers: long (nullable = true)\n |-- total_line_items: long (nullable = true)\n |-- total_items_sold: long (nullable = true)\n |-- gross_revenue: double (nullable = true)\n |-- total_discount: double (nullable = true)\n |-- net_revenue: double (nullable = true)\n |-- total_shipping: double (nullable = true)\n |-- avg_item_value: double (nullable = true)\n |-- avg_quantity_per_item: double (nullable = true)\n |-- cancelled_orders: long (nullable = true)\n |-- returned_orders: long (nullable = true)\n |-- delivered_orders: long (nullable = true)\n |-- orders_with_discount: long (nullable = true)\n |-- is_weekend: boolean (nullable = true)\n |-- day_name: string (nullable = true)\n |-- avg_order_value: double (nullable = true)\n |-- cancel_rate_pct: double (nullable = true)\n |-- return_rate_pct: double (nullable = true)\n |-- discount_rate_pct: double (nullable = true)\n |-- _gold_processed_at: timestamp (nullable = true)\n |-- _gold_version: string (nullable = true)\n\n\n  Sample daily data:\n+----------+-----------+------------+---------------+-----------+---------------+---------------+\n|order_date|channel    |total_orders|total_customers|net_revenue|avg_order_value|cancel_rate_pct|\n+----------+-----------+------------+---------------+-----------+---------------+---------------+\n|2026-02-17|mobile_app |1           |1              |127.26     |127.26         |0.0            |\n|2026-02-16|marketplace|1           |1              |899.46     |899.46         |0.0            |\n|2026-02-16|mobile_app |3           |3              |7679.21    |2559.74        |33.33          |\n|2026-02-16|in_store   |2           |2              |4297.3     |2148.65        |50.0           |\n|2026-02-16|web        |1           |1              |1346.75    |1346.75        |0.0            |\n|2026-02-15|web        |5           |5              |11287.53   |2257.51        |0.0            |\n|2026-02-15|in_store   |1           |1              |4912.33    |4912.33        |0.0            |\n|2026-02-15|mobile_app |1           |1              |1443.93    |1443.93        |0.0            |\n|2026-02-14|web        |2           |2              |6634.44    |3317.22        |0.0            |\n|2026-02-14|marketplace|1           |1              |3763.36    |3763.36        |0.0            |\n+----------+-----------+------------+---------------+-----------+---------------+---------------+\nonly showing top 10 rows\n\n=================================================================\nEXECUTIVE DASHBOARD METRICS\n=================================================================\n\n  PLATFORM TOTALS:\n+------------+---------------------+-----------------+---------------------+----------------+------------------------+\n|total_orders|total_customer_visits|total_net_revenue|total_discounts_given|total_units_sold|platform_avg_order_value|\n+------------+---------------------+-----------------+---------------------+----------------+------------------------+\n|1948        |1948                 |4509792.0        |370844.13            |9581            |2291.34                 |\n+------------+---------------------+-----------------+---------------------+----------------+------------------------+\n\n\n  MONTHLY REVENUE TREND:\n+----------+-----------+------+---------+---------------+---------------+\n|order_year|order_month|orders|  revenue|avg_order_value|avg_cancel_rate|\n+----------+-----------+------+---------+---------------+---------------+\n|      2025|          2|    79|214236.43|        2691.41|          24.56|\n|      2025|          3|   172|372089.73|        2118.72|          14.07|\n|      2025|          4|   172|437767.73|        2618.28|           18.2|\n|      2025|          5|   177|390772.75|        2124.83|          14.24|\n|      2025|          6|   154| 338388.9|        2145.08|          15.98|\n|      2025|          7|   143|305368.27|         2149.0|          17.96|\n|      2025|          8|   174|409339.58|         2286.8|          20.62|\n|      2025|          9|   151|324951.37|        2053.46|          19.25|\n|      2025|         10|   142|324940.33|        2275.67|          21.43|\n|      2025|         11|   153|364104.19|        2373.93|          15.46|\n|      2025|         12|   180|412927.27|        2296.21|          16.62|\n|      2026|          1|   167|390645.03|        2352.68|          16.25|\n|      2026|          2|    84|224260.42|        2777.07|          12.58|\n+----------+-----------+------+---------+---------------+---------------+\n\n\n  CHANNEL PERFORMANCE:\n+-----------+------+----------+---------------+---------------+---------------+\n|    channel|orders|   revenue|avg_order_value|avg_cancel_rate|avg_return_rate|\n+-----------+------+----------+---------------+---------------+---------------+\n| mobile_app|   486| 1170497.0|         2443.0|          16.12|          16.05|\n|marketplace|   494|1143852.81|        2220.95|          19.21|          16.77|\n|   in_store|   496|1125645.29|        2290.11|          15.23|          14.82|\n|        web|   472| 1069796.9|        2213.42|          18.64|          18.73|\n+-----------+------+----------+---------------+---------------+---------------+\n\n\n  WEEKEND vs WEEKDAY:\n+----------+------+----------+---------------+\n|is_weekend|orders|   revenue|avg_order_value|\n+----------+------+----------+---------------+\n|      true|   544|1217875.19|        2223.41|\n|     false|  1404|3291916.81|         2318.6|\n+----------+------+----------+---------------+\n\n\n=================================================================\nGOLD LAYER - ALL TABLES COMPLETE!\n=================================================================\n  1. gold/dim_date         - 1096 rows  (calendar dimension)\n  2. gold/dim_customer     - 500 rows   (customer dimension)\n  3. gold/dim_product      - 50 rows    (product dimension)\n  4. gold/fact_sales       - 4780 rows  (main fact table)\n  5. gold/agg_daily_sales  - 1107 rows   (pre-aggregated)\n\n  STAR SCHEMA:\n        dim_date\n           |\n  dim_customer --- fact_sales --- dim_product\n                       |\n                 agg_daily_sales\n\n[NEXT] Cell 16+ : ML Models (Customer Segmentation, Forecasting)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 15: GOLD LAYER - agg_daily_sales (Pre-Aggregated Table)\n",
    "# ============================================================\n",
    "#\n",
    "# WHAT IS A PRE-AGGREGATED TABLE?\n",
    "# --------------------------------\n",
    "# Instead of running heavy GROUP BY queries on 4780 rows \n",
    "# (or millions in production) every time a dashboard loads,\n",
    "# we pre-compute daily summaries.\n",
    "#\n",
    "# Dashboard query WITHOUT pre-aggregation:\n",
    "#   SELECT date, SUM(net_line_total) FROM fact_sales \n",
    "#   GROUP BY date\n",
    "#   -> Scans ALL rows every time (slow at scale)\n",
    "#\n",
    "# Dashboard query WITH pre-aggregation:\n",
    "#   SELECT * FROM agg_daily_sales WHERE date = '2025-01-15'\n",
    "#   -> Reads ONE pre-computed row (instant)\n",
    "#\n",
    "# WHY IS THIS IMPORTANT?\n",
    "# - Power BI dashboards refresh every 15-30 minutes\n",
    "# - Each refresh runs all queries\n",
    "# - Without pre-agg: 50 queries * millions of rows = slow\n",
    "# - With pre-agg: 50 queries * hundreds of rows = instant\n",
    "#\n",
    "# WHAT METRICS DO WE PRE-COMPUTE?\n",
    "# Everything a business executive looks at daily:\n",
    "#   - Total revenue, orders, customers\n",
    "#   - Average order value\n",
    "#   - Cancellation and return rates\n",
    "#   - Revenue by channel\n",
    "#   - Units sold\n",
    "#\n",
    "# GRAIN: One row per day per channel\n",
    "# This lets us analyze both:\n",
    "#   - Total daily metrics (GROUP BY date)\n",
    "#   - Channel comparison (GROUP BY date, channel)\n",
    "# ============================================================\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 1: Read fact_sales from Gold\n",
    "# ----------------------------------------------------------\n",
    "df_fact = spark.read.format(\"delta\").load(GOLD + \"/fact_sales\")\n",
    "fact_count = df_fact.count()\n",
    "print(\"STEP 1: fact_sales read - \" + str(fact_count) + \" rows\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 2: Build daily aggregation by channel\n",
    "# ----------------------------------------------------------\n",
    "# WHY GROUP BY date AND channel?\n",
    "# \n",
    "# If we only group by date:\n",
    "#   2025-01-15 | revenue: $50,000 | orders: 200\n",
    "#   (Can't drill down by channel)\n",
    "#\n",
    "# If we group by date + channel:\n",
    "#   2025-01-15 | web        | revenue: $15,000 | orders: 60\n",
    "#   2025-01-15 | mobile_app | revenue: $18,000 | orders: 75\n",
    "#   2025-01-15 | in_store   | revenue: $10,000 | orders: 40\n",
    "#   2025-01-15 | marketplace| revenue: $7,000  | orders: 25\n",
    "#   (Can see channel breakdown AND roll up to daily total)\n",
    "#\n",
    "# METRICS EXPLAINED:\n",
    "#   total_orders: COUNT DISTINCT order_id (not line items!)\n",
    "#     One order with 3 items = 1 order, not 3\n",
    "#\n",
    "#   total_customers: unique buyers that day\n",
    "#     Important for \"customer acquisition\" tracking\n",
    "#\n",
    "#   total_items_sold: SUM of quantity\n",
    "#     Physical units moved (logistics metric)\n",
    "#\n",
    "#   gross_revenue: before discounts\n",
    "#   total_discount: money given away\n",
    "#   net_revenue: what we actually earned\n",
    "#   avg_order_value: revenue / orders\n",
    "#     Key e-commerce KPI, target is to increase this\n",
    "#\n",
    "#   cancel_rate: % of orders cancelled\n",
    "#     High cancel rate = UX problem or fraud\n",
    "#\n",
    "#   return_rate: % of orders returned\n",
    "#     High return rate = product quality issue\n",
    "\n",
    "df_agg_daily = df_fact.groupBy(\n",
    "    col(\"order_year\"),\n",
    "    col(\"order_month\"),\n",
    "    col(\"order_day\"),\n",
    "    to_date(col(\"order_date\")).alias(\"order_date\"),\n",
    "    col(\"channel\")\n",
    ").agg(\n",
    "    # Volume metrics\n",
    "    countDistinct(\"order_id\").alias(\"total_orders\"),\n",
    "    countDistinct(\"customer_id\").alias(\"total_customers\"),\n",
    "    count(\"*\").alias(\"total_line_items\"),\n",
    "    sum(\"quantity\").alias(\"total_items_sold\"),\n",
    "    # Revenue metrics\n",
    "    round(sum(\"line_total\"), 2).alias(\"gross_revenue\"),\n",
    "    round(sum(\"discount_amount\"), 2).alias(\"total_discount\"),\n",
    "    round(sum(\"net_line_total\"), 2).alias(\"net_revenue\"),\n",
    "    round(sum(\"shipping_amount\"), 2).alias(\"total_shipping\"),\n",
    "    # Averages\n",
    "    round(avg(\"net_line_total\"), 2).alias(\"avg_item_value\"),\n",
    "    round(avg(\"quantity\"), 2).alias(\"avg_quantity_per_item\"),\n",
    "    # Status counts\n",
    "    countDistinct(when(col(\"order_status\") == \"CANCELLED\", col(\"order_id\"))).alias(\"cancelled_orders\"),\n",
    "    countDistinct(when(col(\"order_status\") == \"RETURNED\", col(\"order_id\"))).alias(\"returned_orders\"),\n",
    "    countDistinct(when(col(\"order_status\") == \"DELIVERED\", col(\"order_id\"))).alias(\"delivered_orders\"),\n",
    "    # Discount metrics\n",
    "    countDistinct(when(col(\"has_discount\") == True, col(\"order_id\"))).alias(\"orders_with_discount\"),\n",
    "    # Weekend flag\n",
    "    first(\"is_weekend\").alias(\"is_weekend\"),\n",
    "    first(\"day_name\").alias(\"day_name\")\n",
    ")\n",
    "\n",
    "# Add calculated rates\n",
    "df_agg_enriched = df_agg_daily \\\n",
    "    .withColumn(\"avg_order_value\",\n",
    "        when(col(\"total_orders\") > 0,\n",
    "            round(col(\"net_revenue\") / col(\"total_orders\"), 2))\n",
    "        .otherwise(lit(0.0))) \\\n",
    "    .withColumn(\"cancel_rate_pct\",\n",
    "        when(col(\"total_orders\") > 0,\n",
    "            round(col(\"cancelled_orders\") / col(\"total_orders\") * 100, 2))\n",
    "        .otherwise(lit(0.0))) \\\n",
    "    .withColumn(\"return_rate_pct\",\n",
    "        when(col(\"total_orders\") > 0,\n",
    "            round(col(\"returned_orders\") / col(\"total_orders\") * 100, 2))\n",
    "        .otherwise(lit(0.0))) \\\n",
    "    .withColumn(\"discount_rate_pct\",\n",
    "        when(col(\"total_orders\") > 0,\n",
    "            round(col(\"orders_with_discount\") / col(\"total_orders\") * 100, 2))\n",
    "        .otherwise(lit(0.0))) \\\n",
    "    .withColumn(\"_gold_processed_at\", current_timestamp()) \\\n",
    "    .withColumn(\"_gold_version\", lit(\"1.0\"))\n",
    "\n",
    "agg_count = df_agg_enriched.count()\n",
    "print(\"STEP 2: Daily aggregation built - \" + str(agg_count) + \" rows\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 3: Write to Gold\n",
    "# ----------------------------------------------------------\n",
    "gold_agg_path = GOLD + \"/agg_daily_sales\"\n",
    "\n",
    "df_agg_enriched.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", True) \\\n",
    "    .save(gold_agg_path)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 4: Verify and show executive dashboard metrics\n",
    "# ----------------------------------------------------------\n",
    "df_verify = spark.read.format(\"delta\").load(gold_agg_path)\n",
    "final_count = df_verify.count()\n",
    "\n",
    "print(\"\")\n",
    "print(\"=\" * 65)\n",
    "print(\"GOLD agg_daily_sales - COMPLETE\")\n",
    "print(\"=\" * 65)\n",
    "print(\"  Fact rows aggregated: \" + str(fact_count))\n",
    "print(\"  Aggregated rows:      \" + str(final_count))\n",
    "print(\"  Compression ratio:    \" + str(fact_count) + \" -> \" + str(final_count) + \" rows\")\n",
    "print(\"  Path:                 \" + gold_agg_path)\n",
    "\n",
    "print(\"\\n  Schema:\")\n",
    "df_verify.printSchema()\n",
    "\n",
    "print(\"\\n  Sample daily data:\")\n",
    "df_verify.select(\n",
    "    \"order_date\", \"channel\", \"total_orders\", \"total_customers\",\n",
    "    \"net_revenue\", \"avg_order_value\", \"cancel_rate_pct\"\n",
    ").orderBy(desc(\"order_date\")).show(10, truncate=False)\n",
    "\n",
    "# ============================================================\n",
    "# EXECUTIVE DASHBOARD QUERIES\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 65)\n",
    "print(\"EXECUTIVE DASHBOARD METRICS\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "# Overall platform metrics (roll up all channels)\n",
    "print(\"\\n  PLATFORM TOTALS:\")\n",
    "df_verify.agg(\n",
    "    sum(\"total_orders\").alias(\"total_orders\"),\n",
    "    sum(\"total_customers\").alias(\"total_customer_visits\"),\n",
    "    round(sum(\"net_revenue\"), 2).alias(\"total_net_revenue\"),\n",
    "    round(sum(\"total_discount\"), 2).alias(\"total_discounts_given\"),\n",
    "    round(sum(\"total_items_sold\")).alias(\"total_units_sold\"),\n",
    "    round(avg(\"avg_order_value\"), 2).alias(\"platform_avg_order_value\")\n",
    ").show(truncate=False)\n",
    "\n",
    "# Monthly trend (what executives look at first)\n",
    "print(\"\\n  MONTHLY REVENUE TREND:\")\n",
    "df_verify.groupBy(\"order_year\", \"order_month\").agg(\n",
    "    sum(\"total_orders\").alias(\"orders\"),\n",
    "    round(sum(\"net_revenue\"), 2).alias(\"revenue\"),\n",
    "    round(avg(\"avg_order_value\"), 2).alias(\"avg_order_value\"),\n",
    "    round(avg(\"cancel_rate_pct\"), 2).alias(\"avg_cancel_rate\")\n",
    ").orderBy(\"order_year\", \"order_month\").show(15)\n",
    "\n",
    "# Channel performance comparison\n",
    "print(\"\\n  CHANNEL PERFORMANCE:\")\n",
    "df_verify.groupBy(\"channel\").agg(\n",
    "    sum(\"total_orders\").alias(\"orders\"),\n",
    "    round(sum(\"net_revenue\"), 2).alias(\"revenue\"),\n",
    "    round(avg(\"avg_order_value\"), 2).alias(\"avg_order_value\"),\n",
    "    round(avg(\"cancel_rate_pct\"), 2).alias(\"avg_cancel_rate\"),\n",
    "    round(avg(\"return_rate_pct\"), 2).alias(\"avg_return_rate\")\n",
    ").orderBy(desc(\"revenue\")).show()\n",
    "\n",
    "# Weekend vs Weekday\n",
    "print(\"\\n  WEEKEND vs WEEKDAY:\")\n",
    "df_verify.groupBy(\"is_weekend\").agg(\n",
    "    sum(\"total_orders\").alias(\"orders\"),\n",
    "    round(sum(\"net_revenue\"), 2).alias(\"revenue\"),\n",
    "    round(avg(\"avg_order_value\"), 2).alias(\"avg_order_value\")\n",
    ").show()\n",
    "\n",
    "# ============================================================\n",
    "# FINAL GOLD LAYER SUMMARY\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 65)\n",
    "print(\"GOLD LAYER - ALL TABLES COMPLETE!\")\n",
    "print(\"=\" * 65)\n",
    "print(\"  1. gold/dim_date         - 1096 rows  (calendar dimension)\")\n",
    "print(\"  2. gold/dim_customer     - 500 rows   (customer dimension)\")\n",
    "print(\"  3. gold/dim_product      - 50 rows    (product dimension)\")\n",
    "print(\"  4. gold/fact_sales       - \" + str(fact_count) + \" rows  (main fact table)\")\n",
    "print(\"  5. gold/agg_daily_sales  - \" + str(final_count) + \" rows   (pre-aggregated)\")\n",
    "print(\"\")\n",
    "print(\"  STAR SCHEMA:\")\n",
    "print(\"        dim_date\")\n",
    "print(\"           |\")\n",
    "print(\"  dim_customer --- fact_sales --- dim_product\")\n",
    "print(\"                       |\")\n",
    "print(\"                 agg_daily_sales\")\n",
    "print(\"\")\n",
    "print(\"[NEXT] Cell 16+ : ML Models (Customer Segmentation, Forecasting)\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "1_Bronze_to_Silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}