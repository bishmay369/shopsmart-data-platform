{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "041c3cc0-9427-49d9-86ce-32a654f33d85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auth successful! fact_sales has 4780 rows\nReady for ML models.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# NOTEBOOK 3: ML MODELS\n",
    "# ============================================================\n",
    "# CELL 0: Authentication + Path Setup (MUST RUN FIRST)\n",
    "# ============================================================\n",
    "\n",
    "# 1. Credentials\n",
    "client_id     = dbutils.secrets.get(scope=\"shopsmart-scope\", key=\"datalake-sp-client-id\")\n",
    "client_secret = dbutils.secrets.get(scope=\"shopsmart-scope\", key=\"datalake-sp-client-secret\")\n",
    "tenant_id     = dbutils.secrets.get(scope=\"shopsmart-scope\", key=\"datalake-sp-tenant-id\")\n",
    "\n",
    "storage_account_name = \"dlsshopsmartdev123\"\n",
    "\n",
    "# 2. Spark OAuth config\n",
    "spark.conf.set(\"fs.azure.account.auth.type.\" + storage_account_name + \".dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(\"fs.azure.account.oauth.provider.type.\" + storage_account_name + \".dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.id.\" + storage_account_name + \".dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.secret.\" + storage_account_name + \".dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.endpoint.\" + storage_account_name + \".dfs.core.windows.net\", \"https://login.microsoftonline.com/\" + tenant_id + \"/oauth2/token\")\n",
    "\n",
    "# 3. Paths\n",
    "BRONZE = \"abfss://bronze@\" + storage_account_name + \".dfs.core.windows.net\"\n",
    "SILVER = \"abfss://silver@\" + storage_account_name + \".dfs.core.windows.net\"\n",
    "GOLD   = \"abfss://gold@\" + storage_account_name + \".dfs.core.windows.net\"\n",
    "\n",
    "# 4. Verify\n",
    "df_test = spark.read.format(\"delta\").load(GOLD + \"/fact_sales\")\n",
    "print(\"Auth successful! fact_sales has \" + str(df_test.count()) + \" rows\")\n",
    "print(\"Ready for ML models.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d00421e-7730-4217-9995-bd3e46f3c7c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1: fact_sales loaded - 4780 rows\nSTEP 2: Reference date for Recency: 2026-02-17 07:27:30\nSTEP 2: RFM features built for 488 customers\nSTEP 3: RFM scores calculated (1-5 scale)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/spark/python/pyspark/sql/connect/expressions.py:1061: UserWarning: WARN WindowExpression: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n=================================================================\nML - RFM FEATURES - COMPLETE\n=================================================================\n  Customers analyzed: 488\n  Features:           25 columns\n  Path:               abfss://gold@dlsshopsmartdev123.dfs.core.windows.net/ml_customer_rfm\n\n  Schema:\nroot\n |-- customer_id: string (nullable = true)\n |-- recency_days: integer (nullable = true)\n |-- frequency: long (nullable = true)\n |-- monetary: double (nullable = true)\n |-- avg_order_value: double (nullable = true)\n |-- total_items_bought: long (nullable = true)\n |-- unique_products: long (nullable = true)\n |-- avg_items_per_order: double (nullable = true)\n |-- channels_used: long (nullable = true)\n |-- cancelled_orders: long (nullable = true)\n |-- returned_orders: long (nullable = true)\n |-- delivered_orders: long (nullable = true)\n |-- customer_lifespan_days: integer (nullable = true)\n |-- first_order_date: timestamp (nullable = true)\n |-- last_order_date: timestamp (nullable = true)\n |-- return_rate: double (nullable = true)\n |-- cancel_rate: double (nullable = true)\n |-- delivery_rate: double (nullable = true)\n |-- r_score: integer (nullable = true)\n |-- f_score: integer (nullable = true)\n |-- m_score: integer (nullable = true)\n |-- rfm_score: integer (nullable = true)\n |-- rfm_segment: string (nullable = true)\n |-- _gold_processed_at: timestamp (nullable = true)\n |-- _gold_version: string (nullable = true)\n\n\n  RFM Sample (top customers by monetary):\n+-----------+------------+---------+--------+-------+-------+-------+---------+---------------+\n|customer_id|recency_days|frequency|monetary|r_score|f_score|m_score|rfm_score|rfm_segment    |\n+-----------+------------+---------+--------+-------+-------+-------+---------+---------------+\n|CUST356    |34          |9        |23630.2 |4      |5      |5      |14       |Champions      |\n|CUST012    |8           |9        |23506.16|5      |5      |5      |15       |Champions      |\n|CUST335    |41          |5        |22736.16|4      |4      |5      |13       |Champions      |\n|CUST247    |1           |8        |22551.3 |5      |5      |5      |15       |Champions      |\n|CUST017    |7           |12       |21731.53|5      |5      |5      |15       |Champions      |\n|CUST482    |96          |7        |20833.44|2      |5      |5      |12       |Loyal Customers|\n|CUST172    |25          |7        |20346.49|4      |5      |5      |14       |Champions      |\n|CUST131    |9           |7        |20233.53|5      |5      |5      |15       |Champions      |\n|CUST113    |60          |5        |19587.72|3      |4      |5      |12       |Loyal Customers|\n|CUST238    |17          |7        |19535.93|5      |5      |5      |15       |Champions      |\n+-----------+------------+---------+--------+-------+-------+-------+---------+---------------+\nonly showing top 10 rows\n\n  CUSTOMER SEGMENT DISTRIBUTION:\n+-------------------+---------+-----------+-------------+------------+-------------+\n|rfm_segment        |customers|avg_recency|avg_frequency|avg_monetary|avg_rfm_score|\n+-------------------+---------+-----------+-------------+------------+-------------+\n|Champions          |87       |24.0       |6.6          |14256.0     |13.9         |\n|Loyal Customers    |132      |56.0       |4.7          |9790.0      |11.0         |\n|Potential Loyalists|150      |91.0       |3.4          |6107.0      |7.9          |\n|At Risk            |67       |134.0      |2.4          |3860.0      |5.5          |\n|Hibernating        |52       |204.0      |1.5          |1948.0      |3.4          |\n+-------------------+---------+-----------+-------------+------------+-------------+\n\n\n  RFM Score distribution:\n+---------+-----+\n|rfm_score|count|\n+---------+-----+\n|        3|   31|\n|        4|   21|\n|        5|   31|\n|        6|   36|\n|        7|   57|\n|        8|   49|\n|        9|   44|\n|       10|   44|\n|       11|   48|\n|       12|   40|\n|       13|   34|\n|       14|   31|\n|       15|   22|\n+---------+-----+\n\n\n  BUSINESS INSIGHTS:\n  Champions:           87 customers (protect and reward)\n  At Risk:             67 customers (re-engage immediately!)\n  Hibernating:         52 customers (win-back campaign)\n\n  Avg channels used by segment:\n+-------------------+------------+-------------------+---------------+\n|rfm_segment        |avg_channels|avg_unique_products|avg_return_rate|\n+-------------------+------------+-------------------+---------------+\n|Champions          |3.4         |14.6               |15.0           |\n|Loyal Customers    |2.9         |10.7               |18.2           |\n|Potential Loyalists|2.4         |7.4                |17.0           |\n|At Risk            |1.9         |5.1                |20.9           |\n|Hibernating        |1.3         |3.0                |14.1           |\n+-------------------+------------+-------------------+---------------+\n\n[DONE] RFM Feature Engineering complete!\n[NEXT] Cell 17 - Final Pipeline Summary + Star Schema Queries\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 16: ML - RFM FEATURE ENGINEERING\n",
    "# ============================================================\n",
    "#\n",
    "# WHAT IS RFM ANALYSIS?\n",
    "# ---------------------\n",
    "# RFM is the most widely used customer segmentation technique\n",
    "# in e-commerce and retail. Every company uses it.\n",
    "#\n",
    "# R = RECENCY\n",
    "#   \"How recently did the customer buy?\"\n",
    "#   Customer who bought yesterday is more valuable than \n",
    "#   one who bought 6 months ago.\n",
    "#   Measured in: days since last purchase\n",
    "#\n",
    "# F = FREQUENCY  \n",
    "#   \"How often does the customer buy?\"\n",
    "#   Customer who buys every week is more valuable than\n",
    "#   one who buys once a year.\n",
    "#   Measured in: total number of orders\n",
    "#\n",
    "# M = MONETARY\n",
    "#   \"How much does the customer spend?\"\n",
    "#   Customer who spends $5000/year is more valuable than\n",
    "#   one who spends $50/year.\n",
    "#   Measured in: total revenue from this customer\n",
    "#\n",
    "# WHY RFM?\n",
    "# - Simple to explain to business stakeholders\n",
    "# - Actionable: each segment gets different marketing\n",
    "# - Proven: used by Amazon, Netflix, Spotify, every retailer\n",
    "# - Interview favorite: \"How would you segment customers?\"\n",
    "#\n",
    "# SEGMENTS WE WILL CREATE:\n",
    "#   Champions:        High R, High F, High M (best customers)\n",
    "#   Loyal Customers:  Medium R, High F, High M\n",
    "#   Potential Loyalists: High R, Medium F, Medium M (nurture!)\n",
    "#   At Risk:          Low R, High F, High M (were good, slipping)\n",
    "#   Hibernating:      Low R, Low F, Low M (almost lost)\n",
    "#\n",
    "# APPROACH:\n",
    "# Step 1: Calculate RFM metrics from fact_sales (this cell)\n",
    "# Step 2: Apply K-Means clustering (next cell)\n",
    "# Step 3: Label clusters with business names\n",
    "# ============================================================\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 1: Read fact_sales\n",
    "# ----------------------------------------------------------\n",
    "df_fact = spark.read.format(\"delta\").load(GOLD + \"/fact_sales\")\n",
    "print(\"STEP 1: fact_sales loaded - \" + str(df_fact.count()) + \" rows\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 2: Calculate RFM metrics per customer\n",
    "# ----------------------------------------------------------\n",
    "# We calculate from the FACT TABLE because it has:\n",
    "#   - order_date (for Recency)\n",
    "#   - order_id (for Frequency) \n",
    "#   - net_line_total (for Monetary)\n",
    "#\n",
    "# IMPORTANT: We exclude cancelled orders from monetary \n",
    "# calculations because cancelled orders don't generate revenue.\n",
    "# But we COUNT them separately as a feature (cancel behavior).\n",
    "#\n",
    "# ADDITIONAL FEATURES beyond basic RFM:\n",
    "#   avg_order_value: monetary / frequency\n",
    "#     High AOV = premium customer\n",
    "#   unique_products: how many different products they bought\n",
    "#     High variety = explorer, Low variety = loyal to specific items\n",
    "#   avg_items_per_order: quantity / orders\n",
    "#     Bulk buyers vs single-item buyers\n",
    "#   channels_used: how many channels (web, mobile, store)\n",
    "#     Multi-channel customers are more valuable\n",
    "#   return_rate: what % of their orders were returned\n",
    "#     High return rate = risky customer\n",
    "#   preferred_channel: where do they shop most\n",
    "#     Useful for targeted marketing\n",
    "#   preferred_payment: how do they pay\n",
    "#     Useful for checkout optimization\n",
    "\n",
    "# Reference date for recency calculation\n",
    "# Using max date in data as \"today\" for consistency\n",
    "max_date = df_fact.agg(max(\"order_date\")).collect()[0][0]\n",
    "print(\"STEP 2: Reference date for Recency: \" + str(max_date))\n",
    "\n",
    "df_rfm = df_fact.groupBy(\"customer_id\").agg(\n",
    "    # RECENCY: days since last order\n",
    "    datediff(lit(max_date), max(\"order_date\")).alias(\"recency_days\"),\n",
    "    \n",
    "    # FREQUENCY: number of distinct orders\n",
    "    countDistinct(\"order_id\").alias(\"frequency\"),\n",
    "    \n",
    "    # MONETARY: total revenue (excluding cancelled)\n",
    "    round(\n",
    "        sum(when(col(\"order_status\") != \"CANCELLED\", col(\"net_line_total\")).otherwise(lit(0))), 2\n",
    "    ).alias(\"monetary\"),\n",
    "    \n",
    "    # Additional features\n",
    "    round(avg(\"net_line_total\"), 2).alias(\"avg_order_value\"),\n",
    "    sum(\"quantity\").alias(\"total_items_bought\"),\n",
    "    countDistinct(\"product_id\").alias(\"unique_products\"),\n",
    "    round(\n",
    "        sum(\"quantity\") / countDistinct(\"order_id\"), 2\n",
    "    ).alias(\"avg_items_per_order\"),\n",
    "    \n",
    "    # Channel behavior\n",
    "    countDistinct(\"channel\").alias(\"channels_used\"),\n",
    "    \n",
    "    # Order status behavior\n",
    "    countDistinct(when(col(\"order_status\") == \"CANCELLED\", col(\"order_id\"))).alias(\"cancelled_orders\"),\n",
    "    countDistinct(when(col(\"order_status\") == \"RETURNED\", col(\"order_id\"))).alias(\"returned_orders\"),\n",
    "    countDistinct(when(col(\"order_status\") == \"DELIVERED\", col(\"order_id\"))).alias(\"delivered_orders\"),\n",
    "    \n",
    "    # Time span\n",
    "    datediff(max(\"order_date\"), min(\"order_date\")).alias(\"customer_lifespan_days\"),\n",
    "    \n",
    "    # First and last order\n",
    "    min(\"order_date\").alias(\"first_order_date\"),\n",
    "    max(\"order_date\").alias(\"last_order_date\")\n",
    ")\n",
    "\n",
    "# Add derived metrics\n",
    "df_rfm_enriched = df_rfm \\\n",
    "    .withColumn(\"return_rate\",\n",
    "        when(col(\"frequency\") > 0,\n",
    "            round(col(\"returned_orders\") / col(\"frequency\") * 100, 2))\n",
    "        .otherwise(lit(0.0))) \\\n",
    "    .withColumn(\"cancel_rate\",\n",
    "        when(col(\"frequency\") > 0,\n",
    "            round(col(\"cancelled_orders\") / col(\"frequency\") * 100, 2))\n",
    "        .otherwise(lit(0.0))) \\\n",
    "    .withColumn(\"delivery_rate\",\n",
    "        when(col(\"frequency\") > 0,\n",
    "            round(col(\"delivered_orders\") / col(\"frequency\") * 100, 2))\n",
    "        .otherwise(lit(0.0)))\n",
    "\n",
    "rfm_count = df_rfm_enriched.count()\n",
    "print(\"STEP 2: RFM features built for \" + str(rfm_count) + \" customers\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 3: RFM Scoring (1-5 scale using quintiles)\n",
    "# ----------------------------------------------------------\n",
    "# WHAT ARE QUINTILES?\n",
    "# Divide customers into 5 equal groups (20% each) based \n",
    "# on each metric.\n",
    "#\n",
    "# For RECENCY (lower is better - more recent):\n",
    "#   Score 5: 0-20 days (most recent 20% of customers)\n",
    "#   Score 4: 21-60 days\n",
    "#   Score 3: 61-120 days\n",
    "#   Score 2: 121-200 days\n",
    "#   Score 1: 200+ days (haven't bought in a long time)\n",
    "#\n",
    "# For FREQUENCY and MONETARY (higher is better):\n",
    "#   Score 5: top 20% (most frequent/highest spending)\n",
    "#   Score 1: bottom 20%\n",
    "#\n",
    "# WHY SCORING?\n",
    "# Raw values are hard to compare:\n",
    "#   Recency: 5 days vs 200 days\n",
    "#   Monetary: $50 vs $5000\n",
    "# Scores (1-5) normalize everything to the same scale.\n",
    "# This is essential for K-Means clustering later.\n",
    "#\n",
    "# ntile(5) splits data into 5 equal buckets.\n",
    "# For recency, we ORDER ASC (low days = high score = 5)\n",
    "# For frequency/monetary, we ORDER ASC (high value = high score = 5)\n",
    "\n",
    "window_r = Window.orderBy(col(\"recency_days\").asc())\n",
    "window_f = Window.orderBy(col(\"frequency\").asc())\n",
    "window_m = Window.orderBy(col(\"monetary\").asc())\n",
    "\n",
    "df_rfm_scored = df_rfm_enriched \\\n",
    "    .withColumn(\"r_score\", ntile(5).over(window_r)) \\\n",
    "    .withColumn(\"f_score\", ntile(5).over(window_f)) \\\n",
    "    .withColumn(\"m_score\", ntile(5).over(window_m)) \\\n",
    "    .withColumn(\"rfm_score\",\n",
    "        col(\"r_score\") + col(\"f_score\") + col(\"m_score\")) \\\n",
    "    .withColumn(\"rfm_segment\",\n",
    "        when(col(\"rfm_score\") >= 13, lit(\"Champions\"))\n",
    "        .when(col(\"rfm_score\") >= 10, lit(\"Loyal Customers\"))\n",
    "        .when(col(\"rfm_score\") >= 7, lit(\"Potential Loyalists\"))\n",
    "        .when(col(\"rfm_score\") >= 5, lit(\"At Risk\"))\n",
    "        .otherwise(lit(\"Hibernating\")))\n",
    "\n",
    "# For recency, REVERSE the score (low recency = high score)\n",
    "# ntile with ASC order already gives score 1 to lowest recency (most recent)\n",
    "# We need to flip: most recent should be score 5\n",
    "df_rfm_scored = df_rfm_scored \\\n",
    "    .withColumn(\"r_score\", lit(6) - col(\"r_score\"))\n",
    "\n",
    "# Recalculate rfm_score and segment with corrected r_score\n",
    "df_rfm_scored = df_rfm_scored \\\n",
    "    .withColumn(\"rfm_score\",\n",
    "        col(\"r_score\") + col(\"f_score\") + col(\"m_score\")) \\\n",
    "    .withColumn(\"rfm_segment\",\n",
    "        when(col(\"rfm_score\") >= 13, lit(\"Champions\"))\n",
    "        .when(col(\"rfm_score\") >= 10, lit(\"Loyal Customers\"))\n",
    "        .when(col(\"rfm_score\") >= 7, lit(\"Potential Loyalists\"))\n",
    "        .when(col(\"rfm_score\") >= 5, lit(\"At Risk\"))\n",
    "        .otherwise(lit(\"Hibernating\")))\n",
    "\n",
    "print(\"STEP 3: RFM scores calculated (1-5 scale)\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 4: Save RFM features to Gold\n",
    "# ----------------------------------------------------------\n",
    "df_rfm_final = df_rfm_scored \\\n",
    "    .withColumn(\"_gold_processed_at\", current_timestamp()) \\\n",
    "    .withColumn(\"_gold_version\", lit(\"1.0\"))\n",
    "\n",
    "gold_rfm_path = GOLD + \"/ml_customer_rfm\"\n",
    "\n",
    "df_rfm_final.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", True) \\\n",
    "    .save(gold_rfm_path)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 5: Verify and analyze\n",
    "# ----------------------------------------------------------\n",
    "df_verify = spark.read.format(\"delta\").load(gold_rfm_path)\n",
    "final_count = df_verify.count()\n",
    "\n",
    "print(\"\")\n",
    "print(\"=\" * 65)\n",
    "print(\"ML - RFM FEATURES - COMPLETE\")\n",
    "print(\"=\" * 65)\n",
    "print(\"  Customers analyzed: \" + str(final_count))\n",
    "print(\"  Features:           \" + str(len(df_verify.columns)) + \" columns\")\n",
    "print(\"  Path:               \" + gold_rfm_path)\n",
    "\n",
    "print(\"\\n  Schema:\")\n",
    "df_verify.printSchema()\n",
    "\n",
    "print(\"\\n  RFM Sample (top customers by monetary):\")\n",
    "df_verify.select(\n",
    "    \"customer_id\", \"recency_days\", \"frequency\", \"monetary\",\n",
    "    \"r_score\", \"f_score\", \"m_score\", \"rfm_score\", \"rfm_segment\"\n",
    ").orderBy(desc(\"monetary\")).show(10, truncate=False)\n",
    "\n",
    "# Segment distribution\n",
    "print(\"\\n  CUSTOMER SEGMENT DISTRIBUTION:\")\n",
    "df_verify.groupBy(\"rfm_segment\").agg(\n",
    "    count(\"*\").alias(\"customers\"),\n",
    "    round(avg(\"recency_days\"), 0).alias(\"avg_recency\"),\n",
    "    round(avg(\"frequency\"), 1).alias(\"avg_frequency\"),\n",
    "    round(avg(\"monetary\"), 0).alias(\"avg_monetary\"),\n",
    "    round(avg(\"rfm_score\"), 1).alias(\"avg_rfm_score\")\n",
    ").orderBy(desc(\"avg_rfm_score\")).show(truncate=False)\n",
    "\n",
    "# RFM score distribution\n",
    "print(\"\\n  RFM Score distribution:\")\n",
    "df_verify.groupBy(\"rfm_score\").count().orderBy(\"rfm_score\").show(15)\n",
    "\n",
    "# Segment business insights\n",
    "print(\"\\n  BUSINESS INSIGHTS:\")\n",
    "champions = df_verify.filter(col(\"rfm_segment\") == \"Champions\").count()\n",
    "at_risk = df_verify.filter(col(\"rfm_segment\") == \"At Risk\").count()\n",
    "hibernating = df_verify.filter(col(\"rfm_segment\") == \"Hibernating\").count()\n",
    "\n",
    "print(\"  Champions:           \" + str(champions) + \" customers (protect and reward)\")\n",
    "print(\"  At Risk:             \" + str(at_risk) + \" customers (re-engage immediately!)\")\n",
    "print(\"  Hibernating:         \" + str(hibernating) + \" customers (win-back campaign)\")\n",
    "\n",
    "# Channel preference by segment\n",
    "print(\"\\n  Avg channels used by segment:\")\n",
    "df_verify.groupBy(\"rfm_segment\").agg(\n",
    "    round(avg(\"channels_used\"), 1).alias(\"avg_channels\"),\n",
    "    round(avg(\"unique_products\"), 1).alias(\"avg_unique_products\"),\n",
    "    round(avg(\"return_rate\"), 1).alias(\"avg_return_rate\")\n",
    ").orderBy(desc(\"avg_channels\")).show(truncate=False)\n",
    "\n",
    "print(\"[DONE] RFM Feature Engineering complete!\")\n",
    "print(\"[NEXT] Cell 17 - Final Pipeline Summary + Star Schema Queries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f51a5037-cd01-4b6d-ac70-9c147cffb000",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\nSHOPSMART AI - COMPLETE DATA PLATFORM VERIFICATION\n=================================================================\n\n  SILVER LAYER:\n  --------------------------------------------------\n    orders           1948 rows    [OK]\n    order_items      4904 rows    [OK]\n    customers         500 rows    [OK]\n    products           50 rows    [OK]\n    inventory         150 rows    [OK]\n    clickstream      3000 rows    [OK]\n    sessions         3000 rows    [OK]\n    payments         2000 rows    [OK]\n\n  GOLD LAYER:\n  --------------------------------------------------\n    dim_date              1096 rows    [OK]\n    dim_customer           500 rows    [OK]\n    dim_product             50 rows    [OK]\n    fact_sales            4780 rows    [OK]\n    agg_daily_sales       1107 rows    [OK]\n    ml_customer_rfm        488 rows    [OK]\n\n  TOTALS:\n    Silver: 15552 rows across 8 tables\n    Gold:   8021 rows across 6 tables\n\n\n=================================================================\nREGISTERING TABLES FOR SQL ANALYTICS\n=================================================================\n  All tables registered as SQL views [OK]\n\n\n=================================================================\nSTAR SCHEMA ANALYTICS\n=================================================================\n\n  QUERY 1: Revenue by Category and Quarter\n+-----------+-------------+------+---------+\n|category   |quarter_label|orders|revenue  |\n+-----------+-------------+------+---------+\n|Beauty     |Q1-2025      |98    |98501.9  |\n|Beauty     |Q1-2026      |100   |117671.15|\n|Beauty     |Q2-2025      |185   |198006.46|\n|Beauty     |Q3-2025      |165   |178693.15|\n|Beauty     |Q4-2025      |194   |211351.93|\n|Electronics|Q1-2025      |99    |122154.99|\n|Electronics|Q1-2026      |113   |159311.41|\n|Electronics|Q2-2025      |201   |250601.44|\n|Electronics|Q3-2025      |174   |221776.4 |\n|Electronics|Q4-2025      |198   |256542.54|\n|Fashion    |Q1-2025      |107   |132916.34|\n|Fashion    |Q1-2026      |95    |94495.48 |\n|Fashion    |Q2-2025      |201   |236337.19|\n|Fashion    |Q3-2025      |198   |216120.32|\n|Fashion    |Q4-2025      |186   |190281.58|\n|Home       |Q1-2025      |131   |147173.35|\n|Home       |Q1-2026      |132   |159160.64|\n|Home       |Q2-2025      |273   |308474.69|\n|Home       |Q3-2025      |260   |282894.56|\n|Home       |Q4-2025      |246   |289457.29|\n|Sports     |Q1-2025      |67    |85579.58 |\n|Sports     |Q1-2026      |67    |84266.77 |\n|Sports     |Q2-2025      |131   |173509.6 |\n|Sports     |Q3-2025      |112   |140174.79|\n|Sports     |Q4-2025      |127   |154338.45|\n+-----------+-------------+------+---------+\n\n\n  QUERY 2: Top 10 Customers by Revenue\n+-----------+----------+------------+---------+---------------+------+--------+\n|customer_id|first_name|loyalty_tier|age_group|rfm_segment    |orders|revenue |\n+-----------+----------+------------+---------+---------------+------+--------+\n|CUST356    |Eric      |Silver      |65+      |Champions      |9     |23630.2 |\n|CUST012    |Cindy     |Platinum    |35-44    |Champions      |9     |23506.16|\n|CUST335    |Christian |Silver      |18-24    |Champions      |5     |22736.16|\n|CUST247    |Kim       |Platinum    |35-44    |Champions      |8     |22551.3 |\n|CUST017    |Kristin   |Bronze      |35-44    |Champions      |12    |21731.53|\n|CUST482    |Ashley    |Bronze      |35-44    |Loyal Customers|7     |20833.44|\n|CUST172    |George    |Platinum    |65+      |Champions      |7     |20346.49|\n|CUST131    |Eric      |Silver      |65+      |Champions      |7     |20233.53|\n|CUST113    |Joseph    |Platinum    |35-44    |Loyal Customers|5     |19587.72|\n|CUST238    |Julie     |Silver      |18-24    |Champions      |7     |19535.93|\n+-----------+----------+------------+---------+---------------+------+--------+\n\n\n  QUERY 3: Channel Performance\n+-----------+------+---------+----------+--------------+\n|channel    |orders|customers|revenue   |avg_item_value|\n+-----------+------+---------+----------+--------------+\n|mobile_app |486   |315      |1170497.0 |969.76        |\n|marketplace|494   |314      |1143852.81|942.22        |\n|in_store   |496   |307      |1125645.29|928.75        |\n|web        |472   |313      |1069796.9 |932.69        |\n+-----------+------+---------+----------+--------------+\n\n\n  QUERY 4: Top 10 Products Revenue + Stock\n+----------+-----------+----------+---------+----------+----------+\n|product_id|category   |price_tier|revenue  |units_sold|margin_pct|\n+----------+-----------+----------+---------+----------+----------+\n|PROD040   |Electronics|Luxury    |164420.5 |202       |54.6      |\n|PROD033   |Home       |Luxury    |163289.19|200       |51.5      |\n|PROD026   |Electronics|Luxury    |160926.37|200       |55.1      |\n|PROD050   |Fashion    |Luxury    |156146.15|208       |39.8      |\n|PROD019   |Home       |Luxury    |149201.97|164       |46.9      |\n|PROD042   |Fashion    |Luxury    |141178.03|184       |53.3      |\n|PROD009   |Home       |Luxury    |140051.92|184       |48.0      |\n|PROD008   |Fashion    |Luxury    |137511.29|203       |46.1      |\n|PROD036   |Home       |Luxury    |135273.52|228       |58.7      |\n|PROD041   |Electronics|Luxury    |134908.48|185       |48.5      |\n+----------+-----------+----------+---------+----------+----------+\n\n\n  QUERY 5: Customer Segments Analysis\n+-------------------+---------+-----------+----------+-----------+--------------+\n|rfm_segment        |customers|avg_revenue|avg_orders|avg_recency|avg_return_pct|\n+-------------------+---------+-----------+----------+-----------+--------------+\n|Champions          |87       |14256.0    |6.6       |24.0       |15.0          |\n|Loyal Customers    |132      |9790.0     |4.7       |56.0       |18.2          |\n|Potential Loyalists|150      |6107.0     |3.4       |91.0       |17.0          |\n|At Risk            |67       |3860.0     |2.4       |134.0      |20.9          |\n|Hibernating        |52       |1948.0     |1.5       |204.0      |14.1          |\n+-------------------+---------+-----------+----------+-----------+--------------+\n\n\n  QUERY 6: Monthly Revenue Trend\n+----------+-----------+------+--------+-----------+\n|order_year|order_month|orders|revenue |cancel_rate|\n+----------+-----------+------+--------+-----------+\n|2025      |2          |79    |214236.0|24.6       |\n|2025      |3          |172   |372090.0|14.1       |\n|2025      |4          |172   |437768.0|18.2       |\n|2025      |5          |177   |390773.0|14.2       |\n|2025      |6          |154   |338389.0|16.0       |\n|2025      |7          |143   |305368.0|18.0       |\n|2025      |8          |174   |409340.0|20.6       |\n|2025      |9          |151   |324951.0|19.3       |\n|2025      |10         |142   |324940.0|21.4       |\n|2025      |11         |153   |364104.0|15.5       |\n|2025      |12         |180   |412927.0|16.6       |\n|2026      |1          |167   |390645.0|16.2       |\n|2026      |2          |84    |224260.0|12.6       |\n+----------+-----------+------+--------+-----------+\n\n\n=================================================================\nDATA QUALITY SUMMARY\n=================================================================\n  Issues Found and Resolved:\n    Orders:      52 null status     -> Quarantined\n    Customers:   53 null emails     -> Flagged\n    Customers:   PII exposed        -> SHA-256 hashed\n    Customers:   Nested JSON        -> Flattened\n    Products:    Nested attributes  -> Flattened\n    Inventory:   4 negative stock   -> Set to 0, flagged\n    Clickstream: 626 anonymous      -> Flagged\n    Clickstream: Nested geo         -> Flattened\n    Payments:    String timestamps  -> Parsed\n    Fact table:  124 orphan items   -> Excluded by JOIN\n\n\n=================================================================\nSHOPSMART AI - COMPLETE ARCHITECTURE SUMMARY\n=================================================================\n\n  DATA SOURCES (6):\n    Orders, Customers, Products, Clickstream, Inventory, Payments\n  \n  INFRASTRUCTURE:\n    Azure Data Lake Gen2 (ADLS) - 4 containers\n    Azure Databricks - Processing engine\n    Azure Key Vault - Secrets management\n    Azure Data Factory - Orchestration\n    Terraform - Infrastructure as Code\n  \n  MEDALLION ARCHITECTURE:\n    BRONZE: 7 raw files (CSV, JSON, JSON Lines)\n    SILVER: 8 Delta tables (cleaned, validated, PII masked)\n    GOLD:   6 Delta tables (Star Schema + ML features)\n  \n  STAR SCHEMA:\n    dim_date (1096) - dim_customer (500) - dim_product (50)\n                      fact_sales (4780)\n                      agg_daily_sales (1107)\n  \n  ML/AI:\n    Customer Segmentation: RFM Analysis (5 segments)\n      Champions (87), Loyal (132), Potential (150)\n      At Risk (67), Hibernating (52)\n  \n  DATA QUALITY:\n    Quarantine pattern, PII masking, null handling\n    Nested JSON flattening, negative value correction\n    Referential integrity via INNER JOINs\n\n  TECHNOLOGIES:\n    PySpark, Delta Lake, Spark SQL, Azure ADLS Gen2\n    Azure Databricks, Key Vault, Data Factory, Terraform\n\n=================================================================\nPROJECT COMPLETE!\n=================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 17: COMPLETE PIPELINE SUMMARY + STAR SCHEMA ANALYTICS\n",
    "# ============================================================\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# PART 1: PIPELINE VERIFICATION\n",
    "# ----------------------------------------------------------\n",
    "print(\"=\" * 65)\n",
    "print(\"SHOPSMART AI - COMPLETE DATA PLATFORM VERIFICATION\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "silver_tables = [\n",
    "    (\"orders\",       SILVER + \"/orders\"),\n",
    "    (\"order_items\",  SILVER + \"/order_items\"),\n",
    "    (\"customers\",    SILVER + \"/customers\"),\n",
    "    (\"products\",     SILVER + \"/products\"),\n",
    "    (\"inventory\",    SILVER + \"/inventory\"),\n",
    "    (\"clickstream\",  SILVER + \"/clickstream\"),\n",
    "    (\"sessions\",     SILVER + \"/sessions\"),\n",
    "    (\"payments\",     SILVER + \"/payments\"),\n",
    "]\n",
    "\n",
    "print(\"\\n  SILVER LAYER:\")\n",
    "print(\"  \" + \"-\" * 50)\n",
    "total_silver = 0\n",
    "for name, path in silver_tables:\n",
    "    try:\n",
    "        count = spark.read.format(\"delta\").load(path).count()\n",
    "        total_silver = total_silver + count\n",
    "        print(\"    \" + name.ljust(15) + str(count).rjust(6) + \" rows    [OK]\")\n",
    "    except:\n",
    "        print(\"    \" + name.ljust(15) + \"  ERROR\")\n",
    "\n",
    "gold_tables = [\n",
    "    (\"dim_date\",         GOLD + \"/dim_date\"),\n",
    "    (\"dim_customer\",     GOLD + \"/dim_customer\"),\n",
    "    (\"dim_product\",      GOLD + \"/dim_product\"),\n",
    "    (\"fact_sales\",       GOLD + \"/fact_sales\"),\n",
    "    (\"agg_daily_sales\",  GOLD + \"/agg_daily_sales\"),\n",
    "    (\"ml_customer_rfm\",  GOLD + \"/ml_customer_rfm\"),\n",
    "]\n",
    "\n",
    "print(\"\\n  GOLD LAYER:\")\n",
    "print(\"  \" + \"-\" * 50)\n",
    "total_gold = 0\n",
    "for name, path in gold_tables:\n",
    "    try:\n",
    "        count = spark.read.format(\"delta\").load(path).count()\n",
    "        total_gold = total_gold + count\n",
    "        print(\"    \" + name.ljust(20) + str(count).rjust(6) + \" rows    [OK]\")\n",
    "    except:\n",
    "        print(\"    \" + name.ljust(20) + \"  ERROR\")\n",
    "\n",
    "print(\"\\n  TOTALS:\")\n",
    "print(\"    Silver: \" + str(total_silver) + \" rows across 8 tables\")\n",
    "print(\"    Gold:   \" + str(total_gold) + \" rows across 6 tables\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# PART 2: REGISTER SQL VIEWS\n",
    "# ----------------------------------------------------------\n",
    "print(\"\\n\\n\" + \"=\" * 65)\n",
    "print(\"REGISTERING TABLES FOR SQL ANALYTICS\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "spark.read.format(\"delta\").load(GOLD + \"/dim_date\").createOrReplaceTempView(\"dim_date\")\n",
    "spark.read.format(\"delta\").load(GOLD + \"/dim_customer\").createOrReplaceTempView(\"dim_customer\")\n",
    "spark.read.format(\"delta\").load(GOLD + \"/dim_product\").createOrReplaceTempView(\"dim_product\")\n",
    "spark.read.format(\"delta\").load(GOLD + \"/fact_sales\").createOrReplaceTempView(\"fact_sales\")\n",
    "spark.read.format(\"delta\").load(GOLD + \"/agg_daily_sales\").createOrReplaceTempView(\"agg_daily_sales\")\n",
    "spark.read.format(\"delta\").load(GOLD + \"/ml_customer_rfm\").createOrReplaceTempView(\"customer_rfm\")\n",
    "spark.read.format(\"delta\").load(SILVER + \"/inventory\").createOrReplaceTempView(\"inventory\")\n",
    "\n",
    "print(\"  All tables registered as SQL views [OK]\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# PART 3: STAR SCHEMA QUERIES\n",
    "# ----------------------------------------------------------\n",
    "print(\"\\n\\n\" + \"=\" * 65)\n",
    "print(\"STAR SCHEMA ANALYTICS\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "# QUERY 1: Revenue by Category by Quarter\n",
    "print(\"\\n  QUERY 1: Revenue by Category and Quarter\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        p.category,\n",
    "        d.quarter_label,\n",
    "        COUNT(DISTINCT f.order_id) as orders,\n",
    "        ROUND(SUM(f.net_line_total), 2) as revenue\n",
    "    FROM fact_sales f\n",
    "    JOIN dim_product p ON f.product_id = p.product_id\n",
    "    JOIN dim_date d ON f.order_date_key = d.date_key\n",
    "    GROUP BY p.category, d.quarter_label\n",
    "    ORDER BY p.category, d.quarter_label\n",
    "\"\"\").show(25, truncate=False)\n",
    "\n",
    "# QUERY 2: Top 10 Customers with RFM Segment\n",
    "print(\"\\n  QUERY 2: Top 10 Customers by Revenue\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        c.customer_id,\n",
    "        c.first_name,\n",
    "        c.loyalty_tier,\n",
    "        c.age_group,\n",
    "        r.rfm_segment,\n",
    "        r.frequency as orders,\n",
    "        ROUND(r.monetary, 2) as revenue\n",
    "    FROM customer_rfm r\n",
    "    JOIN dim_customer c ON r.customer_id = c.customer_id\n",
    "    ORDER BY r.monetary DESC\n",
    "    LIMIT 10\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "# QUERY 3: Channel Performance\n",
    "print(\"\\n  QUERY 3: Channel Performance\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        channel,\n",
    "        COUNT(DISTINCT order_id) as orders,\n",
    "        COUNT(DISTINCT customer_id) as customers,\n",
    "        ROUND(SUM(net_line_total), 2) as revenue,\n",
    "        ROUND(AVG(net_line_total), 2) as avg_item_value\n",
    "    FROM fact_sales\n",
    "    GROUP BY channel\n",
    "    ORDER BY revenue DESC\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "# QUERY 4: Top Products with Stock Status\n",
    "print(\"\\n  QUERY 4: Top 10 Products Revenue + Stock\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        p.product_id,\n",
    "        p.category,\n",
    "        p.price_tier,\n",
    "        ROUND(SUM(f.net_line_total), 2) as revenue,\n",
    "        SUM(f.quantity) as units_sold,\n",
    "        ROUND(p.margin_pct, 1) as margin_pct\n",
    "    FROM fact_sales f\n",
    "    JOIN dim_product p ON f.product_id = p.product_id\n",
    "    GROUP BY p.product_id, p.category, p.price_tier, p.margin_pct\n",
    "    ORDER BY revenue DESC\n",
    "    LIMIT 10\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "# QUERY 5: Segment Deep Dive\n",
    "print(\"\\n  QUERY 5: Customer Segments Analysis\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        r.rfm_segment,\n",
    "        COUNT(*) as customers,\n",
    "        ROUND(AVG(r.monetary), 0) as avg_revenue,\n",
    "        ROUND(AVG(r.frequency), 1) as avg_orders,\n",
    "        ROUND(AVG(r.recency_days), 0) as avg_recency,\n",
    "        ROUND(AVG(r.return_rate), 1) as avg_return_pct\n",
    "    FROM customer_rfm r\n",
    "    GROUP BY r.rfm_segment\n",
    "    ORDER BY avg_revenue DESC\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "# QUERY 6: Monthly Trend\n",
    "print(\"\\n  QUERY 6: Monthly Revenue Trend\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        order_year,\n",
    "        order_month,\n",
    "        SUM(total_orders) as orders,\n",
    "        ROUND(SUM(net_revenue), 0) as revenue,\n",
    "        ROUND(AVG(cancel_rate_pct), 1) as cancel_rate\n",
    "    FROM agg_daily_sales\n",
    "    GROUP BY order_year, order_month\n",
    "    ORDER BY order_year, order_month\n",
    "\"\"\").show(15, truncate=False)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# PART 4: DATA QUALITY SUMMARY\n",
    "# ----------------------------------------------------------\n",
    "print(\"\\n\" + \"=\" * 65)\n",
    "print(\"DATA QUALITY SUMMARY\")\n",
    "print(\"=\" * 65)\n",
    "print(\"  Issues Found and Resolved:\")\n",
    "print(\"    Orders:      52 null status     -> Quarantined\")\n",
    "print(\"    Customers:   53 null emails     -> Flagged\")\n",
    "print(\"    Customers:   PII exposed        -> SHA-256 hashed\")\n",
    "print(\"    Customers:   Nested JSON        -> Flattened\")\n",
    "print(\"    Products:    Nested attributes  -> Flattened\")\n",
    "print(\"    Inventory:   4 negative stock   -> Set to 0, flagged\")\n",
    "print(\"    Clickstream: 626 anonymous      -> Flagged\")\n",
    "print(\"    Clickstream: Nested geo         -> Flattened\")\n",
    "print(\"    Payments:    String timestamps  -> Parsed\")\n",
    "print(\"    Fact table:  124 orphan items   -> Excluded by JOIN\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# PART 5: ARCHITECTURE SUMMARY\n",
    "# ----------------------------------------------------------\n",
    "print(\"\\n\\n\" + \"=\" * 65)\n",
    "print(\"SHOPSMART AI - COMPLETE ARCHITECTURE SUMMARY\")\n",
    "print(\"=\" * 65)\n",
    "print(\"\"\"\n",
    "  DATA SOURCES (6):\n",
    "    Orders, Customers, Products, Clickstream, Inventory, Payments\n",
    "  \n",
    "  INFRASTRUCTURE:\n",
    "    Azure Data Lake Gen2 (ADLS) - 4 containers\n",
    "    Azure Databricks - Processing engine\n",
    "    Azure Key Vault - Secrets management\n",
    "    Azure Data Factory - Orchestration\n",
    "    Terraform - Infrastructure as Code\n",
    "  \n",
    "  MEDALLION ARCHITECTURE:\n",
    "    BRONZE: 7 raw files (CSV, JSON, JSON Lines)\n",
    "    SILVER: 8 Delta tables (cleaned, validated, PII masked)\n",
    "    GOLD:   6 Delta tables (Star Schema + ML features)\n",
    "  \n",
    "  STAR SCHEMA:\n",
    "    dim_date (1096) - dim_customer (500) - dim_product (50)\n",
    "                      fact_sales (4780)\n",
    "                      agg_daily_sales (1107)\n",
    "  \n",
    "  ML/AI:\n",
    "    Customer Segmentation: RFM Analysis (5 segments)\n",
    "      Champions (87), Loyal (132), Potential (150)\n",
    "      At Risk (67), Hibernating (52)\n",
    "  \n",
    "  DATA QUALITY:\n",
    "    Quarantine pattern, PII masking, null handling\n",
    "    Nested JSON flattening, negative value correction\n",
    "    Referential integrity via INNER JOINs\n",
    "\n",
    "  TECHNOLOGIES:\n",
    "    PySpark, Delta Lake, Spark SQL, Azure ADLS Gen2\n",
    "    Azure Databricks, Key Vault, Data Factory, Terraform\n",
    "\"\"\")\n",
    "print(\"=\" * 65)\n",
    "print(\"PROJECT COMPLETE!\")\n",
    "print(\"=\" * 65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "651f1bbf-3cdc-4e0a-91e8-5927050de6d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1: Payments loaded - 2000 rows\n\nSTEP 2: Statistical baselines per payment method:\n+--------------+----------+-----------+-------------+----------+----------+\n|payment_method|total_txns|mean_amount|stddev_amount|min_amount|max_amount|\n+--------------+----------+-----------+-------------+----------+----------+\n|cod           |399       |2587.6     |1660.1       |36.4      |8580.67   |\n|wallet        |407       |2442.82    |1443.65      |34.18     |7488.94   |\n|credit_card   |388       |2555.57    |1602.43      |34.18     |8548.35   |\n|upi           |391       |2531.41    |1580.31      |18.2      |8694.44   |\n|debit_card    |415       |2408.03    |1563.96      |18.2      |8351.54   |\n+--------------+----------+-----------+-------------+----------+----------+\n\nSTEP 3: Statistical anomalies detected: 77\n\n=================================================================\nML - ANOMALY DETECTION - COMPLETE\n=================================================================\n  Total transactions:   2000\n  Statistical anomalies: 77\n  Path:                 abfss://gold@dlsshopsmartdev123.dfs.core.windows.net/ml_anomaly_detection\n\n  Anomaly type distribution:\n+------------+------------+----------+-----------+\n|anomaly_type|transactions|avg_amount|avg_z_score|\n+------------+------------+----------+-----------+\n|NORMAL      |1788        |2281.8    |0.68       |\n|MODERATE    |135         |3248.21   |1.67       |\n|SIGNIFICANT |68          |6123.84   |2.3        |\n|EXTREME     |9           |8062.66   |3.56       |\n+------------+------------+----------+-----------+\n\n\n  Overall risk distribution:\n+------------+------------+--------------+----------+\n|overall_risk|transactions|avg_risk_score|avg_amount|\n+------------+------------+--------------+----------+\n|CRITICAL    |106         |4.2           |4939.32   |\n|HIGH        |325         |3.0           |3259.87   |\n|MEDIUM      |712         |2.0           |2602.75   |\n|LOW         |638         |1.0           |2090.49   |\n|SAFE        |219         |0.0           |1084.22   |\n+------------+------------+--------------+----------+\n\n\n  Top 10 most anomalous transactions:\n+--------------+-------+--------------+-------+------------+------------------+------------+\n|transaction_id|amount |payment_method|z_score|anomaly_type|fraud_signal_count|overall_risk|\n+--------------+-------+--------------+-------+------------+------------------+------------+\n|TXN5FEE8963   |8694.44|upi           |3.9    |EXTREME     |1                 |HIGH        |\n|TXN321474D9   |8351.54|debit_card    |3.8    |EXTREME     |2                 |CRITICAL    |\n|TXN59FFEDE1   |8548.35|credit_card   |3.74   |EXTREME     |1                 |HIGH        |\n|TXN03DC5D6C   |8111.52|debit_card    |3.65   |EXTREME     |2                 |CRITICAL    |\n|TXNE04E6D80   |8580.67|cod           |3.61   |EXTREME     |3                 |CRITICAL    |\n|TXNADCF15DE   |7488.94|wallet        |3.5    |EXTREME     |2                 |CRITICAL    |\n|TXNF8448C55   |8233.78|cod           |3.4    |EXTREME     |2                 |CRITICAL    |\n|TXN3DD3377D   |7170.53|wallet        |3.27   |EXTREME     |4                 |CRITICAL    |\n|TXNE845B70D   |7384.14|debit_card    |3.18   |EXTREME     |2                 |CRITICAL    |\n|TXNAC519418   |6938.7 |debit_card    |2.9    |SIGNIFICANT |1                 |HIGH        |\n+--------------+-------+--------------+-------+------------+------------------+------------+\nonly showing top 10 rows\n\n  Anomalies by payment method:\n+--------------+---------+------------------+-----------+\n|payment_method|anomalies|avg_anomaly_amount|avg_z_score|\n+--------------+---------+------------------+-----------+\n|upi           |20       |6244.14           |2.35       |\n|debit_card    |19       |6342.84           |2.52       |\n|cod           |13       |6871.6            |2.58       |\n|wallet        |13       |5904.47           |2.4        |\n|credit_card   |12       |6458.27           |2.44       |\n+--------------+---------+------------------+-----------+\n\n  ALERT SUMMARY:\n    CRITICAL risk: 106 transactions (investigate immediately)\n    HIGH risk:     325 transactions (review within 24h)\n\n=================================================================\nALL ML MODELS COMPLETE!\n=================================================================\n  1. Customer Segmentation (RFM):  488 customers in 5 segments\n  2. Anomaly Detection (Z-Score):  77 anomalies detected\n  3. Fraud Risk Scoring:           Rule-based + Statistical\n\n  These fulfill the AI/ML layer in the architecture diagram:\n    Customer Segmentation  [DONE]\n    Anomaly Detection      [DONE]\n    Demand Forecasting     [Future enhancement]\n\n=================================================================\nNEXT: Push to GitHub with README\n=================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 18: ML - PAYMENT ANOMALY DETECTION\n",
    "# ============================================================\n",
    "#\n",
    "# WHAT IS ANOMALY DETECTION?\n",
    "# --------------------------\n",
    "# Finding transactions that are \"unusual\" compared to normal\n",
    "# patterns. Unusual = potentially fraudulent.\n",
    "#\n",
    "# We already built fraud signals in Silver (Cell 10):\n",
    "#   is_high_risk, is_off_hours, is_high_amount, is_international\n",
    "#   fraud_signal_count, fraud_risk_label\n",
    "#\n",
    "# Now we add STATISTICAL anomaly detection:\n",
    "#   - Calculate mean and standard deviation of amounts\n",
    "#   - Any transaction > mean + 2*stddev is an anomaly\n",
    "#   - This is called Z-score based anomaly detection\n",
    "#\n",
    "# WHY Z-SCORE (not Isolation Forest)?\n",
    "# Isolation Forest requires sklearn which may not work on\n",
    "# shared clusters. Z-score works purely in PySpark and is\n",
    "# actually what many production fraud systems use as a\n",
    "# first-pass filter.\n",
    "#\n",
    "# IN YOUR ARCHITECTURE DIAGRAM:\n",
    "# This fulfills the \"Anomaly Detection\" box in the AI/ML layer.\n",
    "# ============================================================\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 1: Read Silver Payments\n",
    "# ----------------------------------------------------------\n",
    "df_payments = spark.read.format(\"delta\").load(SILVER + \"/payments\")\n",
    "total_payments = df_payments.count()\n",
    "print(\"STEP 1: Payments loaded - \" + str(total_payments) + \" rows\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 2: Calculate statistical baselines\n",
    "# ----------------------------------------------------------\n",
    "# WHAT ARE WE CALCULATING?\n",
    "# For each payment_method, we calculate:\n",
    "#   mean_amount: average transaction amount\n",
    "#   stddev_amount: standard deviation\n",
    "#\n",
    "# WHY PER PAYMENT METHOD?\n",
    "# A $5000 credit card purchase is normal.\n",
    "# A $5000 UPI transfer is suspicious.\n",
    "# Each payment method has different normal ranges.\n",
    "\n",
    "stats = df_payments.groupBy(\"payment_method\").agg(\n",
    "    count(\"*\").alias(\"total_txns\"),\n",
    "    round(avg(\"amount\"), 2).alias(\"mean_amount\"),\n",
    "    round(stddev(\"amount\"), 2).alias(\"stddev_amount\"),\n",
    "    round(min(\"amount\"), 2).alias(\"min_amount\"),\n",
    "    round(max(\"amount\"), 2).alias(\"max_amount\")\n",
    ")\n",
    "\n",
    "print(\"\\nSTEP 2: Statistical baselines per payment method:\")\n",
    "stats.show(truncate=False)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 3: Calculate Z-scores and detect anomalies\n",
    "# ----------------------------------------------------------\n",
    "# Z-SCORE FORMULA:\n",
    "#   z_score = (amount - mean) / stddev\n",
    "#\n",
    "# INTERPRETATION:\n",
    "#   z_score = 0:  exactly average\n",
    "#   z_score = 1:  one stddev above average\n",
    "#   z_score = 2:  two stddev above average (top ~2.5%)\n",
    "#   z_score = 3:  three stddev above average (top ~0.1%)\n",
    "#\n",
    "# RULE: z_score > 2 = ANOMALY\n",
    "# This catches the top ~2.5% of unusual transactions.\n",
    "#\n",
    "# We use a WINDOW function to calculate mean/stddev per\n",
    "# payment_method and apply it to each row.\n",
    "\n",
    "window_method = Window.partitionBy(\"payment_method\")\n",
    "\n",
    "df_anomaly = df_payments \\\n",
    "    .withColumn(\"method_mean\", avg(\"amount\").over(window_method)) \\\n",
    "    .withColumn(\"method_stddev\", stddev(\"amount\").over(window_method)) \\\n",
    "    .withColumn(\"z_score\",\n",
    "        when(col(\"method_stddev\") > 0,\n",
    "            round(abs(col(\"amount\") - col(\"method_mean\")) / col(\"method_stddev\"), 2))\n",
    "        .otherwise(lit(0.0))) \\\n",
    "    .withColumn(\"is_statistical_anomaly\",\n",
    "        when(col(\"z_score\") > 2, lit(True)).otherwise(lit(False))) \\\n",
    "    .withColumn(\"anomaly_type\",\n",
    "        when(col(\"z_score\") > 3, lit(\"EXTREME\"))\n",
    "        .when(col(\"z_score\") > 2, lit(\"SIGNIFICANT\"))\n",
    "        .when(col(\"z_score\") > 1.5, lit(\"MODERATE\"))\n",
    "        .otherwise(lit(\"NORMAL\")))\n",
    "\n",
    "# Combine with existing fraud signals for overall risk\n",
    "df_anomaly_final = df_anomaly \\\n",
    "    .withColumn(\"combined_risk_score\",\n",
    "        col(\"fraud_signal_count\") + \n",
    "        when(col(\"is_statistical_anomaly\"), lit(2)).otherwise(lit(0))) \\\n",
    "    .withColumn(\"overall_risk\",\n",
    "        when(col(\"combined_risk_score\") >= 4, lit(\"CRITICAL\"))\n",
    "        .when(col(\"combined_risk_score\") >= 3, lit(\"HIGH\"))\n",
    "        .when(col(\"combined_risk_score\") >= 2, lit(\"MEDIUM\"))\n",
    "        .when(col(\"combined_risk_score\") >= 1, lit(\"LOW\"))\n",
    "        .otherwise(lit(\"SAFE\")))\n",
    "\n",
    "anomaly_count = df_anomaly_final.filter(col(\"is_statistical_anomaly\") == True).count()\n",
    "print(\"STEP 3: Statistical anomalies detected: \" + str(anomaly_count))\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 4: Save anomaly results to Gold\n",
    "# ----------------------------------------------------------\n",
    "df_anomaly_save = df_anomaly_final.select(\n",
    "    \"transaction_id\", \"order_id\", \"amount\", \"payment_method\",\n",
    "    \"status\", \"risk_score\", \"risk_level\",\n",
    "    \"is_high_risk\", \"is_off_hours\", \"is_high_amount\", \"is_international\",\n",
    "    \"fraud_signal_count\", \"fraud_risk_label\",\n",
    "    \"z_score\", \"is_statistical_anomaly\", \"anomaly_type\",\n",
    "    \"combined_risk_score\", \"overall_risk\",\n",
    "    \"transaction_timestamp\", \"transaction_date\"\n",
    ").withColumn(\"_gold_processed_at\", current_timestamp()) \\\n",
    " .withColumn(\"_gold_version\", lit(\"1.0\"))\n",
    "\n",
    "gold_anomaly_path = GOLD + \"/ml_anomaly_detection\"\n",
    "\n",
    "df_anomaly_save.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", True) \\\n",
    "    .save(gold_anomaly_path)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 5: Verify and analyze\n",
    "# ----------------------------------------------------------\n",
    "df_verify = spark.read.format(\"delta\").load(gold_anomaly_path)\n",
    "final_count = df_verify.count()\n",
    "\n",
    "print(\"\")\n",
    "print(\"=\" * 65)\n",
    "print(\"ML - ANOMALY DETECTION - COMPLETE\")\n",
    "print(\"=\" * 65)\n",
    "print(\"  Total transactions:   \" + str(final_count))\n",
    "print(\"  Statistical anomalies: \" + str(anomaly_count))\n",
    "print(\"  Path:                 \" + gold_anomaly_path)\n",
    "\n",
    "# Anomaly type distribution\n",
    "print(\"\\n  Anomaly type distribution:\")\n",
    "df_verify.groupBy(\"anomaly_type\").agg(\n",
    "    count(\"*\").alias(\"transactions\"),\n",
    "    round(avg(\"amount\"), 2).alias(\"avg_amount\"),\n",
    "    round(avg(\"z_score\"), 2).alias(\"avg_z_score\")\n",
    ").orderBy(\"avg_z_score\").show(truncate=False)\n",
    "\n",
    "# Overall risk distribution\n",
    "print(\"\\n  Overall risk distribution:\")\n",
    "df_verify.groupBy(\"overall_risk\").agg(\n",
    "    count(\"*\").alias(\"transactions\"),\n",
    "    round(avg(\"combined_risk_score\"), 1).alias(\"avg_risk_score\"),\n",
    "    round(avg(\"amount\"), 2).alias(\"avg_amount\")\n",
    ").orderBy(desc(\"avg_risk_score\")).show(truncate=False)\n",
    "\n",
    "# Top anomalous transactions\n",
    "print(\"\\n  Top 10 most anomalous transactions:\")\n",
    "df_verify.filter(col(\"is_statistical_anomaly\") == True) \\\n",
    "    .select(\n",
    "        \"transaction_id\", \"amount\", \"payment_method\",\n",
    "        \"z_score\", \"anomaly_type\", \"fraud_signal_count\", \"overall_risk\"\n",
    "    ) \\\n",
    "    .orderBy(desc(\"z_score\")) \\\n",
    "    .show(10, truncate=False)\n",
    "\n",
    "# Risk breakdown by payment method\n",
    "print(\"\\n  Anomalies by payment method:\")\n",
    "df_verify.filter(col(\"is_statistical_anomaly\") == True) \\\n",
    "    .groupBy(\"payment_method\").agg(\n",
    "        count(\"*\").alias(\"anomalies\"),\n",
    "        round(avg(\"amount\"), 2).alias(\"avg_anomaly_amount\"),\n",
    "        round(avg(\"z_score\"), 2).alias(\"avg_z_score\")\n",
    "    ).orderBy(desc(\"anomalies\")).show(truncate=False)\n",
    "\n",
    "# CRITICAL risk transactions (need immediate attention)\n",
    "critical = df_verify.filter(col(\"overall_risk\") == \"CRITICAL\").count()\n",
    "high = df_verify.filter(col(\"overall_risk\") == \"HIGH\").count()\n",
    "print(\"  ALERT SUMMARY:\")\n",
    "print(\"    CRITICAL risk: \" + str(critical) + \" transactions (investigate immediately)\")\n",
    "print(\"    HIGH risk:     \" + str(high) + \" transactions (review within 24h)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 65)\n",
    "print(\"ALL ML MODELS COMPLETE!\")\n",
    "print(\"=\" * 65)\n",
    "print(\"  1. Customer Segmentation (RFM):  488 customers in 5 segments\")\n",
    "print(\"  2. Anomaly Detection (Z-Score):  \" + str(anomaly_count) + \" anomalies detected\")\n",
    "print(\"  3. Fraud Risk Scoring:           Rule-based + Statistical\")\n",
    "print(\"\")\n",
    "print(\"  These fulfill the AI/ML layer in the architecture diagram:\")\n",
    "print(\"    Customer Segmentation  [DONE]\")\n",
    "print(\"    Anomaly Detection      [DONE]\")\n",
    "print(\"    Demand Forecasting     [Future enhancement]\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 65)\n",
    "print(\"NEXT: Push to GitHub with README\")\n",
    "print(\"=\" * 65)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03_ML_Model",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}